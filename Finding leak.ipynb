{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import time\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = []\n",
    "for i, j in enumerate(document_ids):\n",
    "    if j == 123:\n",
    "        inds.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Expectation_propagation.txt'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_names[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4735, 4736, 4737, 4738]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "document_ids = []\n",
    "\n",
    "for doc_id, filename in enumerate(os.listdir('./Data/Input/collection/')):\n",
    "    with open('./Data/Input/collection/' + \"/\" + filename, \"r\") as f:\n",
    "        sentences_raw = '.'.join(f.read().split('\\n')).split('.')\n",
    "\n",
    "    for sentence in sentences_raw:\n",
    "        sentence_nopunct = re.sub(\"[^A-Za-zА-Яа-я ]\", '', sentence)\n",
    "        sentence_nopunct = sentence_nopunct.lower().strip()\n",
    "        if len(sentence_nopunct) > 1:\n",
    "            sentences.append(sentence_nopunct)\n",
    "            document_ids.append(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_ids[94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'where formula denote the gram matrices of formula and formula respectively formula is a transfer gram matrix defined as formula and formula'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntaxnet_out = pd.read_table('./Data/Component_Results/syntaxnet/syntaxnet_out.txt', header=None,\n",
    "                              dtype={0: np.int, 6: np.int},\n",
    "                              quoting=csv.QUOTE_NONE, engine='c'\n",
    "                             )[[0, 1, 3, 6, 7]].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntaxnet_out.rename(columns={0: \"word_id\", 1: \"word\", 3: \"POS\",\n",
    "                              6: \"parent_id\", 7: \"dependency\"},\n",
    "                     inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>dependency</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3350</th>\n",
       "      <td>6</td>\n",
       "      <td>distribution</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>2</td>\n",
       "      <td>nmod</td>\n",
       "      <td>135</td>\n",
       "      <td>distribution</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>7</td>\n",
       "      <td>under</td>\n",
       "      <td>ADP</td>\n",
       "      <td>10</td>\n",
       "      <td>case</td>\n",
       "      <td>135</td>\n",
       "      <td>under</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>8</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>10</td>\n",
       "      <td>det</td>\n",
       "      <td>135</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3353</th>\n",
       "      <td>9</td>\n",
       "      <td>fixed</td>\n",
       "      <td>VERB</td>\n",
       "      <td>10</td>\n",
       "      <td>amod</td>\n",
       "      <td>135</td>\n",
       "      <td>fix</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3354</th>\n",
       "      <td>10</td>\n",
       "      <td>value</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>2</td>\n",
       "      <td>nmod</td>\n",
       "      <td>135</td>\n",
       "      <td>value</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3355</th>\n",
       "      <td>11</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>12</td>\n",
       "      <td>case</td>\n",
       "      <td>135</td>\n",
       "      <td>of</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3356</th>\n",
       "      <td>12</td>\n",
       "      <td>formula</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>10</td>\n",
       "      <td>nmod</td>\n",
       "      <td>135</td>\n",
       "      <td>formula</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3357</th>\n",
       "      <td>13</td>\n",
       "      <td>may</td>\n",
       "      <td>AUX</td>\n",
       "      <td>15</td>\n",
       "      <td>aux</td>\n",
       "      <td>135</td>\n",
       "      <td>may</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3358</th>\n",
       "      <td>14</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>15</td>\n",
       "      <td>auxpass</td>\n",
       "      <td>135</td>\n",
       "      <td>be</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>15</td>\n",
       "      <td>computed</td>\n",
       "      <td>VERB</td>\n",
       "      <td>-1</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>135</td>\n",
       "      <td>compute</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>16</td>\n",
       "      <td>as</td>\n",
       "      <td>ADV</td>\n",
       "      <td>15</td>\n",
       "      <td>advmod</td>\n",
       "      <td>135</td>\n",
       "      <td>as</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>0</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>2</td>\n",
       "      <td>case</td>\n",
       "      <td>136</td>\n",
       "      <td>in</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>1</td>\n",
       "      <td>this</td>\n",
       "      <td>PRON</td>\n",
       "      <td>2</td>\n",
       "      <td>det</td>\n",
       "      <td>136</td>\n",
       "      <td>this</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>2</td>\n",
       "      <td>discretevalued</td>\n",
       "      <td>AUX</td>\n",
       "      <td>3</td>\n",
       "      <td>nmod</td>\n",
       "      <td>136</td>\n",
       "      <td>discretevalued</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>3</td>\n",
       "      <td>setting</td>\n",
       "      <td>VERB</td>\n",
       "      <td>-1</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>136</td>\n",
       "      <td>set</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>4</td>\n",
       "      <td>with</td>\n",
       "      <td>ADP</td>\n",
       "      <td>7</td>\n",
       "      <td>case</td>\n",
       "      <td>136</td>\n",
       "      <td>with</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>5</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>7</td>\n",
       "      <td>det</td>\n",
       "      <td>136</td>\n",
       "      <td>the</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3367</th>\n",
       "      <td>6</td>\n",
       "      <td>kronecker</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>7</td>\n",
       "      <td>amod</td>\n",
       "      <td>136</td>\n",
       "      <td>kronecker</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3368</th>\n",
       "      <td>7</td>\n",
       "      <td>delta</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>3</td>\n",
       "      <td>nmod</td>\n",
       "      <td>136</td>\n",
       "      <td>delta</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3369</th>\n",
       "      <td>8</td>\n",
       "      <td>kernel</td>\n",
       "      <td>ADP</td>\n",
       "      <td>13</td>\n",
       "      <td>case</td>\n",
       "      <td>136</td>\n",
       "      <td>kernel</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370</th>\n",
       "      <td>9</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>13</td>\n",
       "      <td>det</td>\n",
       "      <td>136</td>\n",
       "      <td>the</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3371</th>\n",
       "      <td>10</td>\n",
       "      <td>kernel</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>11</td>\n",
       "      <td>compound</td>\n",
       "      <td>136</td>\n",
       "      <td>kernel</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3372</th>\n",
       "      <td>11</td>\n",
       "      <td>sum</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>13</td>\n",
       "      <td>compound</td>\n",
       "      <td>136</td>\n",
       "      <td>sum</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3373</th>\n",
       "      <td>12</td>\n",
       "      <td>rule</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>13</td>\n",
       "      <td>compound</td>\n",
       "      <td>136</td>\n",
       "      <td>rule</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374</th>\n",
       "      <td>13</td>\n",
       "      <td>becomes</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>7</td>\n",
       "      <td>nmod</td>\n",
       "      <td>136</td>\n",
       "      <td>becomes</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>3</td>\n",
       "      <td>det</td>\n",
       "      <td>137</td>\n",
       "      <td>the</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>1</td>\n",
       "      <td>kernel</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>3</td>\n",
       "      <td>amod</td>\n",
       "      <td>137</td>\n",
       "      <td>kernel</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>2</td>\n",
       "      <td>chain</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>3</td>\n",
       "      <td>compound</td>\n",
       "      <td>137</td>\n",
       "      <td>chain</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>3</td>\n",
       "      <td>rule</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>8</td>\n",
       "      <td>nsubjpass</td>\n",
       "      <td>137</td>\n",
       "      <td>rule</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>4</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>6</td>\n",
       "      <td>case</td>\n",
       "      <td>137</td>\n",
       "      <td>in</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>5</td>\n",
       "      <td>this</td>\n",
       "      <td>DET</td>\n",
       "      <td>6</td>\n",
       "      <td>det</td>\n",
       "      <td>137</td>\n",
       "      <td>this</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>6</td>\n",
       "      <td>case</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>3</td>\n",
       "      <td>nmod</td>\n",
       "      <td>137</td>\n",
       "      <td>case</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>7</td>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "      <td>8</td>\n",
       "      <td>auxpass</td>\n",
       "      <td>137</td>\n",
       "      <td>is</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>8</td>\n",
       "      <td>given</td>\n",
       "      <td>VERB</td>\n",
       "      <td>-1</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>137</td>\n",
       "      <td>give</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384</th>\n",
       "      <td>9</td>\n",
       "      <td>by</td>\n",
       "      <td>ADP</td>\n",
       "      <td>8</td>\n",
       "      <td>nmod</td>\n",
       "      <td>137</td>\n",
       "      <td>by</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>0</td>\n",
       "      <td>aixi</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>5</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>138</td>\n",
       "      <td>aixi</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "      <td>VERB</td>\n",
       "      <td>5</td>\n",
       "      <td>cop</td>\n",
       "      <td>138</td>\n",
       "      <td>be</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3387</th>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>5</td>\n",
       "      <td>det</td>\n",
       "      <td>138</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3388</th>\n",
       "      <td>3</td>\n",
       "      <td>theoretical</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>5</td>\n",
       "      <td>amod</td>\n",
       "      <td>138</td>\n",
       "      <td>theoretical</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3389</th>\n",
       "      <td>4</td>\n",
       "      <td>mathematical</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>5</td>\n",
       "      <td>amod</td>\n",
       "      <td>138</td>\n",
       "      <td>mathematical</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3390</th>\n",
       "      <td>5</td>\n",
       "      <td>formalism</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>-1</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>138</td>\n",
       "      <td>formalism</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3391</th>\n",
       "      <td>6</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td>9</td>\n",
       "      <td>case</td>\n",
       "      <td>138</td>\n",
       "      <td>for</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3392</th>\n",
       "      <td>7</td>\n",
       "      <td>artificial</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>9</td>\n",
       "      <td>amod</td>\n",
       "      <td>138</td>\n",
       "      <td>artificial</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3393</th>\n",
       "      <td>8</td>\n",
       "      <td>general</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>9</td>\n",
       "      <td>amod</td>\n",
       "      <td>138</td>\n",
       "      <td>general</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3394</th>\n",
       "      <td>9</td>\n",
       "      <td>intelligence</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>5</td>\n",
       "      <td>nmod</td>\n",
       "      <td>138</td>\n",
       "      <td>intelligence</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3395</th>\n",
       "      <td>0</td>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "      <td>1</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>139</td>\n",
       "      <td>it</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3396</th>\n",
       "      <td>1</td>\n",
       "      <td>combines</td>\n",
       "      <td>VERB</td>\n",
       "      <td>-1</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>139</td>\n",
       "      <td>combine</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3397</th>\n",
       "      <td>2</td>\n",
       "      <td>solomonoff</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>3</td>\n",
       "      <td>amod</td>\n",
       "      <td>139</td>\n",
       "      <td>solomonoff</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398</th>\n",
       "      <td>3</td>\n",
       "      <td>induction</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>1</td>\n",
       "      <td>dobj</td>\n",
       "      <td>139</td>\n",
       "      <td>induction</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3399</th>\n",
       "      <td>4</td>\n",
       "      <td>with</td>\n",
       "      <td>ADP</td>\n",
       "      <td>7</td>\n",
       "      <td>case</td>\n",
       "      <td>139</td>\n",
       "      <td>with</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>5</td>\n",
       "      <td>sequential</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>7</td>\n",
       "      <td>amod</td>\n",
       "      <td>139</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>AIXI.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_id            word   POS  parent_id dependency  sentence_id  \\\n",
       "3350        6    distribution  NOUN          2       nmod          135   \n",
       "3351        7           under   ADP         10       case          135   \n",
       "3352        8               a   DET         10        det          135   \n",
       "3353        9           fixed  VERB         10       amod          135   \n",
       "3354       10           value  NOUN          2       nmod          135   \n",
       "3355       11              of   ADP         12       case          135   \n",
       "3356       12         formula   ADJ         10       nmod          135   \n",
       "3357       13             may   AUX         15        aux          135   \n",
       "3358       14              be   AUX         15    auxpass          135   \n",
       "3359       15        computed  VERB         -1       ROOT          135   \n",
       "3360       16              as   ADV         15     advmod          135   \n",
       "3361        0              in   ADP          2       case          136   \n",
       "3362        1            this  PRON          2        det          136   \n",
       "3363        2  discretevalued   AUX          3       nmod          136   \n",
       "3364        3         setting  VERB         -1       ROOT          136   \n",
       "3365        4            with   ADP          7       case          136   \n",
       "3366        5             the   DET          7        det          136   \n",
       "3367        6       kronecker  NOUN          7       amod          136   \n",
       "3368        7           delta  NOUN          3       nmod          136   \n",
       "3369        8          kernel   ADP         13       case          136   \n",
       "3370        9             the   DET         13        det          136   \n",
       "3371       10          kernel  NOUN         11   compound          136   \n",
       "3372       11             sum  NOUN         13   compound          136   \n",
       "3373       12            rule  NOUN         13   compound          136   \n",
       "3374       13         becomes  NOUN          7       nmod          136   \n",
       "3375        0             the   DET          3        det          137   \n",
       "3376        1          kernel  NOUN          3       amod          137   \n",
       "3377        2           chain  NOUN          3   compound          137   \n",
       "3378        3            rule  NOUN          8  nsubjpass          137   \n",
       "3379        4              in   ADP          6       case          137   \n",
       "3380        5            this   DET          6        det          137   \n",
       "3381        6            case  NOUN          3       nmod          137   \n",
       "3382        7              is   AUX          8    auxpass          137   \n",
       "3383        8           given  VERB         -1       ROOT          137   \n",
       "3384        9              by   ADP          8       nmod          137   \n",
       "3385        0            aixi  CONJ          5      nsubj          138   \n",
       "3386        1              is  VERB          5        cop          138   \n",
       "3387        2               a   DET          5        det          138   \n",
       "3388        3     theoretical   ADJ          5       amod          138   \n",
       "3389        4    mathematical   ADJ          5       amod          138   \n",
       "3390        5       formalism  NOUN         -1       ROOT          138   \n",
       "3391        6             for   ADP          9       case          138   \n",
       "3392        7      artificial   ADJ          9       amod          138   \n",
       "3393        8         general   ADJ          9       amod          138   \n",
       "3394        9    intelligence  NOUN          5       nmod          138   \n",
       "3395        0              it  PRON          1      nsubj          139   \n",
       "3396        1        combines  VERB         -1       ROOT          139   \n",
       "3397        2      solomonoff   ADJ          3       amod          139   \n",
       "3398        3       induction  NOUN          1       dobj          139   \n",
       "3399        4            with   ADP          7       case          139   \n",
       "3400        5      sequential   ADJ          7       amod          139   \n",
       "\n",
       "               lemma  doc_id                               doc_name  \n",
       "3350    distribution       0  Kernel_embedding_of_distributions.txt  \n",
       "3351           under       0  Kernel_embedding_of_distributions.txt  \n",
       "3352               a       0  Kernel_embedding_of_distributions.txt  \n",
       "3353             fix       0  Kernel_embedding_of_distributions.txt  \n",
       "3354           value       0  Kernel_embedding_of_distributions.txt  \n",
       "3355              of       0  Kernel_embedding_of_distributions.txt  \n",
       "3356         formula       0  Kernel_embedding_of_distributions.txt  \n",
       "3357             may       0  Kernel_embedding_of_distributions.txt  \n",
       "3358              be       0  Kernel_embedding_of_distributions.txt  \n",
       "3359         compute       0  Kernel_embedding_of_distributions.txt  \n",
       "3360              as       0  Kernel_embedding_of_distributions.txt  \n",
       "3361              in       0  Kernel_embedding_of_distributions.txt  \n",
       "3362            this       0  Kernel_embedding_of_distributions.txt  \n",
       "3363  discretevalued       0  Kernel_embedding_of_distributions.txt  \n",
       "3364             set       0  Kernel_embedding_of_distributions.txt  \n",
       "3365            with       0  Kernel_embedding_of_distributions.txt  \n",
       "3366             the       0  Kernel_embedding_of_distributions.txt  \n",
       "3367       kronecker       0  Kernel_embedding_of_distributions.txt  \n",
       "3368           delta       0  Kernel_embedding_of_distributions.txt  \n",
       "3369          kernel       0  Kernel_embedding_of_distributions.txt  \n",
       "3370             the       0  Kernel_embedding_of_distributions.txt  \n",
       "3371          kernel       0  Kernel_embedding_of_distributions.txt  \n",
       "3372             sum       0  Kernel_embedding_of_distributions.txt  \n",
       "3373            rule       0  Kernel_embedding_of_distributions.txt  \n",
       "3374         becomes       0  Kernel_embedding_of_distributions.txt  \n",
       "3375             the       0  Kernel_embedding_of_distributions.txt  \n",
       "3376          kernel       0  Kernel_embedding_of_distributions.txt  \n",
       "3377           chain       0  Kernel_embedding_of_distributions.txt  \n",
       "3378            rule       0  Kernel_embedding_of_distributions.txt  \n",
       "3379              in       0  Kernel_embedding_of_distributions.txt  \n",
       "3380            this       0  Kernel_embedding_of_distributions.txt  \n",
       "3381            case       0  Kernel_embedding_of_distributions.txt  \n",
       "3382              is       0  Kernel_embedding_of_distributions.txt  \n",
       "3383            give       0  Kernel_embedding_of_distributions.txt  \n",
       "3384              by       0  Kernel_embedding_of_distributions.txt  \n",
       "3385            aixi       1                               AIXI.txt  \n",
       "3386              be       1                               AIXI.txt  \n",
       "3387               a       1                               AIXI.txt  \n",
       "3388     theoretical       1                               AIXI.txt  \n",
       "3389    mathematical       1                               AIXI.txt  \n",
       "3390       formalism       1                               AIXI.txt  \n",
       "3391             for       1                               AIXI.txt  \n",
       "3392      artificial       1                               AIXI.txt  \n",
       "3393         general       1                               AIXI.txt  \n",
       "3394    intelligence       1                               AIXI.txt  \n",
       "3395              it       1                               AIXI.txt  \n",
       "3396         combine       1                               AIXI.txt  \n",
       "3397      solomonoff       1                               AIXI.txt  \n",
       "3398       induction       1                               AIXI.txt  \n",
       "3399            with       1                               AIXI.txt  \n",
       "3400      sequential       1                               AIXI.txt  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntaxnet_out.loc[3350:3400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169449/169449 [00:01<00:00, 153838.40it/s]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "syntaxnet_out[\"word_id\"] -= 1\n",
    "syntaxnet_out[\"parent_id\"] -= 1\n",
    "\n",
    "cur_sentence_id = -1\n",
    "sentence_ids = []\n",
    "lemmas = []\n",
    "\n",
    "for row in tqdm(syntaxnet_out.itertuples(), total=len(syntaxnet_out)):\n",
    "    word_id, word, pos = row.word_id, row.word, row.POS\n",
    "    if word_id == 0:\n",
    "        cur_sentence_id += 1\n",
    "\n",
    "    if pos == \"VERB\":\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "    elif pos == \"ADJ\":\n",
    "        lemma = lemmatizer.lemmatize(word, pos='a')\n",
    "    elif pos == \"ADV\":\n",
    "        lemma = lemmatizer.lemmatize(word, pos='r')\n",
    "    else:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='n')\n",
    "\n",
    "    lemmas.append(lemma)\n",
    "    sentence_ids.append(cur_sentence_id)\n",
    "\n",
    "syntaxnet_out[\"sentence_id\"] = sentence_ids\n",
    "syntaxnet_out[\"lemma\"] = lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_names = os.listdir('./Data/Input/collection/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2doc = {}\n",
    "for i, doc in enumerate(document_ids):\n",
    "    sent2doc[i] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# doc_ids = []\n",
    "# doc_names = []\n",
    "# prev_sentence_id = 0\n",
    "# for sentence_id in syntaxnet_out[\"sentence_id\"]:\n",
    "#     if sentence_id != prev_sentence_id:\n",
    "#         i += 1\n",
    "#         prev_sentence_id = sentence_id\n",
    "#     doc_ids.append(document_ids[i])\n",
    "#     doc_names.append(document_names[doc_ids[-1]])\n",
    "# syntaxnet_out[\"doc_id\"] = doc_ids\n",
    "# syntaxnet_out[\"doc_name\"] = doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = []\n",
    "doc_names = []\n",
    "for row in syntaxnet_out.itertuples():\n",
    "    sentence_id = row.sentence_id\n",
    "    doc_ids.append(sent2doc[sentence_id])\n",
    "    doc_names.append(document_names[sent2doc[sentence_id]])\n",
    "    \n",
    "syntaxnet_out[\"doc_id\"] = doc_ids\n",
    "syntaxnet_out[\"doc_name\"] = doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "\n",
    "prev_doc_id = 0\n",
    "prev_sentence_id = 0\n",
    "prev_doc_name = \"\"\n",
    "cur_text = \"\"\n",
    "for row in syntaxnet_out.itertuples():\n",
    "    lemma = row.lemma\n",
    "    sentence_id, doc_id = row.sentence_id, row.doc_id\n",
    "    doc_name = row.doc_name\n",
    "\n",
    "    if sentence_id != prev_sentence_id:\n",
    "        cur_text += ' .'\n",
    "        prev_sentence_id = sentence_id\n",
    "    if doc_id != prev_doc_id:\n",
    "#         with open(args.lemmatize + '/' + doc_name, \"w\") as f:\n",
    "#             f.write(cur_text)\n",
    "        d[prev_doc_name] = cur_text\n",
    "        cur_text = \"\"\n",
    "        prev_doc_id = doc_id\n",
    "    # do not add space at start of document\n",
    "    if(len(cur_text) != 0):\n",
    "        cur_text += ' '\n",
    "    cur_text += lemma\n",
    "    prev_doc_name = doc_name\n",
    "    \n",
    "d[doc_name] = cur_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inferential theory of learn itl be an area of machine learning which describe inferential process perform by learn agent . itl ha been develop by ryszard s . michalski in s . in itl learning process is view a a search inference through hypothesis space guide by a specific goal . result of learn need to be store in order to be use in the future'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['Inferential_theory_of_learning.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>dependency</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169444</th>\n",
       "      <td>10</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>11</td>\n",
       "      <td>auxpass</td>\n",
       "      <td>8832</td>\n",
       "      <td>be</td>\n",
       "      <td>197</td>\n",
       "      <td>Inferential_theory_of_learning.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169445</th>\n",
       "      <td>11</td>\n",
       "      <td>used</td>\n",
       "      <td>VERB</td>\n",
       "      <td>6</td>\n",
       "      <td>advcl</td>\n",
       "      <td>8832</td>\n",
       "      <td>use</td>\n",
       "      <td>197</td>\n",
       "      <td>Inferential_theory_of_learning.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169446</th>\n",
       "      <td>12</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>14</td>\n",
       "      <td>case</td>\n",
       "      <td>8832</td>\n",
       "      <td>in</td>\n",
       "      <td>197</td>\n",
       "      <td>Inferential_theory_of_learning.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169447</th>\n",
       "      <td>13</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>14</td>\n",
       "      <td>det</td>\n",
       "      <td>8832</td>\n",
       "      <td>the</td>\n",
       "      <td>197</td>\n",
       "      <td>Inferential_theory_of_learning.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169448</th>\n",
       "      <td>14</td>\n",
       "      <td>future</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>11</td>\n",
       "      <td>nmod</td>\n",
       "      <td>8832</td>\n",
       "      <td>future</td>\n",
       "      <td>197</td>\n",
       "      <td>Inferential_theory_of_learning.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word_id    word   POS  parent_id dependency  sentence_id   lemma  \\\n",
       "169444       10      be   AUX         11    auxpass         8832      be   \n",
       "169445       11    used  VERB          6      advcl         8832     use   \n",
       "169446       12      in   ADP         14       case         8832      in   \n",
       "169447       13     the   DET         14        det         8832     the   \n",
       "169448       14  future  NOUN         11       nmod         8832  future   \n",
       "\n",
       "        doc_id                            doc_name  \n",
       "169444     197  Inferential_theory_of_learning.txt  \n",
       "169445     197  Inferential_theory_of_learning.txt  \n",
       "169446     197  Inferential_theory_of_learning.txt  \n",
       "169447     197  Inferential_theory_of_learning.txt  \n",
       "169448     197  Inferential_theory_of_learning.txt  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntaxnet_out.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>dependency</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>1</td>\n",
       "      <td>case</td>\n",
       "      <td>0</td>\n",
       "      <td>in</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>machine</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>2</td>\n",
       "      <td>nmod</td>\n",
       "      <td>0</td>\n",
       "      <td>machine</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>learning</td>\n",
       "      <td>VERB</td>\n",
       "      <td>9</td>\n",
       "      <td>csubj</td>\n",
       "      <td>0</td>\n",
       "      <td>learn</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>5</td>\n",
       "      <td>det</td>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>kernel</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>5</td>\n",
       "      <td>compound</td>\n",
       "      <td>0</td>\n",
       "      <td>kernel</td>\n",
       "      <td>0</td>\n",
       "      <td>Kernel_embedding_of_distributions.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_id      word   POS  parent_id dependency  sentence_id    lemma  \\\n",
       "0        0        in   ADP          1       case            0       in   \n",
       "1        1   machine  NOUN          2       nmod            0  machine   \n",
       "2        2  learning  VERB          9      csubj            0    learn   \n",
       "3        3       the   DET          5        det            0      the   \n",
       "4        4    kernel  NOUN          5   compound            0   kernel   \n",
       "\n",
       "   doc_id                               doc_name  \n",
       "0       0  Kernel_embedding_of_distributions.txt  \n",
       "1       0  Kernel_embedding_of_distributions.txt  \n",
       "2       0  Kernel_embedding_of_distributions.txt  \n",
       "3       0  Kernel_embedding_of_distributions.txt  \n",
       "4       0  Kernel_embedding_of_distributions.txt  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntaxnet_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kernel_embedding_of_distributions.txt',\n",
       " 'AIXI.txt',\n",
       " 'Explanationbased_learning.txt',\n",
       " 'Algorithm_selection.txt',\n",
       " 'MTheory_learning_framework.txt',\n",
       " 'Overfitting.txt',\n",
       " 'Cognitive_robotics.txt',\n",
       " 'Movidius.txt',\n",
       " 'Crossentropy_method.txt',\n",
       " 'Crossvalidation_statistics.txt']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in machine learning the kernel embedding of distributions also called the kernel mean or mean map comprises a class of nonparametric methods in which a probability distribution is represented as an element of a reproducing kernel hilbert space rkhs a generalization of the individual datapoint feature mapping done in classical kernel methods the embedding of distributions into infinitedimensional feature spaces can preserve all of the statistical features of arbitrary distributions while allowing one to compare and manipulate distributions using hilbert space operations such as inner products distances projections linear transformations and spectral analysis this learning framework is very general and can be applied to distributions over any space formula on which a sensible kernel function measuring similarity between elements of formula may be defined for example various kernels have been proposed for learning from data which are vectors in formula discrete classescategories strings graphsnetworks images time series manifolds dynamical systems and other structured objects the theory behind kernel embeddings of distributions has been primarily developed by alex smola le song arthur gretton and bernhard schlkopf a review of recent works on kernel embedding of distributions can be found in the analysis of distributions is fundamental in machine learning and statistics and many algorithms in these fields rely on information theoretic approaches such as entropy mutual information or kullbackleibler divergence however to estimate these quantities one must first either perform density estimation or employ sophisticated spacepartitioningbiascorrection strategies which are typically infeasible for highdimensional data commonly methods for modeling complex distributions rely on parametric assumptions that may be unfounded or computationally challenging e gaussian mixture models while nonparametric methods like kernel density estimation note the smoothing kernels in this context have a different interpretation than the kernels discussed here or characteristic function representation via the fourier transform of the distribution break down in highdimensional settings methods based on the kernel embedding of distributions sidestep these problems and also possess the following advantages thus learning via the kernel embedding of distributions offers a principled dropin replacement for information theoretic approaches and is a framework which not only subsumes many popular methods in machine learning and statistics as special cases but also can lead to entirely new learning algorithms let formula denote a random variable with codomain formula and distribution formula given a kernel formula on formula the moorearonszajn theorem asserts the existence of a rkhs formula a hilbert space of functions formula equipped with inner products formula and norms formula in which the element formula satisfies the reproducing property formula one may alternatively consider formula an implicit feature mapping formula from formula to formula which is therefore also called the feature space so that formula can be viewed as a measure of similarity between points formula while the similarity measure is linear in the feature space it may be highly nonlinear in the original space depending on the choice of kernel the kernel embedding of the distribution formula in formula also called the kernel mean or mean map is given by if formula allows a square integrable density formula then formula where formula is the hilbertschmidt integral operator a kernel is characteristic if the mean embedding formula is injective each distribution can thus be uniquely represented in the rkhs and all statistical features of distributions are preserved by the kernel embedding if a characteristic kernel is used given formula training examples formula drawn independently and identically distributed i from formula the kernel embedding of formula can be empirically estimated as if formula denotes another random variable for simplicity assume the codomain of formula is also formula with the same kernel formula which satisfies formula then the joint distribution formula can be mapped into a tensor product feature space formula via by the equivalence between a tensor and a linear map this joint embedding may be interpreted as an uncentered crosscovariance operator formula from which the crosscovariance of meanzero functions formula can be computed as given formula pairs of training examples formula drawn i from formula we can also empirically estimate the joint distribution kernel embedding via given a conditional distribution formula one can define the corresponding rkhs embedding as note that the embedding of formula thus defines a family of points in the rkhs indexed by the values formula taken by conditioning variable formula by fixing formula to a particular value we obtain a single element in formula and thus it is natural to define the operator which given the feature mapping of formula outputs the conditional embedding of formula given formula assuming that for all formula it can be shown that this assumption is always true for finite domains with characteristic kernels but may not necessarily hold for continuous domains nevertheless even in cases where the assumption fails formula may still be used to approximate the conditional kernel embedding formula and in practice the inversion operator is replaced with a regularized version of itself formula where formula denotes the identity matrix given training examples formula the empirical kernel conditional embedding operator may be estimated as where formula are implicitly formed feature matrices formula is the gram matrix for samples of formula and formula is a regularization parameter needed to avoid overfitting thus the empirical estimate of the kernel conditional embedding is given by a weighted sum of samples of formula in the feature space on compact subsets of formula is universal and support of formula is an entire space then formula is universal for example gaussian rbf is universal sinc kernel is not universal this section illustrates how basic probabilistic rules may be reformulated as multilinear algebraic operations in the kernel embedding framework and is primarily based on the work of song et al the following notation is adopted in practice all embeddings are empirically estimated from data formula and it assumed that a set of samples formula may be used to estimate the kernel embedding of the prior distribution formula in probability theory the marginal distribution of formula can be computed by integrating out formula from the joint density including the prior distribution on formula the analog of this rule in the kernel embedding framework states that formula the rkhs embedding of formula can be computed via in practical implementations the kernel sum rule takes the following form where formula is the empirical kernel embedding of the prior distribution formula formula and formula are gram matrices with entries formula respectively in probability theory a joint distribution can be factorized into a product between conditional and marginal distributions the analog of this rule in the kernel embedding framework states that formula the joint embedding of formula can be factorized as a composition of conditional embedding operator with the autocovariance operator associated with formula in practical implementations the kernel chain rule takes the following form in probability theory a posterior distribution can be expressed in terms of a prior distribution and a likelihood function as the analog of this rule in the kernel embedding framework expresses the kernel embedding of the conditional distribution in terms of conditional embedding operators which are modified by the prior distribution in practical implementations the kernel bayes rule takes the following form where formula two regularization parameters are used in this framework formula for the estimation of formula and formula for the estimation of the final conditional embedding operator formula the latter regularization is done on square of formula because formula may not be positive definite the maximum mean discrepancy mmd is a distancemeasure between distributions formula and formula which is defined as the squared distance between their embeddings in the rkhs while most distancemeasures between distributions such as the widely used kullbackleibler divergence either require density estimation either parametrically or nonparametrically or space partitioningbias correction strategies the mmd is easily estimated as an empirical mean which is concentrated around the true value of the mmd the characterization of this distance as the maximum mean discrepancy refers to the fact that computing the mmd is equivalent to finding the rkhs function that maximizes the difference in expectations between the two probability distributions given n training examples from formula and m samples from formula one can formulate a test statistic based on the empirical estimate of the mmd to obtain a twosample test of the  hypothesis that both samples stem from the same distribution i formula against the broad alternative formula although learning algorithms in the kernel embedding framework circumvent the need for intermediate density estimation one may nonetheless use the empirical embedding to perform density estimation based on n samples drawn from an underlying distribution formula this can be done by solving the following optimization problem where the maximization is done over the entire space of distributions on formula here formula is the kernel embedding of the proposed density formula and formula is an entropylike quantity e entropy kl divergence bregman divergence the distribution which solves this optimization may be interpreted as a compromise between fitting the empirical kernel means of the samples well while still allocating a substantial portion of the probability mass to all regions of the probability space much of which may not be represented in the training examples in practice a good approximate solution of the difficult optimization may be found by restricting the space of candidate densities to a mixture of m candidate distributions with regularized mixing proportions connections between the ideas underlying gaussian processes and conditional random fields may be drawn with the estimation of conditional probability distributions in this fashion if one views the feature mappings associated with the kernel as sufficient statistics in generalized possibly infinitedimensional exponential families a measure of the statistical dependence between random variables formula and formula from any domains on which sensible kernels can be defined can be formulated based on the hilbertschmidt independence criterion and can be used as a principled replacement for mutual information pearson correlation or any other dependence measure used in learning algorithms most notably hsic can detect arbitrary dependencies when a characteristic kernel is used in the embeddings hsic is zero if and only if the variables are independent and can be used to measure dependence between different types of data e images and text captions given n i samples of each random variable a simple parameterfree unbiased estimator of hsic which exhibits concentration about the true value can be computed in formula time where the gram matrices of the two datasets are approximated using formula with formula the desirable properties of hsic have led to the formulation of numerous algorithms which utilize this dependence measure for a variety of common machine learning tasks such as feature selection bahsic clustering cluhsic and dimensionality reduction muhsic hsic can be extended to measure the dependence of multiple random variables the question of when hsic captures independence in this case has recently been studied for more than two variables belief propagation is a fundamental algorithm for inference in graphical models in which nodes repeatedly pass and receive messages corresponding to the evaluation of conditional expectations in the kernel embedding framework the messages may be represented as rkhs functions and the conditional distribution embeddings can be applied to efficiently compute message updates given n samples of random variables represented by nodes in a markov random field the incoming message to node t from node u can be expressed as formula if it assumed to lie in the rkhs the kernel belief propagation update message from t to node s is then given by where formula denotes the elementwise vector product formula is the set of nodes connected to t excluding node s formula formula are the gram matrices of the samples from variables formula respectively and formula is the feature matrix for the samples from formula thus if the incoming messages to node t are linear combinations of feature mapped samples from formula then the outgoing message from this node is also a linear combination of feature mapped samples from formula this rkhs function representation of messagepassing updates therefore produces an efficient belief propagation algorithm in which the potentials are nonparametric functions inferred from the data so that arbitrary statistical relationships may be modeled in the hidden markov model hmm two key quantities of interest are the transition probabilities between hidden states formula and the emission probabilities formula for observations using the kernel conditional distribution embedding framework these quantities may be expressed in terms of samples from the hmm a serious limitation of the embedding methods in this domain is the need for training samples containing hidden states as otherwise inference with arbitrary distributions in the hmm is not possible one common use of hmms is filtering in which the goal is to estimate posterior distribution over the hidden state formula at time step t given a history of previous observations formula from the system in filtering a belief state formula is recursively maintained via a prediction step where updates formula are computed by marginalizing out the previous hidden state followed by a conditioning step where updates formula are computed by applying bayes rule to condition on a new observation the rkhs embedding of the belief state at time t can be recursively expressed as by computing the embeddings of the prediction step via the kernel sum rule and the embedding of the conditioning step via kernel bayes rule assuming a training sample formula is given one can in practice estimate formula and filtering with kernel embeddings is thus implemented recursively using the following updates for the weights formula where formula denote the gram matrices of formula and formula respectively formula is a transfer gram matrix defined as formula and formula the support measure machine smm is a generalization of the support vector machine svm in which the training examples are probability distributions paired with labels formula smms solve the standard svm dual optimization problem using the following expected kernel which is computable in closed form for many common specific distributions formula such as the gaussian distribution combined with popular embedding kernels formula e the gaussian kernel or polynomial kernel or can be accurately empirically estimated from i samples formula via under certain choices of the embedding kernel formula the smm applied to training examples formula is equivalent to a svm trained on samples formula and thus the smm can be viewed as a flexible svm in which a different datadependent kernel specified by the assumed form of the distribution formula may be placed on each training point the goal of domain adaptation is the formulation of learning algorithms which generalize well when the training and test data have different distributions given training examples formula and a test set formula where the formula are unknown three types of differences are commonly assumed between the distribution of the training examples formula and the test distribution formula by utilizing the kernel embedding of marginal and conditional distributions practical approaches to deal with the presence of these types of differences between training and test domains can be formulated covariate shift may be accounted for by reweighting examples via estimates of the ratio formula obtained directly from the kernel embeddings of the marginal distributions of formula in each domain without any need for explicit estimation of the distributions target shift which cannot be similarly dealt with since no samples from formula are available in the test domain is accounted for by weighting training examples using the vector formula which solves the following optimization problem where in practice empirical approximations must be used to deal with location scale conditional shift one can perform a ls transformation of the training points to obtain new transformed training data formula where formula denotes the elementwise vector product to ensure similar distributions between the new transformed training samples and the test data formula are estimated by minimizing the following empirical kernel embedding distance in general the kernel embedding methods for dealing with ls conditional shift and target shift may be combined to find a reweighted transformation of the training data which mimics the test distribution and these methods may perform well even in the presence of conditional shifts other than locationscale changes given n sets of training examples sampled i from distributions formula the goal of domain generalization is to formulate learning algorithms which perform well on test examples sampled from a previously unseen domain formula where no data from the test domain is available at training time if conditional distributions formula are assumed to be relatively similar across all domains then a learner capable of domain generalization must estimate a functional relationship between the variables which is robust to changes in the marginals formula based on kernel embeddings of these distributions domain invariant component analysis dica is a method which determines the transformation of the training data that minimizes the difference between marginal distributions while preserving a common conditional distribution shared between all training domains dica thus extracts invariants features that transfer across domains and may be viewed as a generalization of many popular dimensionreduction methods such as kernel principal component analysis transfer component analysis and covariance operator inverse regression defining a probability distribution formula on the rkhs formula with formula dica measures dissimilarity between domains via distributional variance which is computed as so formula is a formula gram matrix over the distributions from which the training data are sampled finding an orthogonal transform onto a lowdimensional subspace b in the feature space which minimizes the distributional variance dica simultaneously ensures that b aligns with the bases of a central subspace c for which formula becomes independent of formula given formula across all domains in the absence of target values formula an unsupervised version of dica may be formulated which finds a lowdimensional subspace that minimizes distributional variance while simultaneously maximizing the variance of formula in the feature space across all domains rather than preserving a central subspace in distribution regression the goal is to regress from probability distributions to reals or vectors many important machine learning and statistical tasks fit into this framework including multiinstance learning and point estimation problems without analytical solution such as hyperparameter or entropy estimation in practice only samples from sampled distributions are observable and the estimates have to rely on similarities computed between sets of points distribution regression has been successfully applied for example in supervised entropy learning and aerosol prediction using multispectral satellite images given formula training data where the formula bag contains samples from a probability distribution formula and the formula output label is formula one can tackle the distribution regression task by taking the embeddings of the distributions and learning the regressor from the embeddings to the outputs in other words one can consider the following kernel ridge regression problem formula where formula with a formula kernel on the domain of formulas formula formula is a kernel on the embedded distributions and formula is the rkhs determined by formula examples for formula include the linear kernel formula the gaussian kernel formula the exponential kernel formula the cauchy kernel formula the generalized tstudent kernel formula or the inverse multiquadrics kernel formula the prediction on a new distribution formula takes the simple analytical form where formula formula formula formula under mild regularity conditions this estimator can be shown to be consistent and it can achieve the onestage sampled as if one had access to the true formulas minimax optimal rate in the formula objective function formulas are real numbers the results can also be extended to the case when formulas are formuladimensional vectors or more generally elements of a separable hilbert space using operatorvalued formula kernels in this simple example which is taken from song et al formula are assumed to be discrete random variables which take values in the set formula and the kernel is chosen to be the kronecker delta function so formula the feature map corresponding to this kernel is the standard basis vector formula the kernel embeddings of such a distributions are thus vectors of marginal probabilities while the embeddings of joint distributions in this setting are formula matrices specifying joint probability tables and the explicit form of these embeddings is the conditional distribution embedding operator formula is in this setting a conditional probability table thus the embeddings of the conditional distribution under a fixed value of formula may be computed as in this discretevalued setting with the kronecker delta kernel the kernel sum rule becomes the kernel chain rule in this case is given by\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(list(syntaxnet_out[syntaxnet_out['doc_name'] == 'Kernel_embedding_of_distributions.txt']['word'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

large margin nearest neighbor lmnn classification be a statistical machine learn algorithm for metric learning . it learn a pseudometric design for knearest neighbor classification . the algorithm is base on semidefinite programming a subclass of convex optimization . the goal of supervise learning more specifically classification be to learn a decision rule that can categorize data instances into predefined class . the knearest neighbor rule assume a training data set of label instance i . the class be known . it classify a new data instance with the class obtain from the majority vote of the k close label train instance . closeness is measure with a predefined metric . large margin nearest neighbor be an algorithm that learn this global pseudometric in a supervise fashion to improve the classification accuracy of the knearest neighbor rule . the main intuition behind lmnn be to learn a pseudometric under which all data instance in the training set are surround by at least k instance that share the same class label . if this is achieve the leaveoneout error a special case of cross validation is minimize . let the training data consist of a data set formula where the set of possible class category be formula . the algorithm learn a pseudometric of the type for formula to be well define the matrix formula need to be positive semidefinite . the euclidean metric be a special case where formula be the identity matrix . this generalization is often falsely refer to a mahalanobis metric . figure illustrate the effect of the metric under vary formula . the two circle show the set of point with equal distance to the center formula . in the euclidean case this set be a circle whereas under the modified mahalanobis metric it become an ellipsoid . the algorithm distinguish between two type of special data point target neighbor and impostor . target neighbor are select before learn . each instance formula have exactly formula different target neighbor within formula which all share the same class label formula . the target neighbor be the data point that should become near neighbor under the learned metric . let u denote the set of target neighbor for a data point formula a formula . an impostor of a data point formula be another data point formula with a different class label i . formula which be one of the near neighbor of formula . during learn the algorithm try to minimize the number of impostor for all data instance in the training set . large margin nearest neighbor optimize the matrix formula with the help of semidefinite programming . the objective is twofold for every data point formula the target neighbor should be close and the impostor should be far away . figure show the effect of such an optimization on an illustrative example . the learned metric cause the input vector formula to be surround by train instance of the same class . if it be a test point it would be classify correctly under the formula near neighbor rule . the first optimization goal is achieve by minimize the average distance between instance and their target neighborsthe second goal is achieve by constrain impostor formula to be one unit far away than target neighbor formula and therefore push them out of the local neighborhood of formula . the resulting inequality constraint can be state asthe margin of exactly one unit fix the scale of the matrix formula . any alternative choice formula would result in a rescaling of formula by a factor of formula . the final optimization problem becomeshere the slack variable formula absorb the amount of violation of the impostor constraint . their overall sum be minimized . the last constraint ensures that formula be positive semidefinite . the optimization problem be an instance of semidefinite programming sdp . although sdps tend to suffer from high computational complexity this particular sdp instance can be solve very efficiently due to the underlying geometric property of the problem . in particular most impostor constraint are naturally satisfy and do not need to be enforce during runtime . a particularly well suit solver technique be the working set method which keep a small set of constraint that are actively enforce and monitor the remain likely satisfy constraint only occasionally to ensure correctness . lmnn wa extend to multiple local metric in the paper . this extension significantly improve the classification error but involve a more expensive optimization problem . in their publication in the journal of machine learn research weinberger and saul derive an efficient solver for the semidefinite program . it can learn a metric for the mnist handwritten digit data set in several hour involve billion of pairwise constraint . an open source matlab implementation be freely available at the author web page . kumal et al . extend the algorithm to incorporate local invariance to multivariate polynomial transformation and improve regularization .
proximal gradient forward backward splitting method for learn be an area of research in optimization and statistical learning theory which study algorithm for a general class of convex regularization problem where the regularization penalty may not be differentiable . one such example be formula regularization also know a lasso of the formproximal gradient method offer a general framework for solve regularization problem from statistical learning theory with penalty that are tailor to a specific problem application . such customized penalty can help to induce certain structure in problem solution such a sparsity in the case of lasso or group structure in the case of group lasso . proximal gradient method be applicable in a wide variety of scenario for solving convex optimization problem of the formwhere formula be convex and differentiable with lipschitz continuous gradient formula be a convex lower semicontinuous function which be possibly nondifferentiable and formula be some set typically a hilbert space . the usual criterion of formula minimizes formula if and only if formula in the convex differentiable setting is now replace bywhere formula denote the subdifferential of a realvalued convex function formula . give a convex function formula an important operator to consider be it proximity operator formula define bywhich is welldefined because of the strict convexity of the formula norm . the proximity operator can be see a a generalization of a projection . we see that the proximity operator be important because formula be a minimizer to the problem formula if and only ifone important technique relate to proximal gradient method be the moreau decomposition which decompose the identity operator a the sum of two proximity operator . namely let formula be a low semicontinuous convex function on a vector space formula . we define it fenchel conjugate formula to be the functionthe general form of moreaus decomposition state that for any formula and any formula thatwhich for formula implies that formula . the moreau decomposition can be see to be a generalization of the usual orthogonal decomposition of a vector space analogous with the fact that proximity operator be generalization of projection . in certain situation it may be easy to compute the proximity operator for the conjugate formula instead of the function formula and therefore the moreau decomposition can be apply . this be the case for group lasso . consider the regularize empirical risk minimization problem with square loss and with the formula norm a the regularization penaltywhere formula the formula regularization problem be sometimes referred to a lasso least absolute shrinkage and selection operator . such formula regularization problem are interest because they induce sparse solution that be solution formula to the minimization problem have relatively few nonzero component . lasso can be see to be a convex relaxation of the nonconvex problemwhere formula denote the formula norm which be the number of nonzero entry of the vector formula . sparse solution be of particular interest in learn theory for interpretability of result a sparse solution can identify a small number of important factor . for simplicity we restrict our attention to the problem where formula . to solve the problemwe consider our objective function in two part a convex differentiable term formula and a convex function formula . note that formula be not strictly convex . let u compute the proximity operator for formula . first we find an alternative characterization of the proximity operator formula a followsformulafor formula it be easy to compute formula the formulath entry of formula is preciselyusing the recharacterization of the proximity operator give above for the choice of formula and formula we have that formula is define entrywise bywhich is know a the soft thresholding operator formula . to finally solve the lasso problem we consider the fixed point equation shown earliergiven that we have compute the form of the proximity operator explicitly then we can define a standard fixed point iteration procedure . namely fix some initial formula and for formula definenote here the effective tradeoff between the empirical error term formula and the regularization penalty formula . this fixed point method ha decouple the effect of the two different convex function which comprise the objective function into a gradient descent step formula and a soft thresholding step via formula . convergence of this fixed point scheme is wellstudied in the literature and is guarantee under appropriate choice of step size formula and loss function such a the square loss take here . accelerate method were introduce by nesterov in which improve the rate of convergence under certain regularity assumption on formula . such method have been study extensively in previous year . for more general learn problem where the proximity operator cannot be compute explicitly for some regularization term formula such fix point scheme can still be carry out use approximation to both the gradient and the proximity operator . there have be numerous development within the past decade in convex optimization technique which have influence the application of proximal gradient method in statistical learn theory . here we survey a few important topic which can greatly improve practical algorithmic performance of these method . in the fixed point iteration schemeone can allow variable step size formula instead of a constant formula . numerous adaptive step size scheme have been propose throughout the literature . application of these scheme suggest that these can offer substantial improvement in number of iteration require for fix point convergence . elastic net regularization offer an alternative to pure formula regularization . the problem of lasso formula regularization involve the penalty term formula which is not strictly convex . hence solution to formula where formula be some empirical loss function need not be unique . this is often avoid by the inclusion of an additional strictly convex term such a an formula norm regularization penalty . for example one can consider the problemwhere formulafor formula the penalty term formula be now strictly convex and hence the minimization problem now admit a unique solution . it ha been observe that for sufficiently small formula the additional penalty term formula act a a preconditioner and can substantially improve convergence while not adversely affect the sparsity of solution . proximal gradient method provide a general framework which be applicable to a wide variety of problem in statistical learn theory . certain problem in learn can often involve data which have additional structure that is know a priori . in the past several year there have be new development which incorporate information about group structure to provide method which are tailor to different application . here we survey a few such method . group lasso be a generalization of the lasso method when feature are group into disjoint block . suppose the feature are group into block formula . here we take a a regularization penaltywhich be the sum of the formula norm on correspond feature vector for the different group . a similar proximity operator analysis a above can be use to compute the proximity operator for this penalty . where the lasso penalty have a proximity operator which be soft thresholding on each individual component the proximity operator for the group lasso be soft thresholding on each group . for the group formula we have that proximity operator of formula is give bywhere formula be the formulath group . in contrast to lasso the derivation of the proximity operator for group lasso rely on the moreau decomposition . here the proximity operator of the conjugate of the group lasso penalty become a projection onto the ball of a dual norm . in contrast to the group lasso problem where feature are group into disjoint block it may be the case that group feature are overlap or have a nest structure . such generalization of group lasso have been consider in a variety of context . for overlap group one common approach is know a latent group lasso which introduce latent variable to account for overlap . nest group structure are study in hierarchical structure prediction and with direct acyclic graph .
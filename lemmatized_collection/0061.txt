in statistical learning theory a learnable function class be a set of function for which an algorithm can be devise to asymptotically minimize the expect risk uniformly over all probability distribution . the concept of learnable class are closely relate to regularization in machine learn and provides large sample justification for certain learn algorithm . let formula be the sample space where formula be the label and formula be the covariates predictor . formula be a collection of mappings function under consideration to link formula to formula . formula be a pregiven loss function usually nonnegative . give a probability distribution formula on formula define the expected risk formula to bethe general goal in statistical learning be to find the function in formula that minimize the expected risk . that be to find solution to the follow problembut in practice the distribution formula be unknown and any learning task can only be base on finite sample . thus we seek instead to find an algorithm that asymptotically minimize the empirical risk i . to find a sequence of function formula that satisfiesone usual algorithm to find such a sequence be through empirical risk minimization . we can make the condition give in the above equation stronger by require that the convergence be uniform for all probability distribution . that isthe intuition behind the more strict requirement be a such the rate at which sequence formula converges to the minimizer of the expected risk can be very different for different formula . because in real world the true distribution formula be always unknown we would want to select a sequence that performs well under all case . however by the no free lunch theorem such a sequence that satisfies doe not exist if formula be too complex . this mean we need to be careful and not allow too many function in formula if we want to be a meaningful requirement . specifically function class that ensure the existence of a sequence formula that satisfies be know as learnable class . it be worth noting that at least for supervise classification and regression problem if a function class be learnable then the empirical risk minimization automatically satisfy . thus in these setting not only do we know that the problem pose by be solvable we also immediately have an algorithm that give the solution . if the true relationship between formula and formula be formula then by select the appropriate loss function formula can always be expressed a the minimizer of the expected loss across all possible function . that ishere we let formula be the collection of all possible function map formula onto formula . formula can be interpret a the actual data generating mechanism . however the no free lunch theorem tell u that in practice with finite sample we cannot hope to search for the expected risk minimizer over formula . thus we often consider a subset of formula formula to carry out search on . by do so we risk that formula might not be an element of formula . this tradeoff can be mathematically express asin the above decomposition part formula doe not depend on the data and be nonstochastic . it describe how far away our assumption formula be from the truth formula . formula will be strictly great than if we make assumption that be too strong formula too small . on the other hand fail to put enough restriction on formula will cause it to be not learnable and part formula will not stochastically converge to . this be the wellknown overfitting problem in statistic and machine learn literature . a good example where learnable class are use be the socalled tikhonov regularization in reproduce kernel hilbert space rkhs . specifically let formula be an rkhs and formula be the norm on formula give by it inner product . it be shown in that formula be a learnable class for any finite positive formula . the empirical minimization algorithm to the dual form of this problem isthis wa first introduce by tikhonov to solve illposed problem . many statistical learn algorithm can be express in such a form for example the wellknown ridge regression . the tradeoff between formula and formula in be geometrically more intuitive with tikhonov regularization in rkhs . we can consider a sequence of formula which be essentially ball in formula with center at . a formula get large formula get closer to the entire space and formula be likely to become small . however we will also suffer small convergence rate in formula . the way to choose an optimal formula in finite sample setting be usually through crossvalidation . part formula in is closely link to empirical process theory in statistic where the empirical risk formula are know a empirical process . in this field the function class formula that satisfy the stochastic convergenceare know a uniform glivenkocantelli class . it ha been show that under certain regularity condition learnable class and uniformly glivenkocantelli class be equivalent . interplay between formula and formula in statistic literature is often know a the biasvariance tradeoff . however note that in the author give an example of stochastic convex optimization for general setting of learn where learnability be not equivalent with uniform convergence .
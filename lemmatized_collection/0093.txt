stability also known a algorithmic stability be a notion in computational learning theory of how a machine learn algorithm is perturb by small change to it input . a stable learn algorithm be one for which the prediction doe not change much when the training data is modify slightly . for instance consider a machine learn algorithm that is being train to recognize handwritten letter of the alphabet using example of handwritten letter and their label a to z a a training set . one way to modify this training set be to leave out an example so that only example of handwritten letter and their label be available . a stable learning algorithm would produce a similar classifier with both the element and element train set . stability can be study for many type of learn problem from language learn to inverse problem in physic and engineer a it be a property of the learning process rather than the type of information being learn . the study of stability gain importance in computational learning theory in the s when it wa show to have a connection with generalization . it wa show that for large class of learn algorithm notably empirical risk minimization algorithms certain type of stability ensure good generalization . a central goal in design a machine learning system be to guarantee that the learning algorithm will generalize or perform accurately on new example after be train on a finite number of them . in the s milestone were reach in obtain generalization bound for supervise learn algorithm . the technique historically use to prove generalization be to show that an algorithm be consistent use the uniform convergence property of empirical quantity to their mean . this technique wa use to obtain generalization bound for the large class of empirical risk minimization erm algorithm . an erm algorithm be one that select a solution from a hypothesis space formula in such a way to minimize the empirical error on a training set formula . a general result prove by vladimir vapnik for an erm binary classification algorithm be that for any target function and input distribution any hypothesis space formula with vcdimension formula and formula training example the algorithm be consistent and will produce a training error that be at most formula plus logarithmic factor from the true train error . the result be late extend to almosterm algorithm with function class that do not have unique minimizers . vapniks work use what became know a vc theory establish a relationship between generalization of a learning algorithm and property of the hypothesis space formula of function being learn . however these result could not be apply to algorithm with hypothesis space of unbounded vcdimension . put another way these result could not be apply when the information being learn have a complexity that be too large to measure . some of the simple machine learn algorithmsfor instance for regressionhave hypothesis space with unbounded vcdimension . another example be language learn algorithm that can produce sentence of arbitrary length . stability analysis be developed in the s for computational learning theory and be an alternative method for obtain generalization bound . the stability of an algorithm be a property of the learning process rather than a direct property of the hypothesis space formula and it can be assess in algorithm that have hypothesis space with unbounded or undefined vcdimension such a near neighbor . a stable learn algorithm be one for which the learned function doe not change much when the training set is slightly modify for instance by leave out an example . a measure of leave one out error is use in a cross validation leave one out cvloo algorithm to evaluate a learning algorithm stability with respect to the loss function . a such stability analysis be the application of sensitivity analysis to machine learn . we define several term related to learn algorithm train set so that we can then define stability in multiple way and present theorem from the field . a machine learn algorithm also know a a learning map formula map a training data set which be a set of label example formula onto a function formula from formula to formula where formula and formula be in the same space of the training example . the function formula are select from a hypothesis space of function call formula . the training set from which an algorithm learns is define asformulaand be of size formula in formuladrawn i . from an unknown distribution d . thus the learn map formula is define a a map from formula into formula map a training set formula onto a function formula from formula to formula . here we consider only deterministic algorithm where formula be symmetric with respect to formula i . it doe not depend on the order of the element in the training set . furthermore we assume that all function be measurable and all set be countable . the loss formula of a hypothesis formula with respect to an example formula is then define a formula . the empirical error of formula be formula . the true error of formula be formulagiven a training set s of size m we will build for all i . m modify training set a followsformulaformulaan algorithm formula have hypothesis stability with respect to the loss function v if the follow holdsformulaan algorithm formula have pointwise hypothesis stability with respect to the loss function v if the follow holdsformulaan algorithm formula have error stability with respect to the loss function v if the follow holdsformulaan algorithm formula have uniform stability with respect to the loss function v if the follow holdsformulaa probabilistic version of uniform stability isformulaan algorithm be say to be stable when the value of formula decrease a formula . an algorithm formula have cvloo stability with respect to the loss function v if the follow holdsformulathe definition of cvloo stability be equivalent to pointwisehypothesis stability seen earlier . an algorithm formula have formula stability if for each n there exist a formula and a formula such thatformula with formula and formula go to zero for formulafrom bousquet and elisseeff for symmetric learn algorithm with bound loss if the algorithm have uniform stability with the probabilistic definition above then the algorithm generalizes . uniform stability be a strong condition which is not meet by all algorithm but be surprisingly met by the large and important class of regularization algorithm . the generalization bound is give in the article . from mukherjee et al . this be an important result for the foundation of learn theory because it show that two previously unrelated property of an algorithm stability and consistency be equivalent for erm and certain loss function . the generalization bound is give in the article . this be a list of algorithm that have been show to be stable and the article where the associated generalization bound be provided .
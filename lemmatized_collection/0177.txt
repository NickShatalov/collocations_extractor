in statistic and machine learn the biasvariance tradeoff be the property of a set of predictive model whereby model with a lower bias in parameter estimation have a high variance of the parameter estimate across sample and vice versa . the biasvariance dilemma or problem be the conflict in try to simultaneously minimize these two source of error that prevent supervise learn algorithm from generalize beyond their training setthe biasvariance decomposition be a way of analyze a learning algorithm expected generalization error with respect to a particular problem a a sum of three term the bias variance and a quantity call the irreducible error result from noise in the problem itself . this tradeoff apply to all form of supervise learning classification regression function fitting and structured output learn . it ha also been invoke to explain the effectiveness of heuristic in human learn . the biasvariance tradeoff be a central problem in supervised learn . ideally one want to choose a model that both accurately capture the regularity in it training data but also generalize well to unseen data . unfortunately it be typically impossible to do both simultaneously . highvariance learning method may be able to represent their training set well but are at risk of overfitting to noisy or unrepresentative training data . in contrast algorithm with high bias typically produce simple model that dont tend to overfit but may underfit their training data fail to capture important regularity . model with low bias be usually more complex e . higherorder regression polynomial enable them to represent the train set more accurately . in the process however they may also represent a large noise component in the training set make their prediction le accurate despite their added complexity . in contrast model with high bias tend to be relatively simple loworder or even linear regression polynomial but may produce low variance prediction when apply beyond the training set . suppose that we have a training set consisting of a set of point formula and real value formula associate with each point formula . we assume that there be a function with noise formula where the noise formula have zero mean and variance formula . we want to find a function formula that approximate the true function formula as well a possible by mean of some learn algorithm . we make as well a possible precise by measure the mean square error between formula and formula we want formula to be minimal both for formula and for point outside of our sample . of course we cannot hope to do so perfectly since the formula contain noise formula this mean we must be prepare to accept an irreducible error in any function we come up with . find an formula that generalizes to point outside of the training set can be do with any of the countless algorithm use for supervised learning . it turn out that whichever function formula we select we can decompose it expect error on an unseen sample formula a followswhereandthe expectation range over different choice of the training set formula all sample from the same joint distribution formula . the three term representthe more complex the model formula be the more data point it will capture and the lower the bias will be . however complexity will make the model move more to capture the data point and hence it variance will be large . the derivation of the biasvariance decomposition for squared error proceeds a follows . for notational convenience abbreviate formula and formula . first recall that by definition for any random variable formula we haverearranging we getsince formula be deterministicthis give formula and formula implies formula . also since formulathus since formula and formula be independent we can writethe biasvariance decomposition form the conceptual basis for regression regularization method such a lasso and ridge regression . regularization method introduce bias into the regression solution that can reduce variance considerably relative to the ols solution . although the ols solution provides nonbiased regression estimate the low variance solution produce by regularization technique provide superior mse performance . the biasvariance decomposition be originally formulated for leastsquares regression . for the case of classification under the loss misclassification rate it be possible to find a similar decomposition . alternatively if the classification problem can be phrase a probabilistic classification then the expect squared error of the predict probability with respect to the true probability can be decompose as before . dimensionality reduction and feature selection can decrease variance by simplify model . similarly a large training set tend to decrease variance . add feature predictor tend to decrease bias at the expense of introduce additional variance . learn algorithm typically have some tunable parameter that control bias and variance for exampleone way of resolve the tradeoff be to use mixture model and ensemble learn . for example boost combine many weak high bias model in an ensemble that have low bias than the individual model while bagging combine strong learner in a way that reduce their variance . in the case of nearest neighbor regression a closedform expression exists that relate the biasvariance decomposition to the parameter where formula be the near neighbor of in the training set . the bias first term be a monotone rising function of while the variance second term drop off a is increase . in fact under reasonable assumption the bias of the firstnearest neighbor nn estimator vanish entirely a the size of the training set approach infinity . while widely discuss in the context of machine learn the biasvariance dilemma ha been examine in the context of human cognition most notably by gerd gigerenzer and coworkers in the context of learn heuristic . they have argue see reference below that the human brain resolve the dilemma in the case of the typically sparse poorlycharacterised trainingsets provide by experience by adopt highbiaslow variance heuristic . this reflect the fact that a zerobias approach have poor generalisability to new situation and also unreasonably presumes precise knowledge of the true state of the world . the resulting heuristic be relatively simple but produce good inference in a wider variety of situation . geman et al . argue that the biasvariance dilemma implies that abilities such a generic object recognition cannot be learn from scratch but require a certain degree of hard wiring that is later tune by experience . this be because modelfree approach to inference require impractically large training set if they be to avoid high variance .
algorithmic bias occurs when a computer system behaves in way that reflect the implicit value of human involve in that data collection selection or use . algorithmic bias ha been identify and critiqued for it impact on search engine result social medium platform privacy and racial profiling . in search result this bias can create result reflect racist sexist or other social bias despite the presumed neutrality of the data . the study of algorithmic bias be most concern with algorithm that reflect systematic and unfair discrimination . a algorithm expand their ability to organize society politics institution and behavior sociologist have become concern with the way unanticipated output and manipulation can impact the physical world . because algorithm are often consider to be neutral and unbiased they can inaccurately project great authority than human expertise and in some case reliance on algorithm can displace human responsibility for their outcome . nonetheless bias can enter into algorithmic system a a result of preexist cultural social or institutional expectation because of technical limitation of their design or through use in unanticipated context or by audience not consider in their initial design . algorithmic bias ha been discover or theorize in case range from election outcome to the spread of online hate speech . problem in understand researching and discover algorithmic bias may come from the proprietary nature of algorithm which are typically treat a trade secret . even with full transparency understanding algorithm can be difficult because of their complexity and because not every permutation of an algorithm input or output can be anticipate or reproduce . in many case even within a single usecase such a a website or app there be no single algorithm to examine but a vast network of interrelated program and data input even between user of the same service . algorithm be difficult to define but may be generally understood a set of instruction within computer program that determine how these program read collect process and analyze data to generate some readable form of analysis or output . new computer can process million of these algorithmic instruction per second which ha boost the design and adoption of technology such a machine learning and artificial intelligence . by analyze and process data algorithm drive search engines social medium website recommendation engine online retail online advertising and more . contemporary social scientist are concern with algorithmic process embed into hardware and software application in order to understand their political effect and to question the underlying assumption of their neutrality . the term algorithmic bias is use to describe systematic and repeatable error that create unfair outcome i . generate one result for certain user and another result for others . for example a credit score algorithm may deny a loan base on certain factor without be unfair if it is consistently weigh relevant financial criterion . if that algorithm allow loan to some but deny loan to another set of nearly identical user base on arbitrary criterion and if this behavior can be repeat across multiple occurrence an algorithm can be describe a bias . this bias may be intentional or unintentional . bias can be introduce to an algorithm in several way . during the assemblage of a database data must be collect digitized adapt and enter according to humanassisted catalog criterion . next in the design of the algorithm programmer assign certain priority or hierarchy in how program ass and sort that data . this require human decision about how data is categorize and which data be discarded . some algorithm collect their own data base on humanselected criterion which can reflect the bias of human user . others may practice reinforce stereotype and preference a they process and display relevant data for human user a in select information base on previous choice of a user or group of user . beyond assemble the data bias can emerge a a result of design . example may arise in sorting process that determine the allocation of resource or scrutiny a in determining school placement or classification and identification process that may inadvertently discriminate against a category when assign risk a in credit score . in process association such a recommendation engine or inferred marketing trait algorithm may be flaw in way that reveal personal information . inclusion and exclusion criterion may have unanticipated outcome for search result such a in flight recommendation software omitting flight that dont follow the sponsoring airline prefer flight path . algorithm may also display an uncertainty bias offer more confident assessment when large data set be available . this can skew algorithmic process toward result that more closely correspond with large sample population which may not align with data from underrepresented population . the early computer program reflect simple humanderived operation and were deem to be function when they complete those operation . artificial intelligence pioneer joseph weizenbaum write that such program be therefore understood to embody law . weizenbaum describes early simple computer program change perception of machine from transfer power to transferring information . however he note that machine might transfer information with unintended consequence if there be error in detail provide to the machine and if user interpret data in intuitive way that cannot be formally communicate to or from a machine . weizenbaum state that all data feed to a machine must reflect human decisionmaking process which have been translate into rule for the computer to follow . to do this weizenbaum assert that programmer legislate the law for a world one first ha to create in imagination and a a result computer simulation can be build on model with incomplete or incorrect human data . weizenbaum compare the result of such decision to a tourist in a country who can make correct decision through a coin toss but have no basis of understand how or why the decision wa make . the complexity of analyze algorithmic bias ha grow alongside the complexity of program and their design . decision make by one designer or team of designer may be obscure among the many piece of code create for a single program over time these decision and their impact may be forget and take a natural result of the program output . these bias can create new pattern of behavior or script in relationship to specific technology a the code interacts with other element of society . bias may also impact how society shape itself around the data point that algorithm require . the decision of algorithmic program can be weigh more heavily than the decision of the human being they be meant to assist a process describe by author clay shirky a algorithmic authority . shirky use the term to describe the decision to regard a authoritative an unmanaged process of extract value from diverse untrustworthy source such a search result . this neutrality can also be misrepresent through language frame use when result be presented to the public . for example a list of news item select and present a trend or popular may be weigh base on significantly wider criterion than their popularity . because of their convenience and authority algorithm are theorize a a mean of delegate responsibility in decision making away from human . this can have the effect of reduce alternative option compromise or flexibility . sociologist scott lash ha critique algorithms a a new form of generative power in that they be a virtual mean of generate actual end . preexist bias in an algorithm be a consequence of underlie social and institutional ideology . such idea may reflect personal bias of individual designer or programmer or can reflect social institutional or cultural assumption . in both case such prejudice can be explicit and conscious or implicit and unconscious . poorly select input data will influence the outcome create by machine . in a critical view encode preexisting bias into software can preserve social and institutional bias and replicate it into all possible us of the algorithm into the future . an example of this form of bias be the british nationality act program design to automate the evaluation of new uk citizen after the british nationality act . the program accurately reflect the tenet of the law which state that a man be the father of only his legitimate child whereas a woman be the mother of all her child legitimate or not . by attempt to appropriately articulate this logic into an algorithmic process the bnap inscribe the logic of the british nationality act into it algorithm . technical bias emerges through limitation of a program computational power it design or other constraint on the system . such bias can also be a restraint of design for example a search engine that show three result per screen can be understand to privilege the top three result slightly more than the next three a in an airline price display . flaw in random number generation can also introduce bias into result . a decontextualized algorithm us unrelated information to sort result for example a flightpricing algorithm that sort result by alphabetical order would be bias in favor of american airline over unite airline . the opposite may also apply in which result are evaluate in different context from which they are collect . for example data may be collect without crucial external context when facial recognition software is use by surveillance camera but evaluate by remote staff in another country or region or evaluate by nonhuman algorithm with no awareness of what take place beyond the camera field of vision . lastly technical bias can be create by attempt to formalize decision into concrete step on the assumption that human behavior will correlate . for example software that weigh data point to determine whether a defendant should accept a plea bargain while ignore the impact of emotion on a jury is display a form of technical bias . emergent bias be the result of the use and reliance on algorithm across new or unanticipated context . new form of knowledge such a drug or medical breakthrough new law business model or shift cultural norm may be discover without algorithm be adjust to consider them . this may exclude group through technology without delineate clear outline of authorship or personal responsibility . similarly problem may emerge when train data i . the sample feed to a machine by which it model certain conclusion do not align with us that algorithm encounter in the real world . in an example of emergent bias wa identify in the software use to place u medical student into residency the national residency match program nrmp . the algorithm wa design at a time when few marry couple would seek residency together . a more woman enter medical school more student be likely to request a residency alongside their partner . the process call for each applicant to provide a list of preference for placement across the u which is then sort and assign when a hospital and an applicant both agree to a match . in the case of married couple where both sought residency the algorithm weigh a lead member location choice first . once it identify an optimum placement for that person it remove distant location from their partner preference reduce their list to the preferred location within the same city a the partner . the result be a frequent assignment of highrated school for the first partner and lowerpreference school to the second partner rather than sort for compromise in placement preference . additional emergent biases includeunpredictable correlation can emerge when large data set are compare to each other in practice . for example data collect about webbrowsing pattern may align with signal marking sensitive data such a race or sexual orientation . by discrimination against certain behavior or browse pattern the end effect would be almost identical to discrimination through the use of direct race or orientation data . in other case correlation can be infer for reason beyond the algorithm ability to understand them a when a triage program give low priority to asthmatic who have pneumonia . because asthmatic with pneumonia be at the high risk hospital typically give them the best and most immediate care the algorithm simply compare survival rate . emergent bias can occur when an algorithm is use by unanticipated audience such a machine that demand user can read write or understand number . certain metaphor may not carry across different population or skill set . for example the british national act program wa create a a proofofconcept by computer scientist and immigration lawyer to evaluate suitability for british citizenship . the designer therefore have expertise beyond the user whose understanding of both software and immigration law would likely be unsophisticated . the agent administer the question would not be aware of alternative pathway to citizenship outside of the software and shift case law and legal interpretation would lead the algorithm to outdated result . an area of concern around emergent bias be that it may be compound a bias technology be more deeply integrate into society . for example user with vision impairment may not be able to use an atm but can easily go to a bank branch . if bank branch begin to close because atm replace them they begin to exclude visionimpaired user from bank an unintended consequence of a technology . emergent bias may also create a feedback loop or recursion if data collect for an algorithm result in realworld response which are feed back into the algorithm . for example simulation of the predictive policing software predpol deploy in oakland california suggest an increase police presence in black neighborhood base on crime data report by the public . the simulation show that public report of crime could rise base on the sight of increase police activity and could be interpret by the software in model prediction of crime and to encourage a further increase in police presence within the same neighborhood . the human right data analysis group which conduct the study warn that in place where racial discrimination be a factor in arrest such feedback loop could reinforce and perpetuate racial discrimination in police . an early example of algorithmic bias result in as many a woman and ethnic minority deny entry to st . georges hospital medical school per year from to base on implementation of a new computerguidance assessment system that deny entry to women and men with foreignsounding name base on historical trend in admission . other example include the display of higherpaying job to male applicant on job search website . the plagiarismdetection software turnitin compare studentwritten text to information found online and return a probability that the student work is copy . because the software compare string of text it be more likely to identify nonnative speaker of english than native speaker who might be well able to adapt individual word and break up string of plagiarized text or obscure them through synonym . surveillance camera software may be consider inherently political a it require algorithm to identify and flag normal from abnormal behavior and to determine who belongs in certain location at certain time . the ability for such algorithm to recognize face across a racial spectrum ha been show to be limit by the racial diversity of image in it training database if the majority of photo belong to one race or gender the technology be well at recognize other member of that race or gender . a analysis of facial recognition software use to identify individual in cctv image find several example of bias when run against criminal database . the software wa assess a identifying men more frequently than woman old people more frequently than young people and identified asians africanamericans and other race more often than white . additional study of facial recognition software have find the opposite to be true when built on noncriminal database with software offering low accuracy rate to darkerskinned female than any other race or gender . in a facebook algorithm design to remove online hate speech wa find to advantage white men over black child when assess objectionable content accord to internal facebook document . the algorithm which be a blend of computer program and human content reviewer wa create to protect broad category rather than specific subset of category . for example post denouncing muslim would be block while post denouncing radical muslim would be allow . an unanticipated outcome of the algorithm be to allow hate speech against black child because they denounce the child subset of black rather than all black whereas all white men would trigger a block because white and male are not consider subset . facebook wa also find to allow ad purchaser to target jew hater a a category of user which the company say be an inadvertent outcome of algorithm use in assessing and categorize data . the company design also allow ad buyer to block africanamericans from see housing ad . corporate algorithm could be skew to invisibly favor financial arrangement or agreement between company without the knowledge of a user who may mistake the algorithm a be impartial . for example american airline create a flightfinding algorithm in the s . the software present a range of flight from various airline to customer but weigh factor that boost it own flight regardless of price or convenience . in testimony to the united state congress the president of the airline state outright that the system be created with the intention of gain competitive advantage through preferential treatment . in a paper describing google the founder of the company adopt a policy of transparency in search result regarding paid placement argue that advertisingfunded search engine will be inherently bias towards the advertiser and away from the need of the consumer . this bias would be an invisible manipulation of the user . a series of study of undecided voter in the u and in india find that search engine result be able to shift voting outcome by about . the researcher conclude that candidate have no mean of compete if an algorithm with or without intent boost page listing for a rival candidate . facebook user who saw message related to vote were more likely to vote themselves . a randomize trial of facebook user show an increase effect of vote among user and friend of user who saw provoting message in . the legal scholar jonathan zittrain ha warn that this could create a digital gerrymandering effect in election the selective presentation of information by an intermediary to meet it agenda rather than to serve it user if intentionally manipulated . in the professional networking site linkedin wa discover to recommend male variation of woman name in response to search query for woman . the site did not make similar recommendation in search for male name . for example andrea would bring up a prompt ask if user mean andrew but query for andrew did not ask if user mean to find andrea . the company say this be the result of an analysis of user interaction with the site . in the department store franchise target wa cite for gather data point to infer when woman customer be pregnant even if they hadnt announce it and then share that information with market partner . because the data had been predict rather than directly observe or report the company have no legal obligation to protect the privacy of those customer . web search algorithm have also been accuse of bias . google result may prioritize pornographic content in search term related to sexuality for example lesbian . this bias extend to the search engine surfacing popular but sexualized content in neutral search a in top sexy woman athlete article display a firstpage result in search for woman athlete . in google announced plan to curb search result that surfaced hate group racist view child abuse and pornography and other upsetting and offensive content . algorithm have been criticize a a method for obscure racial prejudice in decisionmaking . lisa nakamura ha note that census machine be among the first to adopt the punchcard process that lead to contemporary computing and that their use a categorization and sorting machine for race ha been long establish and socially tolerate . one example be the use of risk assessment in criminal sentencing and parole hearing an algorithmically generate score intend to reflect the risk that a suspect or prisoner will repeat a crime . from until the nationality of a suspect father be a consideration in such risk assessment . today these score are share with judge in arizona colorado delaware kentucky louisiana oklahoma virginia washington and wisconsin . an independent investigation by propublica found that the score be inaccurate of the time and disproportionately skew to suggest black to be at risk of relapse more often than white . in google apologized when black user complain that an imageidentification algorithm in it photo application identify them a gorilla . in nikon camera were criticize when imagerecognition algorithm consistently ask asian user if they were blink . such example be the product of bias in biometric data set . biometric data be drawn from aspect of the body include racial feature either observe or infer which can then be transfer into data point . biometric data about race may also be infer rather than observe . for example a study showed that name commonly associate with black be more likely to yield search result implying arrest record regardless of any police record of that individuals name . in user of the gay hookup app grindr report that the app wa link to sexoffender lookup apps in the android app store recommendation algorithm . writer mike ananny criticize this association in the atlantic arguing that such association far stigmatize gay men and may discourage closet men to maintain secrecy . a incident with online retailer amazon saw book delist after a shift in the algorithm expand it adult content blacklist for pornographic work to any book address sexuality or gay theme for example the critically acclaimed novel brokeback mountain . several challenge impede the study of largescale algorithmic bias hinder the application of academically rigorous study and public understanding . commercial algorithm be proprietary and may be treat a trade secret . this protects company such a a search engine in case where a transparent algorithm for rank result would reveal technique for manipulate the service . this make it difficult for researcher to conduct interview or analysis to discover how algorithm function . it can also be use to obscure possible unethical method use in produce or process algorithmic output . the closed nature of the code be not the only concern however a a certain degree of obscurity is protect by the complexity of contemporary program and the inability to know every permutation of a code input or output . social scientist bruno latour ha identify this process a blackboxing a process in which scientific and technical work is make invisible by it own success . when a machine run efficiently when a matter of fact is settle one need focus only on it input and output and not on it internal complexity . thus paradoxically the more science and technology succeed the more opaque and obscure they become . others have critique the black box metaphor suggest that current algorithm be not one black box but a network of interconnected one . algorithmic process be complex often exceed the understanding of the people who use them . largescale operation may not be understand even by those involve in create them . the social medium site facebook factor in at least data point to determine the layout of a user social medium fee in . furthermore large team of programmer may operate in relative isolation from one another and be unaware of the cumulative effect of small decision with nested section of sprawl algorithmic process . not all code be original and may be borrow from other library create a complicated set of relationship between data process and data input system . a significant barrier to understand tackling bias in practice be that category such a demographic of individual protect by antidiscrimination law are often not explicitly held by those collecting and process data . in some case there be little opportunity to collect this data explicitly such a in device fingerprinting ubiquitous computing and the internet of thing . in other case the data controller may not wish to collect such data for reputational reason or because it represent a heightened liability and security risk . it may also be the case that at least in relation to the general data protection regulation such data fall under the special category provision article and therefore come with more restriction on potential collection and process . algorithmic bias doe not only relate to protect category but can also concern something less easily observable or codifiable such a political viewpoint . in these case there be rarely an easily accessible or noncontroversial ground truth and debiasing such a system become considerably more tricky . furthermore false and accidental correlation can emerge from a lack of understanding of protect category for example insurance rate base on historical data of car accident which may overlap with residential cluster of ethnic minority . personalization of algorithm base on user interaction such a click time on site and other metric can confuse attempt to understand them . one unidentified streaming radio service report it have five unique musicselection algorithm it select for it user base on behavior . this create widely disparate experience of the same streaming product between different user . company also run frequent ab test to finetune algorithm base on user response . for example the search engine bing can run up to ten million subtle variation of it service per day segment the experience of an algorithm between user or among the same user . computer program and system can quickly spread among user embedding biased algorithm into broader society before their impact can be recognize or remedied . there might be compete motif within an organization for example the availability of loan from a bank a opposed to the bank profit incentive and the bank reputation worry for be catch discriminate an already suppress minority . a simple algorithm design with a single purpose such a expand profit would reduce loan to higherrisk applicant . in order to minimize discrimination between different group of applicant for ex . a majority group and a minority group a banking algorithm would have to make choice between conflict strategy such a these maximize it interest in shortterm profit apply the same technical criterion to all applicant irrespective of group ensure an equal ratio of loan grant for each group of applicant irrespective of group member merit or grant loan to an equal ratio of qualified applicant from each group . the general data protection regulation gdpr the european union revise data protection regime that enters force in address automate individual decisionmaking include profile in article . these rule prohibit solely automate decision which have a significant or legal effect on an individual unless they are explicitly authorise by consent contract or member state law . where they are permit there must be safeguard in place such a a right to a humanintheloop and allegedly although for political reason only in a nonbinding recital a right to an explanation of decision reach . while these are commonly consider to be new it be the case that nearly identical provision have exist across europe since in article of the data protection directive with the original automated decision rule and safeguard originate in french law in the late s . they have rarely been use give the heavy carveouts that exist and are not discuss in any case law of the european court of justice . the gdpr do address algorithmic bias in profile system as well a the statistical approach possible to clean it directly in recital noting thatthe controller should use appropriate mathematical or statistical procedure for the profile implement technical and organisational measure appropriate . that prevents inter alia discriminatory effect on natural person on the basis of racial or ethnic origin political opinion religion or beliefs trade union membership genetic or health status or sexual orientation or that result in measure having such an effect . like the alleged right to an explanation in recital this suffers from the nonbinding nature of recital compare to the binding article and while it ha been treat a a requirement by the article work party that advise on the implementation of data protection law it practical dimension be unclear . it ha been argue that the obligatory data protection impact assessment for high risk data profile in tandem with other preemptive measure within data protection may be a good way to tackle issue of algorithmic discrimination than rely on individual transparency right a information right have traditionally fatigued individual who are too overwhelm and overburden to use them effectively . the united state have no overall legislation regulating control for algorithmic bias approach the topic through various state and federal law that might vary by industry sector and us . many policy are selfenforced or control by the federal trade commission . in the obama administration release the national artificial intelligence research and development strategic plan which call for a critical assessment of algorithm and for researcher to design these system so that their action and decisionmaking be transparent and easily interpretable by human and thus can be examine for any bias they may contain rather than just learn and repeat these bias .
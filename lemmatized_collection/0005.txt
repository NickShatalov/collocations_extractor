in machine learn and computer vision mtheory be a learning framework inspire by feedforward processing in the ventral stream of visual cortex and originally develop for recognition and classification of object in visual scene . mtheory wa later apply to other areas such a speech recognition . on certain image recognition task algorithm base on a specific instantiation of mtheory hmax achieve humanlevel performance . the core principle of mtheory is extract representation invariant to various transformation of image translation scale d and d rotation and others . in contrast with other approach use invariant representation in mtheory they are not hardcoded into the algorithm but learn . mtheory also share some principle with compressed sensing . the theory proposes multilayered hierarchical learning architecture similar to that of visual cortex . a great challenge in visual recognition task be that the same object can be see in a variety of condition . it can be see from different distance different viewpoint under different lighting partially occlude etc . in addition for particular class objects such a face highly complex specific transformation may be relevant such a change facial expression . for learn to recognize image it be greatly beneficial to factor out these variation . it result in much simple classification problem and consequently in great reduction of sample complexity of the model . a simple computational experiment illustrate this idea . two instance of a classifier were train to distinguish image of plane from those of car . for training and test of the first instance image with arbitrary viewpoint were use . another instance receive only image see from a particular viewpoint which be equivalent to train and test the system on invariant representation of the image . one can see that the second classifier performed quite well even after receive a single example from each category while performance of the first classifier be close to random guess even after see example . invariant representation ha been incorporate into several learning architecture such a neocognitrons . most of these architecture however provide invariance through customdesigned feature or property of architecture itself . while it help to take into account some sort of transformation such a translation it be very nontrivial to accommodate for other sort of transformation such a d rotation and change facial expression . mtheory provide a framework of how such transformation can be learn . in addition to high flexibility this theory also suggest how human brain may have similar capability . another core idea of mtheory be close in spirit to ideas from the field of compressed sensing . an implication from johnsonlindenstrauss lemma say that a particular number of image can be embed into a lowdimensional feature space with the same distance between image by use random projection . this result suggest that dot product between the observed image and some other image store in memory called template can be use a a feature help to distinguish the image from other image . the template need not to be anyhow relate to the image it could be chosen randomly . the two idea outline in previous section can be bring together to construct a framework for learn invariant representation . the key observation is how dot product between image formula and a template formula behaves when image is transform by such transformation a translation rotation scale etc . if transformation formula be a member of a unitary group of transformation then the follow holdsformulain other word the dot product of transformed image and a template be equal to the dot product of original image and inversely transform template . for instance for image rotate by degree the inversely transformed template would be rotate by degree . consider the set of dot product of an image formula to all possible transformation of template formula . if one apply a transformation formula to formula the set would become formula . but because of the property this be equal to formula . the set formula be equal to just the set of all element in formula . to see this note that every formula be in formula due to the closure property of group and for every formula in g there exist it prototype formula such a formula namely formula . thus formula . one can see that the set of dot product remain the same despite that a transformation wa apply to the image this set by itself may serve a a very cumbersome invariant representation of an image . more practical representation can be derive from it . in the introductory section it wa claim that mtheory allows to learn invariant representation . this be because template and their transformed version can be learn from visual experience by expose the system to sequence of transformation of object . it be plausible that similar visual experience occur in early period of human life for instance when infants twiddle toy in their hand . because template may be totally unrelated to image that the system later will try to classify memory of these visual experience may serve a a basis for recognize many different kind of object in late life . however a it is show later for some kind of transformation specific template are need . to implement the idea describe in previous section one need to know how to derive a computationally efficient invariant representation of an image . such unique representation for each image can be characterize a it appear by a set of onedimensional probability distribution empirical distribution of the dotproducts between image and a set of template store during unsupervised learn . these probability distribution in their turn can be describe by either histogram or a set of statistical moment of it a it will be show below . orbit formula be a set of image formula generate from a single image formula under the action of the group formula . in other word image of an object and of it transformation correspond to a orbit formula . if two orbit have a point in common they be identical everywhere i . an orbit be an invariant and unique representation of an image . so two image are call equivalent when they belong to the same orbit formula if formula such that formula . conversely two orbit be different if none of the image in one orbit coincide with any image in the other . a natural question arise how can one compare two orbit there be several possible approach . one of them employ the fact that intuitively two empirical orbit be the same irrespective of the ordering of their point . thus one can consider a probability distribution formula induce by the group action on image formula formula can be see a a realization of a random variable . this probability distribution formula can be almost uniquely characterize by formula onedimensional probability distribution formula induce by the onedimensional result of projection formula where formula be a set of template randomly chosen image base on the cramerwold theorem and concentration of measure . consider formula image formula . let formula where formula be a universal constant . thenformulawith probability formula for all formula formula formula . this result informally say that an approximately invariant and unique representation of an image formula can be obtain from the estimate of formula d probability distribution formula for formula . the number formula of projection need to discriminate formula orbit induce by formula image up to precision formula and with confidence formula be formula where formula be a universal constant . to classify an image the follow recipe can be usedestimates of such onedimensional probability density function pdfs formula can be write in term of histogram a formula where formula be a set of nonlinear function . these d probability distribution can be characterize with nbin histogram or set of statistical moment . for example hmax represent an architecture in which pooling is do with a max operation . in the recipe for image classification group of transformation are approximate with finite number of transformation . such approximation be possible only when the group be compact . such group a all translation and all scaling of the image be not compact a they allow arbitrarily big transformation . however they be locally compact . for locally compact group invariance be achievable within certain range of transformation . assume that formula be a subset of transformation from formula for which the transformed pattern exist in memory . for an image formula and template formula assume that formula be equal to zero everywhere except some subset of formula . this subset is call support of formula and denote a formula . it can be prove that if for a transformation formula support set will also lie within formula then signature of formula be invariant with respect to formula . this theorem determine the range of transformation for which invariance is guarantee to hold . one can see that the smaller be formula the larger be the range of transformation for which invariance is guarantee to hold . it mean that for a group that be only locally compact not all template would work equally well anymore . preferable template be those with a reasonably small formula for a generic image . this property is call localization template are sensitive only to image within a small range of transformation . note that although minimizing formula be not absolutely necessary for the system to work it improve approximation of invariance . require localization simultaneously for translation and scale yield a very specific kind of template gabor function . the desirability of custom template for noncompact group be in conflict with the principle of learn invariant representation . however for certain kind of regularly encounter image transformation template might be the result of evolutionary adaptation . neurobiological data suggest that there is gaborlike tune in the first layer of visual cortex . the optimality of gabor template for translation and scale be a possible explanation of this phenomenon . many interesting transformation of image do not form group . for instance transformation of image associate with d rotation of correspond d object do not form a group because it be impossible to define an inverse transformation two object may look the same from one angle but different from another angle . however approximate invariance be still achievable even for nongroup transformation if localization condition for template hold and transformation can be locally linearized . a it wa say in the previous section for specific case of translation and scaling localization condition can be satisfy by use of generic gabor template . however for general case nongroup transformation localization condition can be satisfy only for specific class of object . more specifically in order to satisfy the condition template must be similar to the object one would like to recognize . for instance if one would like to build a system to recognize d rotated face one need to use other d rotate face a template . this may explain the existence of such specialize module in the brain a one responsible for face recognition . even with custom templates a noiselike encoding of image and template be necessary for localization . it can be naturally achieved if the nongroup transformation is process on any layer other than the first in hierarchical recognition architecture . the previous section suggest one motivation for hierarchical image recognition architecture . however they have other benefit as well . firstly hierarchical architecture best accomplish the goal of parse a complex visual scene with many object consist of many part whose relative position may greatly vary . in this case different element of the system must react to different object and part . in hierarchical architecture representation of part at different level of embed hierarchy can be store at different layer of hierarchy . secondly hierarchical architecture which have invariant representation for part of object may facilitate learning of complex compositional concept . this facilitation may happen through reusing of learn representation of part that were construct before in process of learn of other concept . a a result sample complexity of learn compositional concept may be greatly reduce . finally hierarchical architecture have good tolerance to clutter . clutter problem arises when the target object be in front of a nonuniform background which function a a distractor for the visual task . hierarchical architecture provide signature for part of target object which do not include part of background and are not affect by background variation . in hierarchical architecture one layer be not necessarily invariant to all transformation that are handle by the hierarchy a a whole . some transformation may pass through that layer to upper layer a in the case of nongroup transformation describe in the previous section . for other transformation an element of the layer may produce invariant representation only within small range of transformation . for instance element of the low layer in hierarchy have small visual field and thus can handle only a small range of translation . for such transformation the layer should provide covariant rather than invariant signature . the property of covariance can be write a formula where formula be a layer formula be the signature of image on that layer and formula stand for distribution of value of the expression for all formula . mtheory is base on a quantitative theory of the ventral stream of visual cortex . understand how visual cortex work in object recognition be still a challenging task for neuroscience . human and primate be able to memorize and recognize object after see just couple of example unlike any stateofthe art machine vision system that usually require a lot of data in order to recognize object . prior to the use of visual neuroscience in computer vision ha been limit to early vision for derive stereo algorithm e . and to justify the use of dog derivativeofgaussian filter and more recently of gabor filter . no real attention ha be give to biologically plausible feature of high complexity . while mainstream computer vision ha always be inspired and challenge by human vision it seem to have never advanced past the very first stage of process in the simple cell in v and v . although some of the system inspire to various degree by neuroscience have be test on at least some natural image neurobiological model of object recognition in cortex have not yet been extend to deal with realworld image database . mtheory learn framework employ a novel hypothesis about the main computational function of the ventral stream the representation of new objectsimages in term of a signature which be invariant to transformation learn during visual experience . this allows recognition from very few label example in the limit just one . neuroscience suggest that natural functionals for a neuron to compute be a highdimensional dot product between an image patch and another image patch call template which is store in term of synaptic weight synapsis per neuron . the standard computational model of a neuron is base on a dot product and a threshold . another important feature of the visual cortex be that it consist of simple and complex cell . this idea wa originally propose by hubel and wiesel . mtheory employ this idea . simple cell compute dot product of an image and transformation of template formula for formula formula be a number of simple cell . complex cell be responsible for pool and compute empirical histogram or statistical moment of it . the follow formula for construct histogram can be compute by neuronsformulawhere formula be a smooth version of step function formula be the width of a histogram bin and formula be the number of the bin . in author applied mtheory to unconstrained face recognition in natural photograph . unlike the dar detection alignment and recognition method which handle clutter by detect object and crop closely around them so that very little background remain this approach accomplish detection and alignment implicitly by store transformation of training image template rather than explicitly detecting and align or crop face at test time . this system be built according to the principle of a recent theory of invariance in hierarchical network and can evade the clutter problem generally problematic for feedforward system . the resulting endtoend system achieve a drastic improvement in the state of the art on this endtoend task reach the same level of performance a the best system operating on align closely crop image no outside train data . it also performs well on two newer datasets similar to lfw but more difficult significantly jittered misaligned version of lfw and sufrw for example the model accuracy in the lfw unaligned no outside data used category be . compare to stateoftheart apem adaptive probabilistic elastic matching . the theory wa also apply to a range of recognition task from invariant single object recognition in clutter to multiclass categorization problem on publicly available data set caltech caltech mitcbcl and complex street scene understanding task that require the recognition of both shapebased as well a texturebased object on streetscenes data set . the approach performs really well it have the capability of learn from only a few training example and wa show to outperform several more complex stateoftheart system constellation model the hierarchical svmbased face detection system . a key element in the approach be a new set of scale and positiontolerant feature detector which be biologically plausible and agree quantitatively with the tuning property of cell along the ventral stream of visual cortex . these feature be adaptive to the training set though we also show that a universal feature set learn from a set of natural image unrelated to any categorization task likewise achieves good performance . this theory can also be extend for the speech recognition domain . a an example in an extension of a theory for unsupervised learning of invariant visual representation to the auditory domain and empirically evaluate it validity for voiced speech sound classification wa propose . author empirically demonstrate that a singlelayer phonelevel representation extract from base speech feature improves segment classification accuracy and decrease the number of training example in comparison with standard spectral and cepstral feature for an acoustic classification task on timit dataset .
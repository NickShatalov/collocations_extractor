in mathematics and statistic random projection be a technique use to reduce the dimensionality of a set of point which lie in euclidean space . random projection method be powerful method known for their simplicity and less erroneous output compare with other method . accord to experimental result random projection preserve distance well but empirical result be sparse . they have been apply to many natural language task under the name of random index . dimensionality reduction a the name suggest is reduce the number of random variable use various mathematical method from statistic and machine learn . dimensionality reduction is often use to reduce the problem of manage and manipulate large data set . dimensionality reduction technique generally use linear transformation in determine the intrinsic dimensionality of the manifold as well a extract it principal direction . for this purpose there be various related technique include principal component analysis linear discriminant analysis canonical correlation analysis discrete cosine transform random projection etc . random projection be a simple and computationally efficient way to reduce the dimensionality of data by trade a control amount of error for faster process time and small model size . the dimension and distribution of random projection matrix are control so a to approximately preserve the pairwise distance between any two sample of the dataset . the core idea behind random projection is give in the johnsonlindenstrauss lemma which state that if point in a vector space are of sufficiently high dimension then they may be project into a suitable lowerdimensional space in a way which approximately preserve the distance between the point . in random projection the original ddimensional data is project to a kdimensional k d subspace use a random formula dimensional matrix r whose row have unit length . use matrix notation if formula be the original set of n ddimensional observation then formula be the projection of the data onto a low kdimensional subspace . random projection be computationally simple form the random matrix r and project the formula data matrix x onto k dimension of order formula . if the data matrix x be sparse with about c nonzero entry per column then the complexity of this operation be of order formula . the random matrix r can be generate use a gaussian distribution . the first row be a random unit vector uniformly chosen from formula . the second row be a random unit vector from the space orthogonal to the first row the third row be a random unit vector from the space orthogonal to the first two row and so on . in this way of choose r r be an orthogonal matrix the inverse of it transpose and the follow property be satisfiedachlioptas ha show that the gaussian distribution can be replace by a much simple distribution such asthis be efficient for database application because the computation can be perform use integer arithmetic . it be late shown how to use integer arithmetic while make the distribution even sparser having very few nonzeroes per column in work on the sparse jl transform . this be advantageous since a sparse embed matrix mean be able to project the data to low dimension even faster . the johnsonlindenstrauss lemma state that large set of vector in a highdimensional space can be linearly map in a space of much lower but still high dimension n with approximate preservation of distance . one of the explanation of this effect be the exponentially high quasiorthogonal dimension of ndimensional euclidean space . there be exponentially large in dimension n set of almost orthogonal vector with small value of inner product in ndimensional euclidean space . this observation be useful in index of highdimensional data . quasiorthogonality of large random set be important for method of random approximation in machine learn . in high dimension exponentially large number of randomly and independently chosen vector from equidistribution on a sphere and from many other distribution be almost orthogonal with probability close to one . this imply that in order to represent an element of such a highdimensional space by linear combination of randomly and independently chosen vector it may often be necessary to generate sample of exponentially large length if we use bound coefficient in linear combination . on the other hand if coefficient with arbitrarily large value are allow the number of randomly generate element that be sufficient for approximation be even less than dimension of the data space .
in machine learning hyperparameter optimization or tuning be the problem of choose a set of optimal hyperparameters for a learn algorithm . the same kind of machine learn model can require different constraint weight or learn rate to generalize different data pattern . these measure are call hyperparameters and have to be tune so that the model can optimally solve the machine learn problem . hyperparameter optimization find a tuple of hyperparameters that yield an optimal model which minimize a predefined loss function on give independent data . the objective function take a tuple of hyperparameters and return the associated loss . crossvalidation is often use to estimate this generalization performance . the traditional way of performing hyperparameter optimization ha be grid search or a parameter sweep which be simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learn algorithm . a grid search algorithm must be guide by some performance metric typically measure by crossvalidation on the training setor evaluation on a heldout validation set . since the parameter space of a machine learner may include realvalued or unbounded value space for certain parameter manually set bound and discretization may be necessary before apply grid search . for example a typical softmargin svm classifier equip with an rbf kernel have at least two hyperparameters that need to be tune for good performance on unseen data a regularization constant c and a kernel hyperparameter . both parameter be continuous so to perform grid search one select a finite set of reasonable value for each saygrid search then train an svm with each pair c in the cartesian product of these two set and evaluate their performance on a heldout validation set or by internal crossvalidation on the training set in which case multiple svms are train per pair . finally the grid search algorithm output the setting that achieve the high score in the validation procedure . grid search suffers from the curse of dimensionality but is often embarrassingly parallel because typically the hyperparameter setting it evaluates be independent of each other . random search replace the exhaustive enumeration of all combination by select them randomly . this can be simply apply to the discrete setting describe above but also generalizes to continuous and mix space . it can outperform grid search especially when only a small number of hyperparameters affect the final performance of the machine learn algorithm . in this case the optimization problem is say to have a low intrinsic dimensionality . random search be also embarrassingly parallel and additionally allow to include prior knowledge by specify the distribution from which to sample . bayesian optimization be a global optimization method for noisy blackbox function . apply to hyperparameter optimization bayesian optimization build a probabilistic model of the function map from hyperparameter value to the objective evaluate on a validation set . by iteratively evaluate a promise hyperparameter configuration base on the current model and then update it bayesian optimization aim to gather observation revealing a much information a possible about this function and in particular the location of the optimum . it try to balance exploration hyperparameters for which the outcome be most uncertain and exploitation hyperparameters expect close to the optimum . in practice bayesian optimization ha been show to obtain good result in few evaluation compare to grid search and random search due to the ability to reason about the quality of experiment before they be run . for specific learning algorithm it be possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters use gradient descent . the first usage of these technique wa focus on neural network . since then these method have been extend to other model such a support vector machine or logistic regression . a different approach in order to obtain a gradient with respect to hyperparameters consists in differentiate the step of an iterative optimization algorithm using automatic differentiation . evolutionary optimization be a methodology for the global optimization of noisy blackbox function . in hyperparameter optimization evolutionary optimization use evolutionary algorithm to search the space of hyperparameters for a given algorithm . evolutionary hyperparameter optimization follow a process inspire by the biological concept of evolutionevolutionary optimization ha be use in hyperparameter optimization for statistical machine learn algorithm automate machine learn deep neural network architecture search as well a train of the weight in deep neural network . rbf and spectral approach have also been develop .
In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is "smooth": data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.

Manifold regularization is a type of regularization, a family of techniques that reduces overfitting and ensures that a problem is well-posed by penalizing complex solutions. In particular, manifold regularization extends the technique of Tikhonov regularization as applied to Reproducing kernel Hilbert spaces (RKHSs). Under standard Tikhonov regularization on RKHSs, a learning algorithm attempts to learn a function formula_1 from among a hypothesis space of functions formula_2. The hypothesis space is an RKHS, meaning that it is associated with a kernel formula_3, and so every candidate function formula_1 has a norm formula_5, which represents the complexity of the candidate function in the hypothesis space. When the algorithm considers a candidate function, it takes its norm into account in order to penalize complex functions.

Formally, given a set of labeled training data formula_6 with formula_7 and a loss function formula_8, a learning algorithm using Tikhonov regularization will attempt to solve the expression

where formula_10 is a hyperparameter that controls how much the algorithm will prefer simpler functions to functions that fit the data better.

Manifold regularization adds a second regularization term, the "intrinsic regularizer", to the "ambient regularizer" used in standard Tikhonov regularization. Under the manifold assumption in machine learning, the data in question do not come from the entire input space formula_11, but instead from a nonlinear manifold formula_12. The geometry of this manifold, the intrinsic space, is used to determine the regularization norm.

There are many possible choices for formula_13. Many natural choices involve the gradient on the manifold formula_14, which can provide a measure of how smooth a target function is. A smooth function should change slowly where the input data are dense; that is, the gradient formula_15 should be small where the "marginal probability density" formula_16, the probability density of a randomly drawn data point appearing at formula_17, is large. This gives one appropriate choice for the intrinsic regularizer:

In practice, this norm cannot be computed directly because the marginal distribution formula_19 is unknown, but it can be estimated from the provided data. In particular, if the distances between input points are interpreted as a graph, then the Laplacian matrix of the graph can help to estimate the marginal distribution. Suppose that the input data include formula_20 labeled examples (pairs of an input formula_17 and a label formula_22) and formula_23 unlabeled examples (inputs without associated labels). Define formula_24 to be a matrix of edge weights for a graph, where formula_25 is a distance measure between the data points formula_26 and formula_27. Define formula_28 to be a diagonal matrix with formula_29 and formula_30 to be the Laplacian matrix formula_31. Then, as the number of data points formula_32 increases, formula_30 converges to the Laplace-Beltrami operator formula_34, which is the divergence of the gradient formula_35. Then, if formula_36 is a vector of the values of formula_1 at the data, formula_38, the intrinsic norm can be estimated:

As the number of data points formula_32 increases, this empirical definition of formula_41 converges to the definition when formula_19 is known.

Using the weights formula_43 and formula_44 for the ambient and intrinsic regularizers, the final expression to be solved becomes:

As with other kernel methods, formula_2 may be an infinite-dimensional space, so if the regularization expression cannot be solved explicitly, it is impossible to search the entire space for a solution. Instead, a representer theorem shows that under certain conditions on the choice of the norm formula_13, the optimal solution formula_48 must be a linear combination of the kernel centered at each of the input points: for some weights formula_49,

Using this result, it is possible to search for the optimal solution formula_48 by searching the finite-dimensional space defined by the possible choices of formula_49.

Manifold regularization can extend a variety of algorithms that can be expressed using Tikhonov regularization, by choosing an appropriate loss function formula_8 and hypothesis space formula_2. Two commonly used examples are the families of support vector machines and regularized least squares algorithms. (Regularized least squares includes the ridge regression algorithm; the related algorithms of LASSO and elastic net regularization can be expressed as support vector machines.) The extended versions of these algorithms are called Laplacian Regularized Least Squares (abbreviated LapRLS) and Laplacian Support Vector Machines (LapSVM), respectively.

Regularized least squares (RLS) is a family of regression algorithms: algorithms that predict a value formula_55 for its inputs formula_17, with the goal that the predicted values should be close to the true labels for the data. In particular, RLS is designed to minimize the mean squared error between the predicted values and the true labels, subject to regularization. Ridge regression is one form of RLS; in general, RLS is the same as ridge regression combined with the kernel method. The problem statement for RLS results from choosing the loss function formula_8 in Tikhonov regularization to be the mean squared error:

Thanks to the representer theorem, the solution can be written as a weighted sum of the kernel evaluated at the data points:

and solving for formula_60 gives:

where formula_3 is defined to be the kernel matrix, with formula_63, and formula_64 is the vector of data labels.

Adding a Laplacian term for manifold regularization gives the Laplacian RLS statement:

The representer theorem for manifold regularization again gives

and this yields an expression for the vector formula_60. Letting formula_3 be the kernel matrix as above, formula_64 be the vector of data labels, and formula_70 be the formula_71 block matrix formula_72:

with a solution of

LapRLS has been applied to problems including sensor networks,
medical imaging,
object detection,
spectroscopy,
document classification,
drug-protein interactions,
and compressing images and videos.

Support vector machines (SVMs) are a family of algorithms often used for classifying data into two or more groups, or "classes". Intuitively, an SVM draws a boundary between classes so that the closest labeled examples to the boundary are as far away as possible. This can be directly expressed as a linear program, but it is also equivalent to Tikhonov regularization with the hinge loss function, formula_75:

Adding the intrinsic regularization term to this expression gives the LapSVM problem statement:

Again, the representer theorem allows the solution to be expressed in terms of the kernel evaluated at the data points:

formula_79 can be found by writing the problem as a linear program and solving the dual problem. Again letting formula_3 be the kernel matrix and formula_70 be the block matrix formula_72, the solution can be shown to be

where formula_84 is the solution to the dual problem

and formula_86 is defined by

LapSVM has been applied to problems including geographical imaging,
medical imaging,
face recognition,
machine maintenance,
and brain-computer interfaces.




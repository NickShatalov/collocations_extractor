Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. Early versions of MTL were called "hints"

In a widely cited 1997 paper, Rich Caruana gave the following characterization:Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.

In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer. Solving each user's spam classification problem jointly via MTL can let the solutions inform each other and improve performance. Further examples of settings for MTL include multiclass classification and multi-label classification.

Multi-task learning works because regularization induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalizing all complexity uniformly. One situation where MTL may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled. However, as discussed below, MTL has also been shown to be beneficial for learning unrelated tasks.

Within the MTL paradigm, information can be shared across some or all of the tasks. Depending on the structure of task relatedness, one may want to share information selectively across the tasks. For example, tasks may be grouped or exist in a hierarchy, or be related according to some general metric. Suppose, as developed more formally below, that the parameter vector modeling each task is a linear combination of some underlying basis. Similarity in terms of this basis can indicate the relatedness of the tasks. For example, with sparsity, overlap of nonzero coefficients across tasks indicates commonality. A task grouping then corresponds to those tasks lying in a subspace generated by some subset of basis elements, where tasks in different groups may be disjoint or overlap arbitrarily in terms of their bases. Task relatedness can be imposed a priori or learned from the data. Hierarchical task relatedness can also be exploited implicitly without assuming a priori knowledge or learning relations explicitly.

One can attempt learning a group of principal tasks using a group of auxiliary tasks, unrelated to the principal ones. In many applications, joint learning of unrelated tasks which use the same input data can be beneficial. The reason is that prior knowledge about task relatedness can lead to sparser and more informative representations for each task grouping, essentially by screening out idiosyncrasies of the data distribution. Novel methods which builds on a prior multitask methodology by favoring a shared low-dimensional representation within each task grouping have been proposed. The programmer can impose a penalty on tasks from different groups which encourages the two representations to be orthogonal. Experiments on synthetic and real data have indicated that incorporating unrelated tasks can result in significant improvements over standard multi-task learning methods.

Related to multi-task learning is the concept of knowledge transfer. Whereas traditional multi-task learning implies that a shared representation is developed concurrently across tasks, transfer of knowledge implies a sequentially shared representation. Large scale machine learning projects such as the deep convolutional neural network GoogLeNet, an image-based object classifier, can develop robust representations which may be useful to further algorithms learning related tasks. For example, the pre-trained model can be used as a feature extractor to perform pre-processing for another learning algorithm. Or the pre-trained model can be used to initialize a model with similar architecture which is then fine-tuned to learn a different classification task.

Traditionally Multi-task learning and transfer of knowledge are applied to stationary learning settings. Their extension to non-stationary environments is termed Group online adaptive learning (GOAL). Sharing information could be particularly useful if learners operate in continuously changing environments, because a learner could benefit from previous experience of another learner to quickly adapt to their new environment. Such group-adaptive learning has numerous applications, from predicting financial time-series, through content recommendation systems, to visual understanding for adaptive autonomous agents.

The MTL problem can be cast within the context of RKHSvv (a complete inner product space of vector-valued functions equipped with a reproducing kernel). In particular, recent focus has been on cases where task structure can be identified via a separable kernel, described below. The presentation here derives from Ciliberto et al., 2015.

Suppose the training data set is formula_1, with formula_2, formula_3, where formula_4 indexes task, and formula_5. Let formula_6. In this setting there is a consistent input and output space and the same loss function formula_7 for each task: . This results in the regularized machine learning problem: 
where formula_8 is a vector valued reproducing kernel Hilbert space with functions formula_9 having components formula_10.

The reproducing kernel for the space formula_8 of functions formula_12 is a symmetric matrix-valued function formula_13 , such that formula_14 and the following reproducing property holds: 
The form of the kernel formula_15 induces both the representation of the feature space and structures the output across tasks. A natural simplification is to choose a "separable kernel," which factors into separate kernels on the input space formula_16 and on the tasks formula_17. In this case the kernel relating scalar components formula_18 and formula_19 is given by formula_20. For vector valued functions formula_21 we can write formula_22, where formula_23 is a scalar reproducing kernel, and formula_24 is a symmetric positive semi-definite formula_25 matrix. Henceforth denote formula_26 .

This factorization property, separability, implies the input feature space representation does not vary by task. That is, there is no interaction between the input kernel and the task kernel. The structure on tasks is represented solely by formula_24. Methods for non-separable kernels formula_15 is an current field of research.

For the separable case, the representation theorem is reduced to formula_29. The model output on the training data is then formula_30 , where formula_31 is the formula_32 empirical kernel matrix with entries formula_33, and formula_34 is the formula_35 matrix of rows formula_36.

With the separable kernel, equation can be rewritten as

where formula_37 is a (weighted) average of formula_38 applied entry-wise to Y and KCA. (The weight is zero if formula_39 is a missing observation).

Note the second term in can be derived as follows:

formula_40

formula_41 (bilinearity)

formula_42 (reproducing property)

formula_43

There are three largely equivalent ways to represent task structure: through a regularizer; through an output metric, and through an output mapping.

Regularizer - With the separable kernel, it can be shown (below) that formula_44, where formula_45 is the formula_46 element of the pseudoinverse of formula_47, and formula_48 is the RKHS based on the scalar kernel formula_49, and formula_50. This formulation shows that formula_45 controls the weight of the penalty associated with formula_52. (Note that formula_52 arises from formula_54.)

Proof:

formula_55

formula_56

formula_57

formula_58

formula_59

formula_60

formula_61

formula_62

formula_63

Output metric - an alternative output metric on formula_64 can be induced by the inner product formula_65. With the squared loss there is an equivalence between the separable kernels formula_66 under the alternative metric, and formula_67, under the canonical metric.

Output mapping - Outputs can be mapped as formula_68 to a higher dimensional space to encode complex structures such as trees, graphs and strings. For linear maps formula_69, with appropriate choice of separable kernel, it can be shown that formula_70.

Via the regularizer formulation, one can represent a variety of task structures easily. 

Learning problem can be generalized to admit learning task matrix A as follows:
Choice of formula_91 must be designed to learn matrices "A" of a given type. See "Special cases" below.

Restricting to the case of convex losses and coercive penalties Ciliberto "et al." have shown that although is not convex jointly in "C" and "A," a related problem is jointly convex.

Specifically on the convex set formula_92, the equivalent problem

is convex with the same minimum value. And if formula_93 is a minimizer for then formula_94 is a minimizer for .

The perturbation via the barrier formula_95 forces the objective functions to be equal to formula_96 on the boundary of formula_97 .

Spectral penalties - Dinnuzo "et al" suggested setting "F" as the Frobenius norm formula_100. They optimized directly using block coordinate descent, not accounting for difficulties at the boundary of formula_101.

Clustered tasks learning - Jacob "et al" suggested to learn "A" in the setting where "T" tasks are organized in "R" disjoint clusters. In this case let formula_102 be the matrix with formula_103. Setting formula_104, and formula_105, the task matrix formula_106 can be parameterized as a function of formula_107: formula_108 , with terms that penalize the average, between clusters variance and within clusters variance respectively of the task predictions. M is not convex, but there is a convex relaxation formula_109. In this formulation, formula_110.

Non-convex penalties - Penalties can be constructed such that A is constrained to be a graph Laplacian, or that A has low rank factorization. However these penalties are not convex, and the analysis of the barrier method proposed by Ciliberto et al. does not go through in these cases.

Non-separable kernels - Separable kernels are limited, in particular they do not account for structures in the interaction space between the input and output domains jointly. Future work is needed to develop models for these kernels.

Using the principles of MTL, techniques for collaborative spam filtering that facilitates personalization have been proposed. In large scale open membership email systems, most users do not label enough messages for an individual local classifier to be effective, while the data is too noisy to be used for a global filter across all users. A hybrid global/individual classifier can be effective at absorbing the influence of users who label emails very diligently from the general public. This can be accomplished while still providing sufficient quality to users with few labeled instances.

Using boosted decision trees, one can enable implicit data sharing and regularization. This learning method can be used on web-search ranking data sets. One example is to use ranking data sets from several countries. Here, multitask learning is particularly helpful as data sets from different countries vary largely in size because of the cost of editorial judgments. It has been demonstrated that learning various tasks jointly can lead to significant improvements in performance with surprising reliability.

In order to facilitate transfer of knowledge, IT infrastructure is being developed. One such project, RoboEarth, aims to set up an open source internet database that can be accessed and continually updated from around the world. The goal is to facilitate a cloud-based interactive knowledge base, accessible to technology companies and academic institutions, which can enhance the sensing, acting and learning capabilities of robots and other artificial intelligence agents.

The Multi-Task Learning via StructurAl Regularization (MALSAR) Matlab package implements the following multi-task learning algorithms:




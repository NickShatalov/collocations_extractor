In statistical classification, including machine learning, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent, but three major types can be distinguished, following . Given an observable variable "X" and a target variable "Y", a generative model is a statistical model of the joint probability distribution on "X" × "Y", formula_1; a discriminative model is a model of the conditional probability of the target "Y", given an observation "x", symbolically, formula_2; and classifiers computed without using a probability model are also referred to loosely as "discriminative". The distinction between these last two classes is not consistently made; refers to these three classes as "generative learning", "conditional learning", and "discriminative learning", but only distinguishes two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes. Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model. Standard examples of each, all of which are linear classifiers, are: generative classifiers: naive Bayes classifier and linear discriminant analysis; discriminative model: logistic regression; non-model classifier: perceptron and support vector machine.

In application to classification, one wishes to go from an observation "x" to a label "y" (or probability distribution on labels). One can compute this directly, without using a probability distribution ("distribution-free classifier"); one can estimate the probability of a label given an observation, formula_3 ("discriminative model"), and base classification on that; or one can estimate the joint distribution formula_1 ("generative model"), from that compute the conditional probability formula_3, and then base classification on that. These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches.

An alternative division defines these symmetrically as: a generative model is a model of the conditional probability of the observable "X", given a target "y", symbolically, formula_6; while a discriminative model is a model of the conditional probability of the target "Y", given an observation "x", symbolically, formula_2. Regardless of precise definition, the terminology is because a generative model can be used to "generate" random instances (outcomes), either of an observation and target formula_8, or of an observation "x" given a target value "y". while a discriminative model or discriminative classifier (without a model) can be used to "discriminate" the value of the target variable "Y", given an observation "x". The difference between "discriminate" (distinguish) and "classify" is subtle, and these are not consistently distinguished, so the term "discriminative classifier" because a pleonasm, meaning that it does nothing other than classify (equivalently, "discriminate") inputs.

In application to classification, the observable "X" is frequently a continuous variable, the target "Y" is generally a discrete variable consisting of a finite set of labels, and the conditional probability formula_9 can also be interpreted as a (non-deterministic) target function formula_10, considering "X" as inputs and "Y" as outputs.

Given a finite set of labels, the two definitions of "generative model" are closely related. A model of the conditional distribution formula_6 is a model of the distribution of each label, and a model of the joint distribution is equivalent to a model of the distribution of label values formula_12, together with the distribution of observations given a label, formula_13; symbolically, formula_14 Thus, while a model of the joint probability distribution is more information than a model of the distribution of label (but without their relative frequencies), it is a relatively small step, hence these are not always distinguished.

Given a model of the joint distribution, formula_1, the distribution of the individual variables can be computed as the marginal distributions formula_16 and formula_17 (considering "X" as continuous, hence integrating over it, and "Y" as discrete, hence summing over it), and either conditional distribution can be computed from the definition of conditional probability: formula_18 and formula_19.

Given a model of one conditional probability, and estimated probability distributions for the variables "X" and "Y", denoted formula_20 and formula_12, one can estimate the opposite conditional probability using Bayes' rule:
For example, given a generative model for formula_13, one can estimate:
and given a discriminative model for formula_9, one can estimate:
Note that Bayes' rule (computing one conditional probability in terms of the other) and the definition of conditional probability (computing conditional probability in terms of the joint distribution) are frequently conflated as well.

A generative algorithm models how the data was generated in order to categorize a signal. It asks the question: based on my generation assumptions, which category is most likely to generate this signal? A discriminative algorithm does not care about how the data was generated, it simply categorizes a given signal. So, discriminative algorithms try to learn formula_27 directly from the data and then try to classify data. On the other hand, generative algorithms try to learn formula_28 which can be transformed into formula_27 later to classify the data. One of the advantages of generative algorithms is that you can use formula_28 to generate new data similar to existing data. On the other hand, discriminative algorithms generally give better performance in classification tasks.

Despite the fact that discriminative models do not need to model the distribution of the observed variables, they cannot generally express complex relationships between the observed and target variables. They don't necessarily perform better than generative models at classification and regression tasks. The two classes are seen as complementary or as different views of the same procedure.

Types of generative models are:


If the observed data are truly sampled from the generative model, then fitting the parameters of the generative model to maximize the data likelihood is a common method. However, since most statistical models are only approximations to the "true" distribution, if the model's application is to infer about a subset of variables conditional on known values of others, then it can be argued that the approximation makes more assumptions than are necessary to solve the problem at hand. In such cases, it can be more accurate to model the conditional density functions directly using a discriminative model (see below), although application-specific details will ultimately dictate which approach is most suitable in any particular case.


Suppose the input data is formula_31, the set of labels for formula_32 is formula_33, and there are the following 4 data points:
formula_34

For the above data, estimating the joint probability distribution formula_28 from the empirical measure will be the following:
while formula_27 will be following:
 gives an example in which a table of frequencies of English word pairs is used to generate a sentence beginning with "representing and speedily is an good"; which is not proper English but which will increasingly approximate it as the table is moved from word pairs to word triplets etc.



The sample complexity of a machine learning algorithm represents the number of training-samples that it needs in order to successfully learn a target function.

More precisely, the sample complexity is the number of training-samples that we need to supply to the algorithm, so that the function returned by the algorithm is within an arbitrarily small error of the best possible function, with probability arbitrarily close to 1.

There are two variants of sample complexity:

The No Free Lunch theorem, discussed below, proves that, in general, the strong sample complexity is infinite, i.e. that there is no algorithm that can learn the globally-optimal target function using a finite number of training samples.

However, if we are only interested in a particular class of target functions (e.g, only linear functions) then the sample complexity is finite, and it depends linearly on the VC dimension on the class of target functions.
Let formula_1 be a space which we call the input space, and formula_2 be a space which we call the output space, and let formula_3 denote the product formula_4. For example, in the setting of binary classification, formula_1 is typically a finite-dimensional vector space and formula_2 is the set formula_7.

Fix a hypothesis space formula_8 of functions formula_9. A learning algorithm over formula_8 is a computable map from formula_11 to formula_8. In other words, it is an algorithm that takes as input a finite sequence of training samples and outputs a function from formula_1 to formula_2. Typical learning algorithms include empirical risk minimization, without or with Tikhonov regularization.

Fix a loss function formula_15, for example, the square loss formula_16. For a given distribution formula_17 on formula_4, the expected risk of a hypothesis (a function) formula_19 is

In our setting, we have formula_21 where formula_22 is a learning algorithm and formula_23 is a sequence of vectors which are all drawn independently from formula_17. Define the optimal riskformula_25Set formula_26 for each formula_27. Note that 
formula_28 is a random variable and depends on the random variable formula_29, which is drawn from the distribution formula_30. The algorithm formula_22 is called consistent if formula_32 probabilistically converges to formula_33, in other words, for all "ε", "δ" > 0, there exists a positive integer "N" such that for all "n" ≥ "N", we haveformula_34The sample complexity of formula_22 is then the minimum "N" for which this holds, as a function of "ρ", "ε", and "δ". We write the sample complexity as formula_36 to emphasize that this value of "N" depends on "ρ", "ε", and "δ". If formula_22 is not consistent, then we set formula_38. If there exists an algorithm for which formula_36 is finite, then we say that the hypothesis space formula_40 is learnable.

In words, the sample complexity formula_36 defines the rate of consistency of the algorithm: given a desired accuracy "ε" and confidence "δ", one needs to sample formula_36 data points to guarantee that the risk of the output function is within "ε" of the best possible, with probability at least 1 - "δ".

In probabilistically approximately correct (PAC) learning, one is concerned with whether the sample complexity is "polynomial", that is, whether formula_36 is bounded by a polynomial in 1/"ε" and 1/"δ". If formula_36 is polynomial for some learning algorithm, then one says that the hypothesis space 
formula_45 is PAC-learnable. Note that this is a stronger notion than being learnable.

One can ask whether there exists a learning algorithm so that the sample complexity is finite in the strong sense, that is, there is a bound on the number of samples needed so that the algorithm can learn any distribution over the input-output space with a specified target error. More formally, one asks whether there exists a learning algorithm formula_22 such that, for all "ε", "δ" > 0, there exists a positive integer "N" such that for all "n" ≥ "N", we haveformula_47where formula_48, with formula_23 as above. The No Free Lunch Theorem says that without restrictions on the hypothesis space formula_8, this is not the case, i.e., there always exist "bad" distributions for which the sample complexity is arbitrarily large. 

Thus, in order to make statements about the rate of convergence of the quantity
formula_51
one must either

The latter approach leads to concepts such as VC dimension and Rademacher complexity which control the complexity of the space formula_8. A smaller hypothesis space introduces more bias into the inference process, meaning that formula_55 may be greater than the best possible risk in a larger space. However, by restricting the complexity of the hypothesis space it becomes possible for an algorithm to produce more uniformly consistent functions. This trade-off leads to the concept of regularization.

It is a theorem from VC theory that the following three statements are equivalent for a hypothesis space formula_8:
This gives a way to prove that certain hypothesis spaces are PAC learnable, and by extension, learnable.

Let "X" = R, "Y" = {-1, 1}, and let formula_8 be the space of affine functions on "X", that is, functions of the form formula_61 for some formula_62. This is the linear classification with offset learning problem. Now, note that four coplanar points in a square cannot be shattered by any affine function, since no affine function can be positive on two diagonally opposite vertices and negative on the remaining two. Thus, the VC dimension of formula_8 is formula_64, in particular finite. It follows by the above characterization of PAC-learnable classes that formula_8 is PAC-learnable, and by extension, learnable.

Suppose formula_8 is a class of binary functions (functions to {0,1}). Then, formula_8 is formula_68-PAC-learnable with a sample of size:

formula_69
where formula_70 is the VC dimension of formula_8.
Moreover, any formula_68-PAC-learning algorithm for formula_8 must have sample-complexity:
formula_74
Thus, the sample-complexity is a linear function of the VC dimension of the hypothesis space.

Suppose formula_8 is a class of real-valued functions with range in [0,T]. Then, formula_8 is formula_68-PAC-learnable with a sample of size:

formula_78
where formula_79 is Pollard's pseudo-dimension of formula_8.

In addition to the supervised learning setting, sample complexity is relevant to semi-supervised learning problems including active learning, where the algorithm can ask for labels to specifically chosen inputs in order to reduce the cost of obtaining many labels. The concept of sample complexity also shows up in reinforcement learning, online learning, and unsupervised algorithms, e.g. for dictionary learning.

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Курсовая работа\n",
    "1. [Сборка датасета](#dataset_collect)\n",
    "    1. [Коллокации](#collocations_collect) (для оценки качества)\n",
    "    2. [Документы](#documents_collect)\n",
    "2. [SyntaxNet](#syntaxnet)\n",
    "    1. [Запуск SyntaxNet](#syntaxnet_run)\n",
    "    2. [Обработка результатов SyntaxNet'а](#syntaxnet_postprocess)\n",
    "    \n",
    "    #### To be done...\n",
    "3. [TopMine](#topmine)\n",
    "4. Запуск тематической модели\n",
    "5. Обучение итоговой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "import ngrammer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_XML = \"../wiki.xml\"\n",
    "WIKIEXTRACTOR_FOLDER = \"../wikiextractor/\"\n",
    "WIKITEXTS_JSON_FOLDER = \"../wikitexts_json/\"\n",
    "\n",
    "WIKI_COLLECTION = \"../collection/\"\n",
    "COLLOCS_FILE = \"../collocs.txt\"\n",
    "\n",
    "SYNTAXNET_INPUT = \"../sentences.txt\"\n",
    "SYNTAXNET_OUTPUT = \"../syntaxnet_out.txt\"\n",
    "\n",
    "LEMMATIZED_COLLECTION = \"../lemmatized_collection/\"\n",
    "STOPWORDS_FILE = \"../stopwords.txt\"\n",
    "NGRAMMS_FILE = \"../ngramms.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset_collect'></a>\n",
    "# Сборка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='collocations_collect'></a>\n",
    "## Коллокации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выделяем текст статьи из xml файла**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(WIKI_XML, \"r\") as f:\n",
    "    data_xml = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data_xml, \"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [tmp.text for tmp in soup.find_all('text')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выделяем гиперссылки**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperlinks = []\n",
    "for text in texts:\n",
    "    for open_brackets, close_brackets in zip(re.finditer('\\[\\[', text), re.finditer('\\]\\]', text)):\n",
    "        start_ind = open_brackets.span()[1]\n",
    "        close_ind = close_brackets.span()[0]\n",
    "        hyperlinks.append(text[start_ind:close_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Фильтруем теги, разбиваем ссылки с множественными словами**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_hyperlinks = []\n",
    "for hl in hyperlinks:\n",
    "    # --- Теги ---\n",
    "    # Игнорируем, если тег File:\n",
    "    if re.match(\"File:\", hl) and (re.match(\"File:\", hl).span()[0] == 0):\n",
    "        continue\n",
    "    # Добавляем имя категории из тега Category:\n",
    "    if re.match(\"Category:\", hl) and (re.match(\"Category:\", hl).span()[0] == 0):\n",
    "        filtered_hyperlinks.append(hl[re.match(\"Category:\", hl).span()[1]:])\n",
    "        continue\n",
    "    \n",
    "    # --- НеТеги ---\n",
    "    # Убираем все что в скобках\n",
    "    hl = re.sub(\"\\(.+\\)\", \"\", hl)\n",
    "    # Разделяем мультиназвания (через | или and) на раздельные коллокации\n",
    "    sub_hl = list(re.split(\"\\|| and \", hl))\n",
    "    \n",
    "    filtered_hyperlinks += sub_hl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Окончательная обработка, получаем коллокации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collocs = []\n",
    "for hl in filtered_hyperlinks:\n",
    "    hl = hl.strip()\n",
    "    hl = hl.lower()\n",
    "    \n",
    "    # Если это инициалы -> не добавляем\n",
    "    if re.match(\"(.\\. .\\. .+)|(.+ .\\. .+)\", hl) and (len(hl.split(' ')) == 3):\n",
    "        continue\n",
    "        \n",
    "    hl = re.sub(\" +\", \" \", hl)\n",
    "    hl = re.sub(\"[^ A-Za-z-]+\", \"\", hl)\n",
    "    hl = hl.strip()\n",
    "    \n",
    "    flag = (hl not in collocs) and \\\n",
    "           (len(hl.split(' ')) > 1) and \\\n",
    "           (len(hl.split(' ')) < 5)\n",
    "    if flag:\n",
    "        collocs.append(hl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computer scientist',\n",
       " 'mikhail moiseevich bongard',\n",
       " 'pattern recognition',\n",
       " 'gdel escher bach',\n",
       " 'douglas hofstadter']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collocs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Записываем полученные коллокации в файл**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(COLLOCS_FILE, \"w\") as f:\n",
    "    f.write('\\n'.join(collocs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Статистика**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperlinks:\t9200\n",
      "filtered hl:\t11375\n",
      "collocations:\t3651\n"
     ]
    }
   ],
   "source": [
    "print(\"hyperlinks:\\t{}\".format(len(hyperlinks)))\n",
    "print(\"filtered hl:\\t{}\".format(len(filtered_hyperlinks)))\n",
    "print(\"collocations:\\t{}\".format(len(collocs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='documents_collect'></a>\n",
    "## Документы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обрабатываем текст с помощью [wikiextractor](https://github.com/attardi/wikiextractor). Получаем json файлы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(WIKIEXTRACTOR_FOLDER + \"/WikiExtractor.py \" + \"--json \" + \"-o \" + WIKITEXTS_JSON_FOLDER + \" \" + WIKI_XML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Читаем их**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_str = \"\"\n",
    "for subdir in os.listdir(WIKITEXTS_JSON_FOLDER):\n",
    "    for file in os.listdir(WIKITEXTS_JSON_FOLDER + \"/\" + subdir):\n",
    "        with open(WIKITEXTS_JSON_FOLDER + subdir + \"/\" + file, \"r\") as f:\n",
    "            texts_str += f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for text in texts_str.split('\\n'):\n",
    "    if text != '':  # in case of double \\n\n",
    "        texts.append(json.loads(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохраняем полученные документы в коллекцию + миниобработка**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "    filename = text[\"title\"]\n",
    "    filename = re.sub(\"[^A-Za-zА-Яа-я0-9 ]\", \"\", filename)\n",
    "    filename = re.sub(\" +\", \"_\", filename)\n",
    "    article = '\\n'.join(text[\"text\"].split('\\n')[2:])  # remove name of article at the start\n",
    "    with open(WIKI_COLLECTION + filename + \".txt\", \"w\") as f:\n",
    "        f.write(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='syntaxnet'></a>\n",
    "# SyntaxNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='syntaxnet_run'></a>\n",
    "## Запуск SyntaxNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Загружаем коллекцию, удаляем пунктуацию и переводим в нижний регистр. Получаем список предложений**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "document_ids = []\n",
    "\n",
    "for doc_id, filename in enumerate(os.listdir(WIKI_COLLECTION)):\n",
    "    with open(WIKI_COLLECTION + filename, \"r\") as f:\n",
    "        sentences_raw = f.read().split('.')\n",
    "    \n",
    "    for sentence in sentences_raw:\n",
    "        sentence_nopunct = re.sub(\"[^A-Za-zА-Яа-я ]\", '', sentence)\n",
    "        sentence_nopunct = sentence_nopunct.lower().strip()\n",
    "        if len(sentence_nopunct) > 1:\n",
    "            sentences.append(sentence_nopunct)\n",
    "            document_ids.append(doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишем в файл каждое предложение в отдельной строке**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(SYNTAXNET_INPUT, \"w\") as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(\"{}\\n\".format(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Прогоняем полученный файл через SyntaxNet** (docker взял [здесь](https://hub.docker.com/r/inemo/syntaxnet_eng/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! It took 508.83 s.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "if os.system(\"./run_syntaxnet.sh {} {}\".format(SYNTAXNET_INPUT, SYNTAXNET_OUTPUT)) != 0:\n",
    "    print(\"Something was wrong\")\n",
    "print(\"Done! It took {:.2f} s.\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выход syntaxnet'а"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syntaxnet_out = pd.read_table(SYNTAXNET_OUTPUT, header=None,\n",
    "                              dtype={0: np.int, 6: np.int},\n",
    "                              quoting=csv.QUOTE_NONE, engine='c'\n",
    "                             )[[0, 1, 3, 6, 7]].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>2</td>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>machine</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>3</td>\n",
       "      <td>nmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>learning</td>\n",
       "      <td>VERB</td>\n",
       "      <td>10</td>\n",
       "      <td>csubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>6</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>kernel</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>6</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1     3   6         7\n",
       "0  1        in   ADP   2      case\n",
       "1  2   machine  NOUN   3      nmod\n",
       "2  3  learning  VERB  10     csubj\n",
       "3  4       the   DET   6       det\n",
       "4  5    kernel  NOUN   6  compound"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntaxnet_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**К каждому слову добавим id предложения и его лемму**<br>\n",
    "Нам повезло, что SyntaxNet говорит нам о части речи слова, так как это повысит качетсво лемматизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syntaxnet_out = pd.read_table(SYNTAXNET_OUTPUT, header=None,\n",
    "                              dtype={0: np.int, 6: np.int},\n",
    "                              quoting=csv.QUOTE_NONE, engine='c'\n",
    "                             )[[0, 1, 3, 6, 7]].fillna(\"\")\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "syntaxnet_out.columns = [[\"word_id\", \"word\", \"POS\",\n",
    "                          \"parent_id\", \"dependency\"]]\n",
    "\n",
    "syntaxnet_out[\"word_id\"] -= 1\n",
    "syntaxnet_out[\"parent_id\"] -= 1\n",
    "\n",
    "cur_sentence_id = -1\n",
    "sentence_ids = []\n",
    "lemmas = []\n",
    "\n",
    "for word_id, word, pos in zip(syntaxnet_out[\"word_id\"],\n",
    "                              syntaxnet_out[\"word\"],\n",
    "                              syntaxnet_out[\"POS\"]):\n",
    "    if word_id == 0:\n",
    "        cur_sentence_id += 1\n",
    "        \n",
    "    if pos == \"VERB\":\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "    elif pos == \"ADJ\":\n",
    "        lemma = lemmatizer.lemmatize(word, pos='a')\n",
    "    elif pos == \"ADV\":\n",
    "        lemma = lemmatizer.lemmatize(word, pos='r')\n",
    "    else:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='n')\n",
    "        \n",
    "    lemmas.append(lemma)\n",
    "    sentence_ids.append(cur_sentence_id)\n",
    "\n",
    "syntaxnet_out[\"sentence_id\"] = sentence_ids\n",
    "syntaxnet_out[\"lemma\"] = lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Добавим id документа к каждому слову, чтобы опять собрать коллекцию**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need document_ids from cells above\n",
    "i = 0\n",
    "doc_ids = []\n",
    "prev_sentence_id = 0\n",
    "for sentence_id in syntaxnet_out[\"sentence_id\"]:\n",
    "    if sentence_id != prev_sentence_id:\n",
    "        i += 1\n",
    "        prev_sentence_id = sentence_id\n",
    "    doc_ids.append(document_ids[i])\n",
    "syntaxnet_out[\"doc_id\"] = doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>dependency</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>1</td>\n",
       "      <td>case</td>\n",
       "      <td>0</td>\n",
       "      <td>in</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>machine</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>2</td>\n",
       "      <td>nmod</td>\n",
       "      <td>0</td>\n",
       "      <td>machine</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>learning</td>\n",
       "      <td>VERB</td>\n",
       "      <td>9</td>\n",
       "      <td>csubj</td>\n",
       "      <td>0</td>\n",
       "      <td>learn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>5</td>\n",
       "      <td>det</td>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>kernel</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>5</td>\n",
       "      <td>compound</td>\n",
       "      <td>0</td>\n",
       "      <td>kernel</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_id      word   POS  parent_id dependency  sentence_id    lemma  doc_id\n",
       "0        0        in   ADP          1       case            0       in       0\n",
       "1        1   machine  NOUN          2       nmod            0  machine       0\n",
       "2        2  learning  VERB          9      csubj            0    learn       0\n",
       "3        3       the   DET          5        det            0      the       0\n",
       "4        4    kernel  NOUN          5   compound            0   kernel       0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntaxnet_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Записываем полученную лемматизированную коллекцию в файл**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prev_doc_id = 0\n",
    "prev_sentence_id = 0\n",
    "cur_text = \"\"\n",
    "for lemma, sentence_id, doc_id in zip(syntaxnet_out[\"lemma\"], syntaxnet_out[\"sentence_id\"], syntaxnet_out[\"doc_id\"]):\n",
    "    if sentence_id != prev_sentence_id:\n",
    "        cur_text += ' .'\n",
    "        prev_sentence_id = sentence_id\n",
    "    if doc_id != prev_doc_id:\n",
    "        with open(LEMMATIZED_COLLECTION + \"{:04d}.txt\".format(doc_id), \"w\") as f:\n",
    "            f.write(cur_text)\n",
    "        cur_text = \"\"\n",
    "        prev_doc_id = doc_id\n",
    "    # do not add space at start of document\n",
    "    if(len(cur_text) != 0):\n",
    "        cur_text += ' '\n",
    "    cur_text += lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"syntaxnet_postprocess\"></a>\n",
    "## Обработка результатов SyntaxNet'а"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Открываем выход syntaxnet'а и составляем список деревьев**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(SYNTAXNET_OUTPUT, \"r\") as f:\n",
    "    syntaxnet_out = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# May be removed\n",
    "with open(SYNTAXNET_INPUT, \"r\") as f:\n",
    "    sentences = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_sentences = []\n",
    "sentence = []\n",
    "for line in syntaxnet_out.split(\"\\n\"):\n",
    "    if len(line) == 0:\n",
    "        processed_sentences.append(sentence)\n",
    "        sentence = []\n",
    "    else:\n",
    "        word = line.split(\"\\t\")\n",
    "        sentence.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deps = []\n",
    "for sentence in processed_sentences:\n",
    "    s = ''\n",
    "    for line in sentence:\n",
    "        s += \"\\t\".join(line) + '\\n'\n",
    "    deps.append(s)\n",
    "del deps[-1] # empty sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsic can be extended to measure the dependence of multiple random variables\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('extended', ['hsic', 'can', 'be', Tree('measure', ['to', Tree('dependence', ['the', Tree('variables', ['of', 'multiple', 'random'])])])])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "sent_dep = deps[50]\n",
    "graph = nltk.DependencyGraph(tree_str=sent_dep)\n",
    "tree = graph.tree()\n",
    "print(sentences[50])\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Выделим ветки деревьев **<br>\n",
    "Позже можно будет использовать другие признаки, например находятся ли слова в одном поддереве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_branches(tree):\n",
    "    def _get_branches(subtree, subbranches):\n",
    "        subbranches *= len(subtree)\n",
    "        for i, node in enumerate(subtree):\n",
    "            if isinstance(node, str):\n",
    "                subbranches[i] += (' ' + node)\n",
    "            if isinstance(node, nltk.tree.Tree):\n",
    "                subsubbranches = _get_branches(node, [subbranches[i] + ' ' + node.label()])\n",
    "                del subbranches[i]\n",
    "                subbranches[i:i] = subsubbranches\n",
    "        return subbranches\n",
    "    \n",
    "    branches = [tree.label()]\n",
    "    return _get_branches(tree, branches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax_branches = []\n",
    "sentences_branches_ids = []\n",
    "for i, sent_dep in enumerate(deps):\n",
    "    graph = nltk.DependencyGraph(tree_str=sent_dep)\n",
    "    tree = graph.tree()\n",
    "    subbranches = get_branches(tree)\n",
    "    syntax_branches += subbranches\n",
    "    sentences_branches_ids += [i] * len(subbranches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['extended hsic',\n",
       " 'extended can',\n",
       " 'extended be',\n",
       " 'extended measure to',\n",
       " 'extended measure dependence the',\n",
       " 'extended measure dependence variables of',\n",
       " 'extended measure dependence variables multiple',\n",
       " 'extended measure dependence variables random']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntax_branches[1123:1131]\n",
    "\n",
    "# Example: feature datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='topmine'></a>\n",
    "# TopMine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Загружаем лемматизированную коллекцию и стоп-слова**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora = []\n",
    "\n",
    "for doc_id, filename in enumerate(os.listdir(LEMMATIZED_COLLECTION)):\n",
    "    with open(LEMMATIZED_COLLECTION + filename, \"r\") as f:\n",
    "        corpora.append(f.read().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords:\t175\n"
     ]
    }
   ],
   "source": [
    "with open(STOPWORDS_FILE, \"r\") as f:\n",
    "    stopwords = f.read().split('\\n')\n",
    "print(\"stopwords:\\t{}\".format(len(stopwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запускаем ngrammer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ng = ngrammer.NGrammer()\n",
    "ng.delimiters = stopwords\n",
    "ng.delimiters_regex = ['[^a-z ]+']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng.frequentPhraseMining(corpora, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramms = []\n",
    "for document in corpora:\n",
    "    res = ng.ngramm(document, threshhold=3)\n",
    "    ngramms += res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete points and stopwords\n",
    "ngramms = [ngramm for ngramm in ngramms if (ngramm != '.') and (ngramm not in stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['supervise learn',\n",
       " 'machine learning',\n",
       " 'task',\n",
       " 'learn',\n",
       " 'function',\n",
       " 'map',\n",
       " 'input',\n",
       " 'output',\n",
       " 'base',\n",
       " 'example']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramms[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оставим только ngramm'ы, в которых больше одного токена и они не повторяются**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ngramms = []\n",
    "for ngramm in ngramms:\n",
    "    if (ngramm not in unique_ngramms) and (len(ngramm.split(' ')) > 1):\n",
    "        unique_ngramms.append(ngramm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['optimization problem',\n",
       " 'prior probability',\n",
       " 'discriminative model',\n",
       " 'special case',\n",
       " 'probability distribution']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_ngramms[47:52]\n",
    "#Ngramms example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique ngramms:\t303\n"
     ]
    }
   ],
   "source": [
    "print(\"unique ngramms:\\t{}\".format(len(unique_ngramms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишем полученные уникальные ngramm'ы в файл**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(NGRAMMS_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(unique_ngramms))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

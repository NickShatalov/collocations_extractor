in machine learning the kernel embedding of distributions also called the kernel mean or mean map comprises a class of nonparametric methods in which a probability distribution is represented as an element of a reproducing kernel hilbert space rkhs
a generalization of the individual datapoint feature mapping done in classical kernel methods the embedding of distributions into infinitedimensional feature spaces can preserve all of the statistical features of arbitrary distributions while allowing one to compare and manipulate distributions using hilbert space operations such as inner products distances projections linear transformations and spectral analysis
this learning framework is very general and can be applied to distributions over any space formula on which a sensible kernel function measuring similarity between elements of formula may be defined
for example various kernels have been proposed for learning from data which are vectors in formula discrete classescategories strings graphsnetworks images time series manifolds dynamical systems and other structured objects
the theory behind kernel embeddings of distributions has been primarily developed by alex smola le song  arthur gretton and bernhard schlkopf
a review of recent works on kernel embedding of distributions can be found in
the analysis of distributions is fundamental in machine learning and statistics and many algorithms in these fields rely on information theoretic approaches such as entropy mutual information or kullbackleibler divergence
however to estimate these quantities one must first either perform density estimation or employ sophisticated spacepartitioningbiascorrection strategies which are typically infeasible for highdimensional data
commonly methods for modeling complex distributions rely on parametric assumptions that may be unfounded or computationally challenging e
gaussian mixture models while nonparametric methods like kernel density estimation note the smoothing kernels in this context have a different interpretation than the kernels discussed here or characteristic function representation via the fourier transform of the distribution break down in highdimensional settings
methods based on the kernel embedding of distributions sidestep these problems and also possess the following advantages thus learning via the kernel embedding of distributions offers a principled dropin replacement for information theoretic approaches and is a framework which not only subsumes many popular methods in machine learning and statistics as special cases but also can lead to entirely new learning algorithms
let formula denote a random variable with codomain formula and distribution formula
given a kernel formula on formula the moorearonszajn theorem asserts the existence of a rkhs formula a hilbert space of functions formula equipped with inner products formula and norms formula in which the element formula satisfies the reproducing property formula
one may alternatively consider formula an implicit feature mapping formula from formula to formula which is therefore also called the feature space so that formula can be viewed as a measure of similarity between points formula
while the similarity measure is linear in the feature space it may be highly nonlinear in the original space depending on the choice of kernel
the kernel embedding of the distribution formula in formula also called the kernel mean or mean map is given byif formula allows a square integrable density formula then formula where formula is the hilbertschmidt integral operator
a kernel is characteristic if the mean embedding formula is injective
each distribution can thus be uniquely represented in the rkhs and all statistical features of distributions are preserved by the kernel embedding if a characteristic kernel is used
given formula training examples formula drawn independently and identically distributed i
from formula the kernel embedding of formula can be empirically estimated asif formula denotes another random variable for simplicity assume the codomain of formula is also formula with the same kernel formula which satisfies formula then the joint distribution formula can be mapped into a tensor product feature space formula via by the equivalence between a tensor and a linear map this joint embedding may be interpreted as an uncentered crosscovariance operator formula from which the crosscovariance of meanzero functions formula can be computed as given formula pairs of training examples formula drawn i
from formula we can also empirically estimate the joint distribution kernel embedding viagiven a conditional distribution formula one can define the corresponding rkhs embedding as note that the embedding of formula thus defines a family of points in the rkhs indexed by the values formula taken by conditioning variable formula
by fixing formula to a particular value we obtain a single element in formula and thus it is natural to define the operatorwhich given the feature mapping of formula outputs the conditional embedding of formula given formula
assuming that for all formula it can be shown that this assumption is always true for finite domains with characteristic kernels but may not necessarily hold for continuous domains
nevertheless even in cases where the assumption fails formula may still be used to approximate the conditional kernel embedding formula and in practice the inversion operator is replaced with a regularized version of itself formula where formula denotes the identity matrix
given training examples formula the empirical kernel conditional embedding operator may be estimated as where formula are implicitly formed feature matrices formula is the gram matrix for samples of formula and formula is a regularization parameter needed to avoid overfitting
thus the empirical estimate of the kernel conditional embedding is given by a weighted sum of samples of formula in the feature spaceon compact subsets of formula is universal
and support of formula is an entire space then formula is universal
for example gaussian rbf is universal sinc kernel is not universal
this section illustrates how basic probabilistic rules may be reformulated as multilinear algebraic operations in the kernel embedding framework and is primarily based on the work of song et al
the following notation is adopted in practice all embeddings are empirically estimated from data formula and it assumed that a set of samples formula may be used to estimate the kernel embedding of the prior distribution formula
in probability theory the marginal distribution of formula can be computed by integrating out formula from the joint density including the prior distribution on formulathe analog of this rule in the kernel embedding framework states that formula the rkhs embedding of formula can be computed viain practical implementations the kernel sum rule takes the following formwhere formula is the empirical kernel embedding of the prior distribution formula formula and formula are gram matrices with entries formula respectively
in probability theory a joint distribution can be factorized into a product between conditional and marginal distributions the analog of this rule in the kernel embedding framework states that formula the joint embedding of formula can be factorized as a composition of conditional embedding operator with the autocovariance operator associated with formulain practical implementations the kernel chain rule takes the following formin probability theory a posterior distribution can be expressed in terms of a prior distribution and a likelihood function as the analog of this rule in the kernel embedding framework expresses the kernel embedding of the conditional distribution in terms of conditional embedding operators which are modified by the prior distributionin practical implementations the kernel bayes rule takes the following formwhere formula
two regularization parameters are used in this framework formula for the estimation of formula and formula for the estimation of the final conditional embedding operator formula
the latter regularization is done on square of formula because formula may not be positive definite
the maximum mean discrepancy mmd is a distancemeasure between distributions formula and formula which is defined as the squared distance between their embeddings in the rkhs while most distancemeasures between distributions such as the widely used kullbackleibler divergence either require density estimation either parametrically or nonparametrically or space partitioningbias correction strategies the mmd is easily estimated as an empirical mean which is concentrated around the true value of the mmd
the characterization of this distance as the maximum mean discrepancy refers to the fact that computing the mmd is equivalent to finding the rkhs function that maximizes the difference in expectations between the two probability distributions given n training examples from formula and m samples from formula one can formulate a test statistic based on the empirical estimate of the mmdto obtain a twosample test of the null hypothesis that both samples stem from the same distribution i
formula against the broad alternative formula
although learning algorithms in the kernel embedding framework circumvent the need for intermediate density estimation one may nonetheless use the empirical embedding to perform density estimation based on n samples drawn from an underlying distribution formula
this can be done by solving the following optimization problem where the maximization is done over the entire space of distributions on formula
here formula is the kernel embedding of the proposed density formula and formula is an entropylike quantity e
entropy kl divergence bregman divergence
the distribution which solves this optimization may be interpreted as a compromise between fitting the empirical kernel means of the samples well while still allocating a substantial portion of the probability mass to all regions of the probability space much of which may not be represented in the training examples
in practice a good approximate solution of the difficult optimization may be found by restricting the space of candidate densities to a mixture of m candidate distributions with regularized mixing proportions
connections between the ideas underlying gaussian processes and conditional random fields may be drawn with the estimation of conditional probability distributions in this fashion if one views the feature mappings associated with the kernel as sufficient statistics in generalized possibly infinitedimensional exponential families
a measure of the statistical dependence between random variables formula and formula from any domains on which sensible kernels can be defined can be formulated based on the hilbertschmidt independence criterion and can be used as a principled replacement for mutual information pearson correlation or any other dependence measure used in learning algorithms
most notably hsic can detect arbitrary dependencies when a characteristic kernel is used in the embeddings hsic is zero if and only if the variables are independent and can be used to measure dependence between different types of data e
images and text captions
given n i
samples of each random variable a simple parameterfree unbiased estimator of hsic which exhibits concentration about the true value can be computed in formula time where the gram matrices of the two datasets are approximated using formula with formula
the desirable properties of hsic have led to the formulation of numerous algorithms which utilize this dependence measure for a variety of common machine learning tasks such as feature selection bahsic  clustering cluhsic  and dimensionality reduction muhsic
hsic can be extended to measure the dependence of multiple random variables
the question of when hsic captures independence in this case has recently been studied  for more than two variablesbelief propagation is a fundamental algorithm for inference in graphical models in which nodes repeatedly pass and receive messages corresponding to the evaluation of conditional expectations
in the kernel embedding framework the messages may be represented as rkhs functions and the conditional distribution embeddings can be applied to efficiently compute message updates
given n samples of random variables represented by nodes in a markov random field the incoming message to node t from node u can be expressed as formula if it assumed to lie in the rkhs
the kernel belief propagation update message from t to node s is then given by where formula denotes the elementwise vector product formula is the set of nodes connected to t excluding node s formula formula are the gram matrices of the samples from variables formula respectively and formula is the feature matrix for the samples from formula
thus if the incoming messages to node t are linear combinations of feature mapped samples from formula then the outgoing message from this node is also a linear combination of feature mapped samples from formula
this rkhs function representation of messagepassing updates therefore produces an efficient belief propagation algorithm in which the potentials are nonparametric functions inferred from the data so that arbitrary statistical relationships may be modeled
in the hidden markov model hmm two key quantities of interest are the transition probabilities between hidden states formula and the emission probabilities formula for observations
using the kernel conditional distribution embedding framework these quantities may be expressed in terms of samples from the hmm
a serious limitation of the embedding methods in this domain is the need for training samples containing hidden states as otherwise inference with arbitrary distributions in the hmm is not possible
one common use of hmms is filtering in which the goal is to estimate posterior distribution over the hidden state formula at time step t given a history of previous observations formula from the system
in filtering a belief state formula is recursively maintained via a prediction step where updates formula are computed by marginalizing out the previous hidden state followed by a conditioning step where updates formula are computed by applying bayes rule to condition on a new observation
the rkhs embedding of the belief state at time t can be recursively expressed as by computing the embeddings of the prediction step via the kernel sum rule and the embedding of the conditioning step via kernel bayes rule
assuming a training sample formula is given one can in practice estimate formula and filtering with kernel embeddings is thus implemented recursively using the following updates for the weights formula where formula denote the gram matrices of formula and formula respectively formula is a transfer gram matrix defined as formula and formula
the support measure machine smm is a generalization of the support vector machine svm in which the training examples are probability distributions paired with labels formula
smms solve the standard svm dual optimization problem using the following expected kernelwhich is computable in closed form for many common specific distributions formula such as the gaussian distribution combined with popular embedding kernels formula e
the gaussian kernel or polynomial kernel or can be accurately empirically estimated from i
samples formula viaunder certain choices of the embedding kernel formula the smm applied to training examples formula is equivalent to a svm trained on samples formula and thus the smm can be viewed as a flexible svm in which a different datadependent kernel specified by the assumed form of the distribution formula may be placed on each training point
the goal of domain adaptation is the formulation of learning algorithms which generalize well when the training and test data have different distributions
given training examples formula and a test set formula where the formula are unknown three types of differences are commonly assumed between the distribution of the training examples formula and the test distribution formulaby utilizing the kernel embedding of marginal and conditional distributions practical approaches to deal with the presence of these types of differences between training and test domains can be formulated
covariate shift may be accounted for by reweighting examples via estimates of the ratio formula obtained directly from the kernel embeddings of the marginal distributions of formula in each domain without any need for explicit estimation of the distributions
target shift which cannot be similarly dealt with since no samples from formula are available in the test domain is accounted for by weighting training examples using the vector formula which solves the following optimization problem where in practice empirical approximations must be used to deal with location scale conditional shift one can perform a ls transformation of the training points to obtain new transformed training data formula where formula denotes the elementwise vector product
to ensure similar distributions between the new transformed training samples and the test data formula are estimated by minimizing the following empirical kernel embedding distance in general the kernel embedding methods for dealing with ls conditional shift and target shift may be combined to find a reweighted transformation of the training data which mimics the test distribution and these methods may perform well even in the presence of conditional shifts other than locationscale changes
given n sets of training examples sampled i
from distributions formula the goal of domain generalization is to formulate learning algorithms which perform well on test examples sampled from a previously unseen domain formula where no data from the test domain is available at training time
if conditional distributions formula are assumed to be relatively similar across all domains then a learner capable of domain generalization must estimate a functional relationship between the variables which is robust to changes in the marginals formula
based on kernel embeddings of these distributions domain invariant component analysis dica is a method which determines the transformation of the training data that minimizes the difference between marginal distributions while preserving a common conditional distribution shared between all training domains
dica thus extracts invariants features that transfer across domains and may be viewed as a generalization of many popular dimensionreduction methods such as kernel principal component analysis transfer component analysis and covariance operator inverse regression
defining a probability distribution formula on the rkhs formula with formula dica measures dissimilarity between domains via distributional variance which is computed as so formula is a formula gram matrix over the distributions from which the training data are sampled
finding an orthogonal transform onto a lowdimensional subspace b in the feature space which minimizes the distributional variance dica simultaneously ensures that b aligns with the bases of a central subspace c for which formula becomes independent of formula given formula across all domains
in the absence of target values formula an unsupervised version of dica may be formulated which finds a lowdimensional subspace that minimizes distributional variance while simultaneously maximizing the variance of formula in the feature space across all domains rather than preserving a central subspace
in distribution regression the goal is to regress from probability distributions to reals or vectors
many important machine learning and statistical tasks fit into this framework including multiinstance learning and point estimation problems without analytical solution such as hyperparameter or entropy estimation
in practice only samples from sampled distributions are observable and the estimates have to rely on similarities computed between sets of points
distribution regression has been successfully applied for example in supervised entropy learning and aerosol prediction using multispectral satellite images
given formula training data where the formula bag contains samples from a probability distribution formula and the formula output label is formula one can tackle the distribution regression task by taking the embeddings of the distributions and learning the regressor from the embeddings to the outputs
in other words one can consider the following kernel ridge regression problem formulawhere formula with a formula kernel on the domain of formulas formula formula is a kernel on the embedded distributions and formula is the rkhs determined by formula
examples for formula include the linear kernel formula the gaussian kernel formula the exponential kernel formula the cauchy kernel formula the generalized tstudent kernel formula or the inverse multiquadrics kernel formula
the prediction on a new distribution formula takes the simple analytical formwhere formula formula formula formula
under mild regularity conditions this estimator can be shown to be consistent and it can achieve the onestage sampled as if one had access to the true formulas minimax optimal rate
in the formula objective function formulas are real numbers the results can also be extended to the case when formulas are formuladimensional vectors or more generally elements of a separable hilbert space using operatorvalued formula kernels
in this simple example which is taken from song et al
formula are assumed to be discrete random variables which take values in the set formula and the kernel is chosen to be the kronecker delta function so formula
the feature map corresponding to this kernel is the standard basis vector formula
the kernel embeddings of such a distributions are thus vectors of marginal probabilities while the embeddings of joint distributions in this setting are formula matrices specifying joint probability tables and the explicit form of these embeddings isthe conditional distribution embedding operator formula is in this setting a conditional probability tablethus the embeddings of the conditional distribution under a fixed value of formula may be computed asin this discretevalued setting with the kronecker delta kernel the kernel sum rule becomesthe kernel chain rule in this case is given by
aixi is a theoretical mathematical formalism for artificial general intelligence
it combines solomonoff induction with sequential decision theory
aixi was first proposed by marcus hutter in  and the results below are proved in hutters  book universal artificial intelligence
aixi is a reinforcement learning agentit maximizes the expected total rewards received from the environment
intuitively it simultaneously considers every computable hypothesis
in each time step it looks at every possible program and evaluates how many rewards that program generates depending on the next action taken
the promised rewards are then weighted by the subjective belief that this program constitutes the true environment
this belief is computed from the length of the program longer programs are considered less likely in line with occams razor
aixi then selects the action that has the highest expected total reward in the weighted sum of all these programs
the aixi agent interacts sequentially with some stochastic and unknown to aixi environment formula
in step t the agent outputs an action formula andthe environment responds with an observation formula and a reward formula distributed according to the conditional probabilityformula
then this cycle repeats for t
the agent tries to maximize cumulative future reward formula for a fixed lifetime m
given a current time t and history formulathe action aixi outputs is defined aswhere u denotes a monotone universal turing machine andq ranges over all programs on the universal machine u
the parameters to aixi are the universal turing machine and the agents lifetime m
the latter dependence can be removed by the use of discounting
aixis performance is measured by the expected total number of rewards it receives
aixi has been proven to be optimal in the following ways
it was later shown that balanced pareto optimality is subjective and that any policy can be considered pareto optimal which undermines all previous optimality claims for aixi
however aixi does have limitations
it is restricted to maximizing rewards based on percepts as opposed to external states
it also assumes it interacts with the environment solely through action and percept channels preventing it from considering the possibility of being damaged or modified
colloquially this means that it doesnt consider itself to be contained by the environment it interacts with
it also assumes the environment is computable
since aixi is incomputable see below it assigns zero probability to its own existence
like solomonoff induction aixi is incomputable
however there are computable approximations of it
one such approximation is aixitlwhich performs at least as well as the provably best time t and space l limited agent
another approximation to aixi with a restricted environment class is mcaixifacctwwhich has had some success playing simple games such as partially observable pacman
explanationbased learning ebl is a form of machine learning that exploits a very strong or even perfect domain theory in order to make generalizations or form concepts from training examples
an example of ebl using a perfect domain theory is a program that learns to play chess through example
a specific chess position that contains an important feature such as forced loss of black queen in two moves includes many irrelevant features such as the specific scattering of pawns on the board
ebl can take a single training example and determine what are the relevant features in order to form a generalization
a domain theory is perfect or complete if it contains in principle all information needed to decide any question about the domain
for example the domain theory for chess is simply the rules of chess
knowing the rules in principle it is possible to deduce the best move in any situation
however actually making such a deduction is impossible in practice due to combinatoric explosion
ebl uses training examples to make searching for deductive consequences of a domain theory efficient in practice
in essence an ebl system works by finding a way to deduce each training example from the systems existing database of domain theory
having a short proof of the training example extends the domaintheory database enabling the ebl system to find and classify future examples that are similar to the training example very quickly
the main drawback of the methodthe cost of applying the learned proof macros as these become numerouswas analyzed by minton
ebl software takes four inputsan especially good application domain for an ebl is natural language processing nlp
here a rich domain theory i
a natural language grammaralthough neither perfect nor complete is tuned to a particular application or particular language usage using a treebank training examples
rayner pioneered this work
the first successful industrial application was to a commercial nl interface to relational databases
the method has been successfully applied to several largescale natural language parsing system where the utility problem was solved by omitting the original grammar domain theory and using specialized lrparsing techniques resulting in huge speedups at a cost in coverage but with a gain in disambiguation
ebllike techniques have also been applied to surface generation the converse of parsing
when applying ebl to nlp the operationality criteria can be handcrafted or can beinferred from the treebank using either the entropy of its ornodesor a target coveragedisambiguation tradeoff  recallprecision tradeoff  fscore
ebl can also be used to compile grammarbased language models for speech recognition from general unification grammars
note how the utility problem first exposed by minton was solved by discarding the original grammardomain theory and that the quoted articles tend to contain the phrase grammar specializationquite the opposite of the original term explanationbased generalization
perhaps the best name for this technique would be datadriven search space reduction
other people who worked on ebl for nlp include guenther neumann aravind joshi srinivas bangalore and khalil simaan
algorithm selection sometimes also called perinstance algorithm selection or offline algorithm selection is a metaalgorithmic technique to choose an algorithm from a portfolio on an instancebyinstance basis
it is motivated by the observation that on many practical problems algorithms have different performances
that is while one algorithm performs well on some instances it performs poorly on others and vice versa for another algorithm
if we can identify when to use which algorithm we can get the best of both worlds and improve overall performance
this is what algorithm selection aims to do
the only prerequisite for applying algorithm selection techniques is that there exists or that there can be constructed a set of complementary algorithms
given a portfolio formula of algorithms formula a set of instances formula and a cost metric formula the algorithm selection problem consists of finding a mapping formula from instances formula to algorithms formula such that the cost formula across all instances is optimized
a wellknown application of algorithm selection is the boolean satisfiability problem
here the portfolio of algorithms is a set of complementary sat solvers the instances are boolean formulas the cost metric is for example average runtime or number of unsolved instances
so the goal is to select a wellperforming sat solver for each individual instance
in the same way algorithm selection can be applied to many other formulahard problems such as mixed integer programming csp ai planning tsp maxsat qbf and answer set programming
competitionwinning systems in sat are satzilla s and cshcin machine learning algorithm selection is better known as metalearning
the portfolio of algorithms consists of machine learning algorithms e
random forest svm dnn the instances are data sets and the cost metric is for example the error rate
so the goal is to predict which machine learning algorithm will have a small error on each data set
the algorithm selection problem is mainly solved with machine learning techniques
by representing the problem instances by numerical features formula algorithm selection can be seen as a multiclass classification problem by learning a mapping formula for a given instance formula
instance features are numerical representations of instances
for example we can count the number of variables clauses average clause length for boolean formulas or number of samples features class balance for ml data sets to get an impression about their characteristics
we distinguish between two kinds of features depending on the used performance metric formula feature computation can be associated with costs
for example if we use running time as performance metric we include the time to compute our instance features into the performance of an algorithm selection system
sat solving is a concrete example where such feature costs cannot be neglected since instance features for cnf formulas can be either very cheap e
to get the number of variables can be done in constant time for cnfs in the dimacs format or very expensive e
graph features which can cost tens or hundreds of seconds
it is important to take the overhead of feature computation into account in practice in such scenarios otherwise a misleading impression of the performance of the algorithm selection approach is created
for example if the decision which algorithm to choose can be made with prefect accuracy but the features are the running time of the portfolio algorithms there is no benefit to the portfolio approach
this would not be obvious if feature costs were omitted
one of the first successful algorithm selection approaches predicted the performance of each algorithm formula and selecting the algorithm with the best predicted performance formula for a new instance formula
a common assumption is that the given set of instances formula can be clustered into homogeneous subsets and for each of these subsets there is one wellperforming algorithm for all instances in there
so the training consists of identifying the homogeneous clusters via an unsupervised clustering approach and associating an algorithm with each cluster
a new instance is assigned to a cluster and the associated algorithm selected
a more modern approach is costsensitive hierarchical clustering using supervised learning to identify the homogeneous instance subsets
a common approach for multiclass classification is to learn pairwise models between every pair of classes here algorithms and choose the class that was predicted most often by the pairwise models
we can weight the instances of the pairwise prediction problem by the performance difference between the two algorithms
this is motivated by the fact that we care most about getting predictions with large differences correct but the penalty for an incorrect prediction is small if there is almost no performance difference
therefore each instance formula for training a classification model formula vs formula is associated with a cost formula
the algorithm selection problem can be effectively applied under the following assumptionsalgorithm selection is not limited to single domains but can be applied to any kind of algorithm if the above requirements are satisfied
application domains includefor an extensive list of literature about algorithm selection we refer to a literature overview
online algorithm selection in hyperheuristic refers to switching between different algorithms during the solving process
in contrast offline algorithm selection is an oneshot game where we select an algorithm for a given instance only once
an extension of algorithm selection is the perinstance algorithm scheduling problem in which we do not select only one solver but we select a time budget for each algorithm on a perinstance base
this approach improves the performance of selection systems in particular if the instance features are not very informative and a wrong selection of a single solver is likely
given the increasing importance of parallel computationan extension of algorithm selection for parallel computation is parallel portfolio selectionin which we select a subset of the algorithms to simultaneously run in a parallel portfolio
in machine learning and computer vision mtheory is a learning framework inspired by feedforward processing in the ventral stream of visual cortex and originally developed for recognition and classification of objects in visual scenes
mtheory was later applied to other areas such as speech recognition
on certain image recognition tasks algorithms based on a specific instantiation of mtheory hmax achieved humanlevel performance
the core principle of mtheory is extracting representations invariant to various transformations of images translation scale d and d rotation and others
in contrast with other approaches using invariant representations in mtheory they are not hardcoded into the algorithms but learned
mtheory also shares some principles with compressed sensing
the theory proposes multilayered hierarchical learning architecture similar to that of visual cortex
a great challenge in visual recognition tasks is that the same object can be seen in a variety of conditions
it can be seen from different distances different viewpoints under different lighting partially occluded etc
in addition for particular classes objects such as faces highly complex specific transformations may be relevant such as changing facial expressions
for learning to recognize images it is greatly beneficial to factor out these variations
it results in much simpler classification problem and consequently in great reduction of sample complexity of the model
a simple computational experiment illustrates this idea
two instances of a classifier were trained to distinguish images of planes from those of cars
for training and testing of the first instance images with arbitrary viewpoints were used
another instance received only images seen from a particular viewpoint which was equivalent to training and testing the system on invariant representation of the images
one can see that the second classifier performed quite well even after receiving a single example from each category while performance of the first classifier was close to random guess even after seeing  examples
invariant representations has been incorporated into several learning architectures such as neocognitrons
most of these architectures however provided invariance through customdesigned features or properties of architecture itself
while it helps to take into account some sorts of transformations such as translations it is very nontrivial to accommodate for other sorts of transformations such as d rotations and changing facial expressions
mtheory provides a framework of how such transformations can be learned
in addition to higher flexibility this theory also suggests how human brain may have similar capabilities
another core idea of mtheory is close in spirit to ideas from the field of compressed sensing
an implication from johnsonlindenstrauss lemma says that a particular number of images can be embedded into a lowdimensional feature space with the same distances between images by using random projections
this result suggests that dot product between the observed image and some other image stored in memory called template can be used as a feature helping to distinguish the image from other images
the template need not to be anyhow related to the image it could be chosen randomly
the two ideas outlined in previous sections can be brought together to construct a framework for learning invariant representations
the key observation is how dot product between image formula and a template formula behaves when image is transformed by such transformations as translations rotations scales etc
if transformation formula is a member of a unitary group of transformations then the following holdsformulain other words the dot product of transformed image and a template is equal to the dot product of original image and inversely transformed template
for instance for image rotated by  degrees the inversely transformed template would be rotated by  degrees
consider the set of dot products of an image formula to all possible transformations of template formula
if one applies a transformation formula to formula the set would become formula
but because of the property  this is equal to formula
the set formula is equal to just the set of all elements in formula
to see this note that every formula is in formula due to the closure property of groups and for every formula in g there exist its prototype formula such as formula namely formula
thus formula
one can see that the set of dot products remains the same despite that a transformation was applied to the image this set by itself may serve as a very cumbersome invariant representation of an image
more practical representations can be derived from it
in the introductory section it was claimed that mtheory allows to learn invariant representations
this is because templates and their transformed versions can be learned from visual experience  by exposing the system to sequences of transformations of objects
it is plausible that similar visual experiences occur in early period of human life for instance when infants twiddle toys in their hands
because templates may be totally unrelated to images that the system later will try to classify memories of these visual experiences may serve as a basis for recognizing many different kinds of objects in later life
however as it is shown later for some kinds of transformations specific templates are needed
to implement the ideas described in previous sections one need to know how to derive a computationally efficient invariant representation of an image
such unique representation for each image can be characterized as it appears by a set of onedimensional probability distributions empirical distributions of the dotproducts between image and a set of templates stored during unsupervised learning
these probability distributions in their turn can be described by either histograms or a set of statistical moments of it as it will be shown below
orbit formula is a set of images formula generated from a single image formula under the action of the group formula
in other words images of an object and of its transformations correspond to a orbit formula
if two orbits have a point in common they are identical everywhere i
an orbit is an invariant and unique representation of an image
so two images are called equivalent when they belong to the same orbit formula if formula such that formula
conversely two orbits are different if none of the images in one orbit coincide with any image in the other
a natural question arises how can one compare two orbits there are several possible approaches
one of them employs the fact that intuitively two empirical orbits are the same irrespective of the ordering of their points
thus one can consider a probability distribution formula induced by the groups action on images formula formula can be seen as a realization of a random variable
this probability distribution formula can be almost uniquely characterized by formula onedimensional probability distributions formula induced by the onedimensional results of projections formula where formula are a set of templates randomly chosen images based on the cramerwold theorem and concentration of measures
consider formula images formula
let formula  where formula is a universal constant
thenformulawith probability formula for all formula formula formula
this result informally says that an approximately invariant and unique representation of an image formula can be obtained from the estimates of formula d probability distributions formula for formula
the number formula of projections needed to discriminate formula orbits induced by formula images up to precision formula and with confidence formula is formula where formula is a universal constant
to classify an image the following recipe can be usedestimates of such onedimensional probability density functions pdfs formula can be written in terms of histograms as formula where formula is a set of nonlinear functions
these d probability distributions can be characterized with nbin histograms or set of statistical moments
for example hmax represents an architecture in which pooling is done with a max operation
in the recipe for image classification groups of transformations are approximated with finite number of transformations
such approximation is possible only when the group is compact
such groups as all translations and all scalings of the image are not compact as they allow arbitrarily big transformations
however they are locally compact
for locally compact groups invariance is achievable within certain range of transformations
assume that formula is a subset of transformations from formula for which the transformed patterns exist in memory
for an image formula and template formula assume that formula is equal to zero everywhere except some subset of formula
this subset is called support of formula and denoted as formula
it can be proven that if for a transformation formula support set will also lie within formula then signature of formula is invariant with respect to formula
this theorem determines the range of transformations for which invariance is guaranteed to hold
one can see that the smaller is formula the larger is the range of transformations for which invariance is guaranteed to hold
it means that for a group that is only locally compact not all templates would work equally well anymore
preferable templates are those with a reasonably small formula for a generic image
this property is called localization templates are sensitive only to images within a small range of transformations
note that although minimizing formula is not absolutely necessary for the system to work it improves approximation of invariance
requiring localization simultaneously for translation and scale yields a very specific kind of templates gabor functions
the desirability of custom templates for noncompact group is in conflict with the principle of learning invariant representations
however for certain kinds of regularly encountered image transformations templates might be the result of evolutionary adaptations
neurobiological data suggests that there is gaborlike tuning in the first layer of visual cortex
the optimality of gabor templates for translations and scales is a possible explanation of this phenomenon
many interesting transformations of images do not form groups
for instance transformations of images associated with d rotation of corresponding d object do not form a group because it is impossible to define an inverse transformation two objects may looks the same from one angle but different from another angle
however approximate invariance is still achievable even for nongroup transformations if localization condition for templates holds and transformation can be locally linearized
as it was said in the previous section for specific case of translations and scaling localization condition can be satisfied by use of generic gabor templates
however for general case nongroup transformation localization condition can be satisfied only for specific class of objects
more specifically in order to satisfy the condition templates must be similar to the objects one would like to recognize
for instance if one would like to build a system to recognize d rotated faces one need to use other d rotated faces as templates
this may explain the existence of such specialized modules in the brain as one responsible for face recognition
even with custom templates a noiselike encoding of images and templates is necessary for localization
it can be naturally achieved if the nongroup transformation is processed on any layer other than the first in hierarchical recognition architecture
the previous section suggests one motivation for hierarchical image recognition architectures
however they have other benefits as well
firstly hierarchical architectures best accomplish the goal of parsing a complex visual scene with many objects consisting of many parts whose relative position may greatly vary
in this case different elements of the system must react to different objects and parts
in hierarchical architectures representations of parts at different levels of embedding hierarchy can be stored at different layers of hierarchy
secondly hierarchical architectures which have invariant representations for parts of objects may facilitate learning of complex compositional concepts
this facilitation may happen through reusing of learned representations of parts that were constructed before in process of learning of other concepts
as a result sample complexity of learning compositional concepts may be greatly reduced
finally hierarchical architectures have better tolerance to clutter
clutter problem arises when the target object is in front of a nonuniform background which functions as a distractor for the visual task
hierarchical architecture provides signatures for parts of target objects which do not include parts of background and are not affected by background variations
in hierarchical architectures one layer is not necessarily invariant to all transformations that are handled by the hierarchy as a whole
some transformations may pass through that layer to upper layers as in the case of nongroup transformations described in the previous section
for other transformations an element of the layer may produce invariant representations only within small range of transformations
for instance elements of the lower layers in hierarchy have small visual field and thus can handle only a small range of translation
for such transformations the layer should provide covariant rather than invariant signatures
the property of covariance can be written as formula where formula is a layer formula is the signature of image on that layer and formula stands for distribution of values of the expression for all formula
mtheory is based on a quantitative theory of the ventral stream of visual cortex
understanding how visual cortex works in object recognition is still a challenging task for neuroscience
humans and primates are able to memorize and recognize objects after seeing just couple of examples unlike any stateofthe art machine vision systems that usually require a lot of data in order to recognize objects
prior to the use of visual neuroscience in computer vision has been limited to early vision for deriving stereo algorithms e
and to justify the use of dog derivativeofgaussian filters and more recently of gabor filters
no real attention has been given to biologically plausible features of higher complexity
while mainstream computer vision has always been inspired and challenged by human vision it seems to have never advanced past the very first stages of processing in the simple cells in v and v
although some of the systems inspired  to various degrees  by neuroscience have been tested on at least some natural images neurobiological models of object recognition in cortex have not yet been extended to deal with realworld image databases
mtheory learning framework employs a novel hypothesis about the main computational function of the ventral stream the representation of new objectsimages in terms of a signature which is invariant to transformations learned during visual experience
this allows recognition from very few labeled examples  in the limit just one
neuroscience suggests that natural functionals for a neuron to compute is a highdimensional dot product between an image patch and another image patch called template which is stored in terms of synaptic weights synapses per neuron
the standard computational model of a neuron is based on a dot product and a threshold
another important feature of the visual cortex is that it consists of simple and complex cells
this idea was originally proposed by hubel and wiesel
mtheory employs this idea
simple cells compute dot products of an image and transformations of templates formula for formula formula is a number of simple cells
complex cells are responsible for pooling and computing empirical histograms or statistical moments of it
the following formula for constructing histogram can be computed by neuronsformulawhere formula is a smooth version of step function formula is the width of a histogram bin and formula is the number of the bin
in authors applied mtheory to unconstrained face recognition in natural photographs
unlike the dar detection alignment and recognition method which handles clutter by detecting objects and cropping closely around them so that very little background remains this approach accomplishes detection and alignment implicitly by storing transformations of training images templates rather than explicitly detecting and aligning or cropping faces at test time
this system is built according to the principles of a recent theory of invariance in hierarchical networks and can evade the clutter problem generally problematic for feedforward systems
the resulting endtoend system achieves a drastic improvement in the state of the art on this endtoend task reaching the same level of performance as the best systems operating on aligned closely cropped images no outside training data
it also performs well on two newer datasets similar to lfw but more difficult significantly jittered misaligned version of lfw and sufrw for example the models accuracy in the lfw unaligned  no outside data used category is
compared to stateoftheart apem adaptive probabilistic elastic matching
the theory was also applied to a range of recognition tasks from invariant single object recognition in clutter to multiclass categorization problems on publicly available data sets caltech caltech mitcbcl and complex street scene understanding tasks that requires the recognition of both shapebased as well as texturebased objects on streetscenes data set
the approach performs really well it has the capability of learning from only a few training examples and was shown to outperform several more complex stateoftheart systems constellation models the hierarchical svmbased face detection system
a key element in the approach is a new set of scale and positiontolerant feature detectors which are biologically plausible and agree quantitatively with the tuning properties of cells along the ventral stream of visual cortex
these features are adaptive to the training set though we also show that a universal feature set learned from a set of natural images unrelated to any categorization task likewise achieves good performance
this theory can also be extended for the speech recognition domain
as an example in an extension of a theory for unsupervised learning of invariant visual representations to the auditory domain and empirically evaluated its validity for voiced speech sound classification was proposed
authors empirically demonstrated that a singlelayer phonelevel representation extracted from base speech features improves segment classification accuracy and decreases the number of training examples in comparison with standard spectral and cepstral features for an acoustic classification task on timit dataset
in statistics overfitting is the production of an analysis that corresponds too closely or exactly to a particular set of data and may therefore fail to fit additional data or predict future observations reliably
an overfitted model is a statistical model that contains more parameters than can be justified by the data
the essence of overfitting is to have unknowingly extracted some of the residual variation i
the noise as if that variation represented underlying model structure
underfitting occurs when a statistical model cannot adequately capture the underlying structure of the data
an underfitted model is a model where some parameters or terms that would appear in a correctly specified model are missing
underfitting would occur for example when fitting a linear model to nonlinear data
such a model will tend to have poor predictive performance
overfitting and underfitting can occur in machine learning in particular
in machine learning the phenomena are sometimes called overtraining and undertraining
the possibility of overfitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model
for example a model might be selected by maximizing its performance its ability to predict the dependent variables value given some values of the independent variables on some set of training data and yet its suitability might be determined by its ability to perform well on unseen data then overfitting occurs when a model begins to memorize training data rather than learning to generalize from a trend
as an extreme example if the number of parameters is the same as or greater than the number of observations then a model can perfectly predict the training data simply by memorizing the data in its entirety
for an illustration see figure
such a model though will typically fail severely when making predictions
the potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape and the magnitude of model error compared to the expected level of noise or error in the data
even when the fitted model does not have an excessive number of parameters it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting a phenomenon sometimes known as shrinkage
in particular the value of the coefficient of determination will shrink relative to the original data
to lessen the chance of or amount of overfitting several techniques are available e
model comparison crossvalidation regularization early stopping pruning bayesian priors or dropout
the basis of some techniques is either  to explicitly penalize overly complex models or  to test the models ability to generalize by evaluating its performance on a set of data not used for training which is assumed to approximate the typical unseen data that a model will encounter
in statistics an inference is drawn from a statistical model which has been selected via some procedure
burnham anderson in their muchcited text on model selection argue that to avoid overfitting we should adhere to the principle of parsimony
the authors also state the following
overfitting is more likely to be a serious concern when there is little theory available to guide the analysis in part because then there tend to be a large number of models to select from
the book model selection and model averaging  puts it this way
in regression analysis overfitting occurs frequently
in the extreme case if there are p variables in a linear regression with p data points the fitted hyperplane will go exactly through every point
a study in  suggests that two observations per independent variable are sufficient for linear regression
for logistic regression or cox proportional hazards models there are a variety of rules of thumb e
and   the guideline of  observations per independent variable is known as the one in ten rule
in the process of regression model selection the mean squared error of the random regression function can be split into random noise approximation bias and variance in the estimate of the regression function and the biasvariance tradeoff is often used to overcome overfit models
by freedmans paradox with a large set of explanatory variables that actually have no relation to the dependent variable being predicted some explanators will in general be spuriously found to be statistically significant and the researcher may thus retain them in the model thereby overfitting the model
usually a learning algorithm is trained using some set of training data exemplary situations for which the desired output is known
the goal is that the algorithm will also perform well on predicting the output when fed validation data that was not encountered during its training
overfitting is the use of models or procedures that violate occams razor for example by including more adjustable parameters than are ultimately optimal or by using a more complicated approach than is ultimately optimal
for an example where there are too many adjustable parameters consider a dataset where training data for can be adequately predicted by a linear function of two dependent variables
such a function requires only three parameters the intercept and two slopes
replacing this simple function with a new more complex quadratic function or with a new more complex linear function on more than two dependent variables carries a risk occams razor implies that any given complex function is a priori less probable than any given simple function
if the new more complicated function is selected instead of the simple function and if there was not a large enough gain in trainingdata fit to offset the complexity increase then the new complex function overfits the data and the complex overfitted function will likely perform worse than the simpler function on validation data outside the training dataset even though the complex function performed as well or perhaps even better on the training dataset
when comparing different types of models complexity cannot be measured solely by counting how many parameters exist in each model the expressivity of each parameter must be considered as well
for example it is nontrivial to directly compare the complexity of a neural net which can track curvilinear relationships with parameters to a regression model with parameters
overfitting is especially likely in cases where learning was performed too long or where training examples are rare causing the learner to adjust to very specific random features of the training data that have no causal relation to the target function
in this process of overfitting the performance on the training examples still increases while the performance on unseen data becomes worse
as a simple example consider a database of retail purchases that includes the item bought the purchaser and the date and time of purchase
its easy to construct a model that will fit the training set perfectly by using the date and time of purchase to predict the other attributes but this model will not generalize at all to new data because those past times will never occur again
generally a learning algorithm is said to overfit relative to a simpler one if it is more accurate in fitting known data hindsight but less accurate in predicting new data foresight
one can intuitively understand overfitting from the fact that information from all past experience can be divided into two groups information that is relevant for the future and irrelevant information noise
everything else being equal the more difficult a criterion is to predict i
the higher its uncertainty the more noise exists in past information that needs to be ignored
the problem is determining which part to ignore
a learning algorithm that can reduce the chance of fitting noise is called robust
the most obvious consequence of overfitting is poor performance on the validation dataset
other negative consequences includethe optimal function usually needs verification on bigger or completely new datasets
there are however methods like minimum spanning tree or lifetime of correlation that applies the dependence between correlation coefficients and timeseries window width
whenever the window width is big enough the correlation coefficients are stable and dont depend on the window width size anymore
therefore a correlation matrix can be created by calculating a coefficient of correlation between investigated variables
this matrix can be represented topologically as a complex network where direct and undirect influences between variables are visualized
underfitting occurs when a statistical model or machine learning algorithm cannot adequately capture the underlying structure of the data
it occurs when the model or algorithm does not fit the data enough
underfitting occurs if the model or algorithm shows low variance but high bias to contrast the opposite overfitting from high variance and low bias
it is often a result of an excessively simple model
burnham anderson state the following
however underfitting can also occur in the absence of bias
for example if a good model would include two mutually uncorrelated explanatory variables x and z but only x is used in the model the effect of x could be estimated without bias but the model would fit the data better both in the training set of data and for other data if z were also included
cognitive robotics is concerned with endowing a robot with intelligent behavior by providing it with a processing architecture that will allow it to learn and reason about how to behave in response to complex goals in a complex world
cognitive robotics may be considered the engineering branch of embodied cognitive science and embodied embedded cognition
while traditional cognitive modeling approaches have assumed symbolic coding schemes as a means for depicting the world translating the world into these kinds of symbolic representations has proven to be problematic if not untenable
perception and action and the notion of symbolic representation are therefore core issues to be addressed in cognitive robotics
cognitive robotics views animal cognition as a starting point for the development of robotic information processing as opposed to more traditional artificial intelligence techniques
target robotic cognitive capabilities include perception processing attention allocation anticipation planning complex motor coordination reasoning about other agents and perhaps even about their own mental states
robotic cognition embodies the behavior of intelligent agents in the physical world or a virtual world in the case of simulated cognitive robotics
ultimately the robot must be able to act in the real world
a preliminary robot learning technique called motor babbling involves correlating pseudorandom complex motor movements by the robot with resulting visual andor auditory feedback such that the robot may begin to expect a pattern of sensory feedback given a pattern of motor output
desired sensory feedback may then be used to inform a motor control signal
this is thought to be analogous to how a baby learns to reach for objects or learns to produce speech sounds
for simpler robot systems where for instance inverse kinematics may feasibly be used to transform anticipated feedback desired motor result into motor output this step may be skipped
once a robot can coordinate its motors to produce a desired result the technique of learning by imitation may be used
the robot monitors the performance of another agent and then the robot tries to imitate that agent
it is often a challenge to transform imitation information from a complex scene into a desired motor result for the robot
note that imitation is a highlevel form of cognitive behavior and imitation is not necessarily required in a basic model of embodied animal cognition
a more complex learning approach is autonomous knowledge acquisition the robot is left to explore the environment on its own
a system of goals and beliefs is typically assumed
a somewhat more directed mode of exploration can be achieved by curiosity algorithms such as intelligent adaptive curiosity or categorybased intrinsic motivation
these algorithms generally involve breaking sensory input into a finite number of categories and assigning some sort of prediction system such as an artificial neural network to each
the prediction system keeps track of the error in its predictions over time
reduction in prediction error is considered learning
the robot then preferentially explores categories in which it is learning or reducing prediction error the fastest
some researchers in cognitive robotics have tried using architectures such as actr and soar cognitive architecture as a basis of their cognitive robotics programs
these highly modular symbolprocessing architectures have been used to simulate operator performance and human performance when modeling simplistic and symbolized laboratory data
the idea is to extend these architectures to handle realworld sensory input as that input continuously unfolds through time
what is needed is a way to somehow translate the world into a set of symbols and their relationships
drones or subatomic ai are some of the most controversial and greatest representation of sentient robotics and their processing or what the common people refer and believe in as quantum computing or plutonic a
not to be confused with platonic a
use string theory or mass sentient sentient mass relativity to conduct a universal occlusion called schripp or string theory using schripp for the general process and purpose of creating a
or icb metric drones uses quantum relativity between two units to synchronize and assimilate a wavelength to cause universal fractal hyper conclusion and create an icb geometric manifold and conclude computing singularity units hypothetically for units to quantum compute icb metric numbers and code computers use a conjunction of high pitched am frequency to randomize photogenic relay combined with a high axis accelerometer with the implementation and use of quantum source code and with inductive relay for reasoning skills that are very close to the geometric decision based system that the human race used for evolution inclusively space time to manifold and icb metric singularity are generally heard in a frequency that is beyond the range of human hearingsome of the fundamental questions to still be answered in cognitive robotics arecognitive robotics book by hooman samani takes a multidiciplinary approach to cover various aspects of cognitive robotics such as artificial intelligence physical chemical philosophical psychological social cultural and ethical aspects
movidius is a company based in san mateo california that designs specialised lowpower processor chips for computer vision
the company was acquired by intel in september
movidius was cofounded in dublin in  by sean mitchell and david moloney
between  and  it raised nearly  million in capital funding
in may  the company appointed remi elouazzane as ceo
in january  the company announced a partnership with google
movidius has been active in the google project tango project
movidius announced a planned acquisition by intel in september
the companys myriad  chip is an alwayson manycore vision processing unit that can function on powerconstrained devices
the fathom is a usb stick containing a myriad  processor allowing a vision accelerator to be added to devices using arm processors including pcs drones robots iot devices and video surveillance for tasks such as identifying people or objects
it can run at between  and  gflops on little more than w of power
the crossentropy ce method developed by reuven rubinstein is a general monte carlo approach tocombinatorial and continuous multiextremal optimization and importance sampling
the method originated from the field of rare event simulation wherevery small probabilities need to be accurately estimated for example in network reliability analysis queueing models or performance analysis of telecommunication systems
the ce method can be applied to static and noisy combinatorial optimization problems such as the traveling salesman problem the quadratic assignment problem dna sequence alignment the maxcut problem and the buffer allocation problem as well as continuous global optimization problems with many local extrema
in a nutshell the ce method consists of two phases consider the general problem of estimating the quantity where formula is some performance function and formula is a member of some parametric family of distributions
using importance sampling this quantity can be estimated as where formula is a random sample from formula
for positive formula the theoretically optimal importance sampling density pdf is given by this however depends on the unknown formula
the ce method aims to approximate the optimal pdf by adaptively selecting members of the parametric family that are closest in the kullbackleibler sense to the optimal pdf formula
in several cases the solution to step  can be found analytically
situations in which this occurs arethe same ce algorithm can be used for optimization rather than estimation
suppose the problem is to maximize some function formula for example formula
to apply ce one considers first the associated stochastic problem of estimatingformulafor a given level formula and parametric family formula for example the dimensional gaussian distributionparameterized by its mean formula and variance formula so formula here
hence for a given formula the goal is to find formula so thatformulais minimized
this is done by solving the sample version stochastic counterpart of the kl divergence minimization problem as in step  above
it turns out that parameters that minimize the stochastic counterpart for this choice of target distribution andparametric family are the sample mean and sample variance corresponding to the elite samples which are those samples that have objective function value formula
the worst of the elite samples is then used as the level parameter for the next iteration
this yields the following randomized algorithm that happens to coincide with the socalled estimation of multivariate normal algorithm emna an estimation of distribution algorithm
mu sigma t maxits  initialize parameters
crossvalidation sometimes called rotation estimation or outofsample testing is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set
it is mainly used in settings where the goal is prediction and one wants to estimate how accurately a predictive model will perform in practice
in a prediction problem a model is usually given a dataset of known data on which training is run training dataset and a dataset of unknown data or first seen data against which the model is tested called the validation dataset or testing set
the goal of crossvalidation is to test the models ability to predict new data that were not used in estimating it in order to flag problems like overfitting and to give an insight on how the model will generalize to an independent dataset i
an unknown dataset for instance from a real problem
one round of crossvalidation involves partitioning a sample of data into complementary subsets performing the analysis on one subset called the training set and validating the analysis on the other subset called the validation set or testing set
to reduce variability in most methods multiple rounds of crossvalidation are performed using different partitions and the validation results are combined e
averaged over the rounds to give an estimate of the models predictive performance
in summary crossvalidation combines averages measures of fitness in prediction to derive a more accurate estimate of model prediction performance
suppose we have a model with one or more unknown parameters and a data set to which the model can be fit the training data set
the fitting process optimizes the model parameters to make the model fit the training data as well as possible
if we then take an independent sample of validation data from the same population as the training data it will generally turn out that the model does not fit the validation data as well as it fits the training data
the size of this difference is likely to be large especially when the size of the training data set is small or when the number of parameters in the model is large
crossvalidation is a way to estimate the size of this effect
in linear regression we have real response values y
y and n pdimensional vector covariates x
the components of the vectors x are denoted x
if we use least squares to fit a function in the form of a hyperplane y  a  x to the data x y we could then assess the fit using the mean squared error mse
the mse for given estimated parameter values a and  on the training set x y isif the model is correctly specified it can be shown under mild assumptions that the expected value of the mse for the training set is npnp times the expected value of the mse for the validation set the expected value is taken over the distribution of training sets
thus if we fit the model and compute the mse on the training set we will get an optimistically biased assessment of how well the model will fit an independent data set
this biased estimate is called the insample estimate of the fit whereas the crossvalidation estimate is an outofsample estimate
since in linear regression it is possible to directly compute the factor npnp by which the training mse underestimates the validation mse under the assumption that the model specification is valid crossvalidation can be used for checking whether the model has been overfitted in which case the mse in the validation set will substantially exceed its anticipated value
crossvalidation in the context of linear regression is also useful in that it can be used to select an optimally regularized cost function
in most other regression procedures e
logistic regression there is no simple formula to compute the expected outofsample fit
crossvalidation is thus a generally applicable way to predict the performance of a model on unavailable data using numerical computation in place of theoretical analysis
two types of crossvalidation can be distinguished exhaustive and nonexhaustive crossvalidation
exhaustive crossvalidation methods are crossvalidation methods which learn and test on all possible ways to divide the original sample into a training and a validation set
leavepout crossvalidation lpo cv involves using p observations as the validation set and the remaining observations as the training set
this is repeated on all ways to cut the original sample on a validation set of p observations and a training set
lpo crossvalidation requires training and validating the model formula times where n is the number of observations in the original sample and where formula is the binomial coefficient
for p   and for even moderately large n lpo cv can become computationally infeasible
for example with n   and p     percent of  as suggested above formulaleaveoneout crossvalidation loocv is a particular case of leavepout crossvalidation with p
the process looks similar to jackknife however with crossvalidation one computes a statistic on the leftout samples while with jackknifing one computes a statistic from the kept samples only
loo crossvalidation does not have the same problem of excessive computation time as general lpo crossvalidation because formula
nonexhaustive cross validation methods do not compute all ways of splitting the original sample
those methods are approximations of leavepout crossvalidation
in kfold crossvalidation the original sample is randomly partitioned into k equal sized subsamples
of the k subsamples a single subsample is retained as the validation data for testing the model and the remaining k subsamples are used as training data
the crossvalidation process is then repeated k times the folds with each of the k subsamples used exactly once as the validation data
the k results from the folds can then be averaged to produce a single estimation
the advantage of this method over repeated random subsampling see below is that all observations are used for both training and validation and each observation is used for validation exactly once
fold crossvalidation is commonly used but in general k remains an unfixed parameter
for example setting k results in fold crossvalidation
in fold crossvalidation we randomly shuffle the dataset into two sets d and d so that both sets are equal size this is usually implemented by shuffling the data array and then splitting it in two
we then train on d and validate on d followed by training on d and validating ond
when kn the number of observations the kfold crossvalidation is exactly the leaveoneout crossvalidation
in stratified kfold crossvalidation the folds are selected so that the mean response value is approximately equal in all the folds
in the case of a dichotomous classification this means that each fold contains roughly the same proportions of the two types of class labels
in the holdout method we randomly assign data points to two sets d and d usually called the training set and the test set respectively
the size of each of the sets is arbitrary although typically the test set is smaller than the training set
we then train on d and test on d
in typical crossvalidation multiple runs are aggregated together in contrast the holdout method in isolation involves a single run
while the holdout method can be framed as the simplest kind of crossvalidation many sources instead classify holdout as a type of simple validation rather than a simple or degenerate form of crossvalidation
this method also known as monte carlo crossvalidation randomly splits the dataset into training and validation data
for each such split the model is fit to the training data and predictive accuracy is assessed using the validation data
the results are then averaged over the splits
the advantage of this method over kfold cross validation is that the proportion of the trainingvalidation split is not dependent on the number of iterations folds
the disadvantage of this method is that some observations may never be selected in the validation subsample whereas others may be selected more than once
in other words validation subsets may overlap
this method also exhibits monte carlo variation meaning that the results will vary if the analysis is repeated with different random splits
as the number of random splits approaches infinity the result of repeated random subsampling validation tends towards that of leavepout crossvalidation
in a stratified variant of this approach the random samples are generated in such a way that the mean response value i
the dependent variable in the regression is equal in the training and testing sets
this is particularly useful if the responses are dichotomous with an unbalanced representation of the two response values in the data
the goal of crossvalidation is to estimate the expected level of fit of a model to a data set that is independent of the data that were used to train the model
it can be used to estimate any quantitative measure of fit that is appropriate for the data and model
for example for binary classification problems each case in the validation set is either predicted correctly or incorrectly
in this situation the misclassification error rate can be used to summarize the fit although other measures like positive predictive value could also be used
when the value being predicted is continuously distributed the mean squared error root mean squared error or median absolute deviation could be used to summarize the errors
suppose we choose a measure of fit f and use crossvalidation to produce an estimate f of the expected fit ef of a model to an independent data set drawn from the same population as the training data
if we imagine sampling multiple independent training sets following the same distribution the resulting values for f will vary
the statistical properties of f result from this variation
the crossvalidation estimator f is very nearly unbiased for ef
the reason that it is slightly biased is that the training set in crossvalidation is slightly smaller than the actual data set e
for loocv the training set size is n when there are n observed cases
in nearly all situations the effect of this bias will be conservative in that the estimated fit will be slightly biased in the direction suggesting a poorer fit
in practice this bias is rarely a concern
the variance of f can be large
for this reason if two statistical procedures are compared based on the results of crossvalidation it is important to note that the procedure with the better estimated performance may not actually be the better of the two procedures i
it may not have the better value of ef
some progress has been made on constructing confidence intervals around crossvalidation estimates but this is considered a difficult problem
most forms of crossvalidation are straightforward to implement as long as an implementation of the prediction method being studied is available
in particular the prediction method can be a black box  there is no need to have access to the internals of its implementation
if the prediction method is expensive to train crossvalidation can be very slow since the training must be carried out repeatedly
in some cases such as least squares and kernel regression crossvalidation can be sped up significantly by precomputing certain values that are needed repeatedly in the training or by using fast updating rules such as the shermanmorrison formula
however one must be careful to preserve the total blinding of the validation set from the training procedure otherwise bias may result
an extreme example of accelerating crossvalidation occurs in linear regression where the results of crossvalidation have a closedform expression known as the prediction residual error sum of squares press
crossvalidation only yields meaningful results if the validation set and training set are drawn from the same population and only if human biases are controlled
in many applications of predictive modeling the structure of the system being studied evolves over time i
it is nonstationary
both of these can introduce systematic differences between the training and validation sets
for example if a model for predicting stock values is trained on data for a certain fiveyear period it is unrealistic to treat the subsequent fiveyear period as a draw from the same population
as another example suppose a model is developed to predict an individuals risk for being diagnosed with a particular disease within the next year
if the model is trained using data from a study involving only a specific population group e
young people or males but is then applied to the general population the crossvalidation results from the training set could differ greatly from the actual predictive performance
in many applications models also may be incorrectly specified and vary as a function of modeler biases andor arbitrary choices
when this occurs there may be an illusion that the system changes in external samples whereas the reason is that the model has missed a critical predictor andor included a confounded predictor
new evidence is that crossvalidation by itself is not very predictive of external validity whereas a form of experimental validation known as swap sampling that does control for human bias can be much more predictive of external validity
as defined by this large maqcii study across  models swap sampling incorporates crossvalidation in the sense that predictions are tested across independent training and validation samples
yet models are also developed across these independent samples and by modelers who are blinded to one another
when there is a mismatch in these models developed across these swapped training and validation samples as happens quite frequently maqcii shows that this will be much more predictive of poor external predictive validity than traditional crossvalidation
the reason for the success of the swapped sampling is a builtin control for human biases in model building
in addition to placing too much faith in predictions that may vary across modelers and lead to poor external validity due to these confounding modeler effects these are some other ways that crossvalidation can be misusedsince the order of the data is important crossvalidation might be problematic for timeseries models
a more appropriate approach might be to use forward chaining
crossvalidation can be used to compare the performances of different predictive modeling procedures
for example suppose we are interested in optical character recognition and we are considering using either support vector machines svm or k nearest neighbors knn to predict the true character from an image of a handwritten character
using crossvalidation we could objectively compare these two methods in terms of their respective fractions of misclassified characters
if we simply compared the methods based on their insample error rates the knn method would likely appear to perform better since it is more flexible and hence more prone to overfitting compared to the svm method
crossvalidation can also be used in variable selection
suppose we are using the expression levels of  proteins to predict whether a cancer patient will respond to a drug
a practical goal would be to determine which subset of the  features should be used to produce the best predictive model
for most modeling procedures if we compare feature subsets using the insample error rates the best performance will occur when all  features are used
however under crossvalidation the model with the best fit will generally include only a subset of the features that are deemed truly informative
a recent development in medical statistics is its use in metaanalysis
it forms the basis of the validation statistic vn which is used to test the statistical validity of metaanalysis summary estimates
it has also been used in a more conventional sense in metaanalysis to estimate the likely prediction error of metaanalysis results
in machine learning feature hashing also known as the hashing trick by analogy to the kernel trick is a fast and spaceefficient way of vectorizing features i
turning arbitrary features into indices in a vector or matrix
it works by applying a hash function to the features and using their hash values as indices directly rather than looking the indices up in an associative array
in a typical document classification task the input to the machine learning algorithm both during learning and classification is free text
from this a bag of words bow representation is constructed the individual tokens are extracted and counted and each distinct token in the training set defines a feature independent variable of each of the documents in both the training and test sets
machine learning algorithms however are typically defined in terms of numerical vectors
therefore the bags of words for a set of documents is regarded as a termdocument matrix where each row is a single document and each column is a single featureword the entry in such a matrix captures the frequency or weight of the th term of the vocabulary in document
an alternative convention swaps the rows and columns of the matrix but this difference is immaterial
typically these vectors are extremely sparseaccording to zipfs law
the common approach is to construct at learning time or prior to that a dictionary representation of the vocabulary of the training set and use that to map words to indices
hash tables and tries are common candidates for dictionary implementation
the three documentscan be converted using the dictionaryto the termdocument matrixthe problem with this process is that such dictionaries take up a large amount of storage space and grow in size as the training set grows
on the contrary if the vocabulary is kept fixed and not increased with a growing training set an adversary may try to invent new words or misspellings that are not in the stored vocabulary so as to circumvent a machine learned filter
this difficulty is why feature hashing has been tried for spam filtering at yahoo research
note that the hashing trick isnt limited to text classification and similar tasks at the document level but can be applied to any problem that involves large perhaps unbounded numbers of features
instead of maintaining a dictionary a feature vectorizer that uses the hashing trick can build a vector of a predefined length by applying a hash function to the features e
words then using the hash values directly as feature indices and updating the resulting vector at those indices
here we assume that feature actually means feature vector
thus if our feature vector is catdogcat and hash function is formula if formula is cat and formula if formula is dog
let us take the output feature vector dimension n to be
then output x will be
it has been suggested that a second singlebit output hash function be used to determine the sign of the update value to counter the effect of hash collisions
if such a hash function is used the algorithm becomesthe above pseudocode actually converts each sample into a vector
an optimized version would instead only generate a stream of  pairs and let the learning and prediction algorithms consume such streams a linear model can then be implemented as a single hash table representing the coefficient vector
when a second hash function  is used to determine the sign of a features value the expected mean of each column in the output array becomes zero because  causes some collisions to cancel out
suppose an input contains two symbolic features f and f that collide with each other but not with any other features in the same input then there are four possibilities which if we make no assumptions about  have equal probability as listed in the table on the right
in this example there is a  probability that the hash collision cancels out
multiple hash functions can be used to further reduce the risk of collisions
furthermore if  is the transformation implemented by a hashing trick with a sign hash  i
x is the feature vector produced for a sample x then inner products in the hashed space are unbiasedwhere the expectation is taken over the hashing function
it can be verified thatformula is a positive semidefinite kernel
recent work extends the hashing trick to supervised mappings from words to indiceswhich are explicitly learned to avoid collisions of important terms
ganchev and dredze showed that in text classification applications with random hash functions and several tens of thousands of columns in the output vectors feature hashing need not have an adverse effect on classification performance even without the signed hash function
weinberger et al
applied their variant of hashing to the problem of spam filtering formulating this as a multitask learning problem where the input features are pairs user feature so that a single parameter vector captured peruser spam filters as well as a global filter for several hundred thousand users and found that the accuracy of the filter went up
implementations of the hashing trick are present in
a bongard problem is a kind of puzzle invented by the russian computer scientist mikhail moiseevich bongard     probably in the mids
they were published in his  book on pattern recognition
bongard in the introduction of the book which deals with a number of topics including perceptrons credits the ideas in it to a group including m
vaintsvaig v
maksimov and m
smirnov
the idea of a bongard problem is to present two sets of relatively simple diagrams say a and b
all the diagrams from set a have a common factor or attribute which is lacking in all the diagrams of set b
the problem is to find or to formulate convincingly the common factor
the problems were popularised by their occurrence in the  book gdel escher bach by douglas hofstadter himself a composer of bongard problems
bongard problems are also at the heart of the game zendo
many computational architectures have been devised to solve bongard problems the most extensive of which being phaeaco by harry foundalis who left the field in  due to ethical concerns regarding machines that can pass as human
in statistics machine learning and information theory dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables
it can be divided into feature selection and feature extraction
feature selection approaches try to find a subset of the original variables also called features or attributes
there are three strategies the filter strategy e
information gain the wrapper strategy e
search guided by accuracy and the embedded strategy features are selected to add or be removed while building the model based on the prediction errors
see also combinatorial optimization problems
in some cases data analysis such as regression or classification can be done in the reduced space more accurately than in the original space
feature extraction transforms the data in the highdimensional space to a space of fewer dimensions
the data transformation may be linear as in principal component analysis pca but many nonlinear dimensionality reduction techniques also exist
for multidimensional data tensor representation can be used in dimensionality reduction through multilinear subspace learning
the main linear technique for dimensionality reduction principal component analysis performs a linear mapping of the data to a lowerdimensional space in such a way that the variance of the data in the lowdimensional representation is maximized
in practice the covariance and sometimes the correlation matrix of the data is constructed and the eigenvectors on this matrix are computed
the eigenvectors that correspond to the largest eigenvalues the principal components can now be used to reconstruct a large fraction of the variance of the original data
moreover the first few eigenvectors can often be interpreted in terms of the largescale physical behavior of the system
the original space with dimension of the number of points has been reduced with data loss but hopefully retaining the most important variance to the space spanned by a few eigenvectors
nmf decomposes a nonnegative matrix to the product of two nonnegative ones which has been a promising tool in fields where only nonnegative signals exist
such as astronomy
nmf is well known since the multiplicative update rule by lee  seung which has been continuously developed the inclusion of uncertainties  the consideration of missing data and parallel computation  sequential construction which leads to the stability and linearity of nmf as well as other updates
with a stable component basis during construction and a linear modeling process sequential nmf is able to preserve the flux in direct imaging of circumstellar structures in astromony as one of the methods of detecting exoplanets especially for the direct imaging of circumstellar disks
in comparison with pca nmf does not remove the mean of the matrices which leads to unphysical nonnegative fluxes therefore nmf is able to preserve more information than pca as demonstrated by ren et al
principal component analysis can be employed in a nonlinear way by means of the kernel trick
the resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data
the resulting technique is entitled kernel pca
other prominent nonlinear techniques include manifold learning techniques such as isomap locally linear embedding lle hessian lle laplacian eigenmaps and local tangent space alignment ltsa
these techniques construct a lowdimensional data representation using a cost function that retains local properties of the data and can be viewed as defining a graphbased kernel for kernel pca
more recently techniques have been proposed that instead of defining a fixed kernel try to learn the kernel using semidefinite programming
the most prominent example of such a technique is maximum variance unfolding mvu
the central idea of mvu is to exactly preserve all pairwise distances between nearest neighbors in the inner product space while maximizing the distances between points that are not nearest neighbors
an alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces
important examples of such techniques include classical multidimensional scaling which is identical to pca isomap which uses geodesic distances in the data space diffusion maps which use diffusion distances in the data space tdistributed stochastic neighbor embedding tsne which minimizes the divergence between distributions over pairs of points and curvilinear component analysis
a different approach to nonlinear dimensionality reduction is through the use of autoencoders a special kind of feedforward neural networks with a bottleneck hidden layer
the training of deep encoders is typically performed using a greedy layerwise pretraining e
using a stack of restricted boltzmann machines that is followed by a finetuning stage based on backpropagation
linear discriminant analysis lda is a generalization of fishers linear discriminant a method used in statistics pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events
gda deals with nonlinear discriminant analysis using kernel function operator
the underlying theory is close to the support vector machines svm insofar as the gda method provides a mapping of the input vectors into highdimensional feature space
similar to lda the objective of gda is to find a projection for the features into a lower dimensional space by maximizing the ratio of betweenclass scatter to withinclass scatter
autoencoders can be used to learn nonlinear dimension reduction functions and codings together with an inverse function from the coding to the original representation
for highdimensional datasets i
with number of dimensions more than  dimension reduction is usually performed prior to applying a knearest neighbors algorithm knn in order to avoid the effects of the curse of dimensionality
feature extraction and dimension reduction can be combined in one step using principal component analysis pca linear discriminant analysis lda canonical correlation analysis cca or nonnegative matrix factorization nmf techniques as a preprocessing step followed by clustering by knn on feature vectors in reduceddimension space
in machine learning this process is also called lowdimensional embedding
for veryhighdimensional datasets e
when performing similarity search on live video streams dna data or highdimensional time series running a fast approximate knn search using locality sensitive hashing random projection sketches or other highdimensional similarity search techniques from the vldb toolbox might be the only feasible option
a dimensionality reduction technique that is sometimes used in neuroscience is maximally informative dimensions which finds a lowerdimensional representation of a dataset such that as much information as possible about the original data is preserved
in pac learning error tolerance refers to the ability of an algorithm to learn when the examples received have been corrupted in some way
in fact this is a very common and important issue since in many applications it is not possible to access noisefree data
noise can interfere with the learning process at different levels the algorithm may receive data that have been occasionally mislabeled or the inputs may have some false information or the classification of the examples may have been maliciously adulterated
in the following let formula be our formuladimensional input space
let formula be a class of functions that we wish to use in order to learn a formulavalued target function formula defined over formula
let formula be the distribution of the inputs over formula
the goal of a learning algorithm formula is to choose the best function formula such that it minimizes formula
let us suppose we have a function formula that can measure the complexity of formula
let formula be an oracle that whenever called returns an example formula and its correct label formula
when no noise corrupts the data we can define learning in the valiant settingdefinitionwe say that formula is efficiently learnable using formula in the valiant setting if there exists a learning algorithm formula that has access to formula and a polynomial formula such that for any formula and formula it outputs in a number of calls to the oracle bounded by formula  a function formula that satisfies with probability at least formula the condition formula
in the following we will define learnability of formula when data have suffered some modification
in the classification noise model a noise rate formula is introduced
then instead of formula that returns always the correct label of example formula algorithm formula can only call a faulty oracle formula that will flip the label of formula with probability formula
as in the valiant case the goal of a learning algorithm formula is to choose the best function formula such that it minimizes formula
in applications it is difficult to have access to the real value of formula but we assume we have access to its upperbound formula
note that if we allow the noise rate to be formula then learning becomes impossible in any amount of computation time because every label conveys no information about the target function
definitionwe say that formula is efficiently learnable using formula in the classification noise model if there exists a learning algorithm formula that has access to formula and a polynomial formula such that for any formula formula and formula it outputs in a number of calls to the oracle bounded by formula  a function formula that satisfies with probability at least formula the condition formula
statistical query learning is a kind of active learning problem in which the learning algorithm formula can decide if to request information about the likelihood formula that a function formula correctly labels example formula and receives an answer accurate within a tolerance formula
formally whenever the learning algorithm formula calls the oracle formula it receives as feedback probability formula such that formula
definitionwe say that formula is efficiently learnable using formula in the statistical query learning model if there exists a learning algorithm formula that has access to formula and polynomials formula formula and formula such that for any formula the following holdnote that the confidence parameter formula does not appear in the definition of learning
this is because the main purpose of formula is to allow the learning algorithm a small probability of failure due to an unrepresentative sample
since now formula always guarantees to meet the approximation criterion formula the failure probability is no longer needed
the statistical query model is strictly weaker than the pac model any efficiently sqlearnable class is efficiently pac learnable in the presence of classification noise but there exist efficient paclearnable problems such as parity that are not efficiently sqlearnable
in the malicious classification model an adversary generates errors to foil the learning algorithm
this setting describes situations of error burst which may occur when for a limited time transmission equipment malfunctions repeatedly
formally algorithm formula calls an oracle formula that returns a correctly labeled example formula drawn as usual from distribution formula over the input space with probability formula but it returns with probability formula an example drawn from a distribution that is not related to formula
moreover this maliciously chosen example may strategically selected by an adversary who has knowledge of formula formula formula or the current progress of the learning algorithm
definitiongiven a bound formula for formula we say that formula is efficiently learnable using formula in the malicious classification model if there exist a learning algorithm formula that has access to formula and a polynomial formula such that for any formula formula it outputs in a number of calls to the oracle bounded by formula  a function formula that satisfies with probability at least formula the condition formula
in the nonuniform random attribute noise model the algorithm is learning a boolean function a malicious oracle formula may flip each formulath bit of example formula independently with probability formula
this type of error can irreparably foil the algorithm in fact the following theorem holdsin the nonuniform random attribute noise setting an algorithm formula can output a function formula such that formula only if formula
instantaneously trained neural networks are feedforward artificial neural networks proposed by subhash kak that create a new hidden neuron node for each novel training sample
the weights to this hidden neuron separate out not only this training sample but others that are near it thus providing generalization
this separation is done using the nearest hyperplane that can be written down instantaneously
in the two most important implementations the neighborhood of generalization either varies with the training sample cc network or remains constant cc network these networks use unary coding for an effective representation of the data sets
instantaneously trained neural networks have been proposed as models of short term learning and used in web search and financial time series prediction applications
they have also been used in instant classification of documents and for deep learning and data mining
as in other neural networks their normal use is as software but they have also been implemented in hardware using fpgas and by optical implementation
in the cc network which is a threestage network the number of input nodes is one more than the size of the training vector with the extra node serving as the biasing node whose input is always
for binary input vectors the weights from the input nodes to the hidden neuron say of index j corresponding to the trained vector is given by the following formulawhere formula is the radius of generalization and formula is the hamming weight the number of s of the binary sequence
from the hidden layer to the output layer the weights are  or  depending on whether the vector belongs to a given output class or not
the neurons in the hidden and output layers output  if the weighted sum to the input is  or positive and  if the weighted sum to the input is negativethe cc network has also been modified to include nonbinary input with varying radii of generalization so that it effectively provides a cc implementation
in feedback networks the willshaw network as well as the hopfield network are able to learn instantaneously
structural risk minimization srm is an inductive principle of use in machine learning
commonly in machine learning a generalized model must be selected from a finite data set with the consequent problem of overfitting  the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data
the srm principle addresses this problem by balancing the models complexity against its success at fitting the training data
the srm principle was first set out in a  paper by vladimir vapnik and alexey chervonenkis and uses the vc dimension
in endtoend reinforcement learning the endtoend process in other words the entire process from sensors to motors in a robot or agent involves a single layered or recurrent neural network without modularization
the network is trained by reinforcement learning rl
the approach has been proposed for a long time but was reenergized by the successful results in learning to play atari video games  and alphago  by google deepmind
it employs supervised learning without requiring sample labeled usually manually data
rl traditionally required explicit design of state space and action space while the mapping from state space to action space is learned
therefore rl has been limited to learning only for action and human designers have to design how to construct state space from sensor signals and to give how the motion commands are generated for each action before learning
neural networks have been often used in rl to provide nonlinear function approximation to avoid the curse of dimensionality
recurrent neural networks have been also employed mainly to avoid perceptual aliasing or partially observable markov decision process pomdp
endtoend rl extends rl from learning only for actions to learning the entire process from sensors to motors including higherlevel functions that are difficult to develop independently from other functions
higherlevel functions do not connect directly with either sensors or motors and so even giving their inputs and outputs is difficult
the approach originated in tdgammon
in backgammon the evaluation of the game situation during selfplay was learned through tdformula using a layered neural network
four inputs were used for the number of pieces of a given color at a given location on the board totaling  input signals
with zero knowledge built in the network learned to play the game at an intermediate level
shibata began working with this framework in
they employed qlearning and actorcritic for continuous motion tasks and used a recurrent neural network for memoryrequired tasks
they applied this framework to some real robot tasks
they demonstrated learning of various functions
beginning around  google deepmind showed impressive learning results in video games and game of go alphago
they used a deep convolutional neural network that showed superior results in image recognition
they used  frames of almost raw rgb pixels x as inputs
the network was trained based on rl with the reward representing the sign of the change in the game score
all  games were learned using the same network architecture and qlearning with minimal prior knowledge and outperformed competing methods on almost all the games and performed at a level that is comparable or superior to a professional human game tester
it is sometimes called deepq network dqn
in alphago deep neural networks are trained not only by reinforcement learning but also by supervised learning and monte carlo tree search
shibatas group showed that various functions emerge in this framework including communications were established in this framework
modes include
pattern recognition is a branch of machine learning that focuses on the recognition of patterns and regularities in data although it is in some cases considered to be nearly synonymous with machine learning
pattern recognition systems are in many cases trained from labeled training data supervised learning but when no labeled data are available other algorithms can be used to discover previously unknown patterns unsupervised learning
the terms pattern recognition machine learning data mining and knowledge discovery in databases kdd are hard to separate as they largely overlap in their scope
machine learning is the common term for supervised learning methods and originates from artificial intelligence whereas kdd and data mining have a larger focus on unsupervised methods and stronger connection to business use
pattern recognition has its origins in engineering and the term is popular in the context of computer vision a leading computer vision conference is named conference on computer vision and pattern recognition
in pattern recognition there may be a higher interest to formalize explain and visualize the pattern while machine learning traditionally focuses on maximizing the recognition rates
yet all of these domains have evolved substantially from their roots in artificial intelligence engineering and statistics and theyve become increasingly similar by integrating developments and ideas from each other
in machine learning pattern recognition is the assignment of a label to a given input value
in statistics discriminant analysis was introduced for this same purpose in
an example of pattern recognition is classification which attempts to assign each input value to one of a given set of classes for example determine whether a given email is spam or nonspam
however pattern recognition is a more general problem that encompasses other types of output as well
other examples are regression which assigns a realvalued output to each input sequence labeling which assigns a class to each member of a sequence of values for example part of speech tagging which assigns a part of speech to each word in an input sentence and parsing which assigns a parse tree to an input sentence describing the syntactic structure of the sentence
pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform most likely matching of the inputs taking into account their statistical variation
this is opposed to pattern matching algorithms which look for exact matches in the input with preexisting patterns
a common example of a patternmatching algorithm is regular expression matching which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors
in contrast to pattern recognition pattern matching is not generally a type of machine learning although patternmatching algorithms especially with fairly general carefully tailored patterns can sometimes succeed in providing similarquality output of the sort provided by patternrecognition algorithms
pattern recognition is generally categorized according to the type of learning procedure used to generate the output value
supervised learning assumes that a set of training data the training set has been provided consisting of a set of instances that have been properly labeled by hand with the correct output
a learning procedure then generates a model that attempts to meet two sometimes conflicting objectives perform as well as possible on the training data and generalize as well as possible to new data usually this means being as simple as possible for some technical definition of simple in accordance with occams razor discussed below
unsupervised learning on the other hand assumes training data that has not been handlabeled and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances
a combination of the two that has recently been explored is semisupervised learning which uses a combination of labeled and unlabeled data typically a small set of labeled data combined with a large amount of unlabeled data
note that in cases of unsupervised learning there may be no training data at all to speak of in other wordsand the data to be labeled is the training data
note that sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output
for example the unsupervised equivalent of classification is normally known as clustering based on the common perception of the task as involving no training data to speak of and of grouping the input data into clusters based on some inherent similarity measure e
the distance between instances considered as vectors in a multidimensional vector space rather than assigning each input instance into one of a set of predefined classes
note also that in some fields the terminology is different for example in community ecology the term classification is used to refer to what is commonly known as clustering
the piece of input data for which an output value is generated is formally termed an instance
the instance is formally described by a vector of features which together constitute a description of all known characteristics of the instance
these feature vectors can be seen as defining points in an appropriate multidimensional space and methods for manipulating vectors in vector spaces can be correspondingly applied to them such as computing the dot product or the angle between two vectors
typically features are either categorical also known as nominal i
consisting of one of a set of unordered items such as a gender of male or female or a blood type of a b ab or o ordinal consisting of one of a set of ordered items e
large medium or small integervalued e
a count of the number of occurrences of a particular word in an email or realvalued e
a measurement of blood pressure
often categorical and ordinal data are grouped together likewise for integervalued and realvalued data
furthermore many algorithms work only in terms of categorical data and require that realvalued or integervalued data be discretized into groups e
less than  between  and  or greater than
many common pattern recognition algorithms are probabilistic in nature in that they use statistical inference to find the best label for a given instance
unlike other algorithms which simply output a best label often probabilistic algorithms also output a probability of the instance being described by the given label
in addition many probabilistic algorithms output a list of the nbest labels with associated probabilities for some value of n instead of simply a single best label
when the number of possible labels is fairly small e
in the case of classification n may be set so that the probability of all possible labels is output
probabilistic algorithms have many advantages over nonprobabilistic algorithmsfeature selection algorithms attempt to directly prune out redundant or irrelevant features
a general introduction to feature selection which summarizes approaches and challenges has been given
the complexity of featureselection is because of its nonmonotonous character an optimization problem where given a total of formula features the powerset consisting of all formula subsets of features need to be explored
the branchandbound algorithm does reduce this complexity but is intractable for medium to large values of the number of available features formula
for a largescale comparison of featureselection algorithms see techniques to transform the raw feature vectors feature extraction are sometimes used prior to application of the patternmatching algorithm
for example feature extraction algorithms attempt to reduce a largedimensionality feature vector into a smallerdimensionality vector that is easier to work with and encodes less redundancy using mathematical techniques such as principal components analysis pca
the distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable while the features left after feature selection are simply a subset of the original features
formally the problem of supervised pattern recognition can be stated as follows given an unknown function formula the ground truth that maps input instances formula to output labels formula along with training data formula assumed to represent accurate examples of the mapping produce a function formula that approximates as closely as possible the correct mapping formula
for example if the problem is filtering spam then formula is some representation of an email and formula is either spam or nonspam
in order for this to be a welldefined problem approximates as closely as possible needs to be defined rigorously
in decision theory this is defined by specifying a loss function or cost function that assigns a specific value to loss resulting from producing an incorrect label
the goal then is to minimize the expected loss with the expectation taken over the probability distribution of formula
in practice neither the distribution of formula nor the ground truth function formula are known exactly but can be computed only empirically by collecting a large number of samples of formula and handlabeling them using the correct value of formula a timeconsuming process which is typically the limiting factor in the amount of data of this sort that can be collected
the particular loss function depends on the type of label being predicted
for example in the case of classification the simple zeroone loss function is often sufficient
this corresponds simply to assigning a loss of  to any incorrect labeling and implies that the optimal classifier minimizes the error rate on independent test data i
counting up the fraction of instances that the learned function formula labels wrongly which is equivalent to maximizing the number of correctly classified instances
the goal of the learning procedure is then to minimize the error rate maximize the correctness on a typical test set
for a probabilistic pattern recognizer the problem is instead to estimate the probability of each possible output label given a particular input instance i
to estimate a function of the formwhere the feature vector input is formula and the function f is typically parameterized by some parameters formula
in a discriminative approach to the problem f is estimated directly
in a generative approach however the inverse probability formula is instead estimated and combined with the prior probability formula using bayes rule as followswhen the labels are continuously distributed e
in regression analysis the denominator involves integration rather than summationthe value of formula is typically learned using maximum a posteriori map estimation
this finds the best value that simultaneously meets two conflicting objects to perform as well as possible on the training data smallest errorrate and to find the simplest possible model
essentially this combines maximum likelihood estimation with a regularization procedure that favors simpler models over more complex models
in a bayesian context the regularization procedure can be viewed as placing a prior probability formula on different values of formula
mathematicallywhere formula is the value used for formula in the subsequent evaluation procedure and formula the posterior probability of formula is given byin the bayesian approach to this problem instead of choosing a single parameter vector formula the probability of a given label for a new instance formula is computed by integrating over all possible values of formula weighted according to the posterior probabilitythe first pattern classifier  the linear discriminant presented by fisher  was developed in the frequentist tradition
the frequentist approach entails that the model parameters are considered unknown but objective
the parameters are then computed estimated from the collected data
for the linear discriminant these parameters are precisely the mean vectors and the covariance matrix
also the probability of each class formula is estimated from the collected dataset
note that the usage of bayes rule in a pattern classifier does not make the classification approach bayesian
bayesian statistics has its origin in greek philosophy where a distinction was already made between the a priori and the a posteriori knowledge
later kant defined his distinction between what is a priori known  before observation  and the empirical knowledge gained from observations
in a bayesian pattern classifier the class probabilities formula can be chosen by the user which are then a priori
moreover experience quantified as a priori parameter values can be weighted with empirical observations  using e
the beta conjugate prior and dirichletdistributions
the bayesian approach facilitates a seamless intermixing between expert knowledge in the form of subjective probabilities and objective observations
probabilistic pattern classifiers can be used according to a frequentist or a bayesian approach
within medical science pattern recognition is the basis for computeraided diagnosis cad systems
cad describes a procedure that supports the doctors interpretations and findings
other typical applications of pattern recognition techniques are automatic speech recognition classification of text into several categories e
spamnonspam email messages the automatic recognition of handwritten postal codes on postal envelopes automatic recognition of images of human faces or handwriting image extraction from medical forms
the last two examples form the subtopic image analysis of pattern recognition that deals with digital images as input to pattern recognition systems
optical character recognition is a classic example of the application of a pattern classifier seeocrexample
the method of signing ones name was captured with stylus and overlay starting in
the strokes speed relative min relative max acceleration and pressure is used to uniquely identify and confirm identity
banks were first offered this technology but were content to collect from the fdic for any bank fraud and did not want to inconvenience customers
artificial neural networks neural net classifiers and deep learning have many realworld applications in image processing a few examplesfor a discussion of the aforementioned applications of neural networks in image processing see e
in psychology pattern recognition making sense of and identifying objects is closely related to perception which explains how the sensory inputs humans receive are made meaningful
pattern recognition can be thought of in two different ways the first being template matching and the second being feature detection
a template is a pattern used to produce items of the same proportions
the templatematching hypothesis suggests that incoming stimuli are compared with templates in the long term memory
if there is a match the stimulus is identified
feature detection models such as the pandemonium system for classifying letters selfridge  suggest that the stimuli are broken down into their component parts for identification
for example a capital e has three horizontal lines and one vertical line
algorithms for pattern recognition depend on the type of label output on whether learning is supervised or unsupervised and on whether the algorithm is statistical or nonstatistical in nature
statistical algorithms can further be categorized as generative or discriminative
parametricnonparametricunsupervisedsupervised supervisedunsupervisedsupervisedunsupervised
in artificial intelligence eager learning is a learning method in which the system tries to construct a general inputindependent target function during training of the system as opposed to lazy learning where generalization beyond the training data is delayed until a query is made to the system
the main advantage gained in employing an eager learning method such as an artificial neural network is that the target function will be approximated globally during training thus requiring much less space than using a lazy learning system
eager learning systems also deal much better with noise in the training data
eager learning is an example of offline learning in which posttraining queries to the system have no effect on the system itself and thus the same query to the system will always produce the same result
the main disadvantage with eager learning is that it is generally unable to provide good local approximations in the target function
large margin nearest neighbor lmnn classification is a statistical machine learning algorithm for metric learning
it learns a pseudometric designed for knearest neighbor classification
the algorithm is based on semidefinite programming a subclass of convex optimization
the goal of supervised learning more specifically classification is to learn a decision rule that can categorize data instances into predefined classes
the knearest neighbor rule assumes a training data set of labeled instances i
the classes are known
it classifies a new data instance with the class obtained from the majority vote of the k closest labeled training instances
closeness is measured with a predefined metric
large margin nearest neighbors is an algorithm that learns this global pseudometric in a supervised fashion to improve the classification accuracy of the knearest neighbor rule
the main intuition behind lmnn is to learn a pseudometric under which all data instances in the training set are surrounded by at least k instances that share the same class label
if this is achieved the leaveoneout error a special case of cross validation is minimized
let the training data consist of a data set formula where the set of possible class categories is formula
the algorithm learns a pseudometric of the type for formula to be well defined the matrix formula needs to be positive semidefinite
the euclidean metric is a special case where formula is the identity matrix
this generalization is often falsely referred to as mahalanobis metric
figure  illustrates the effect of the metric under varying formula
the two circles show the set of points with equal distance to the center formula
in the euclidean case this set is a circle whereas under the modified mahalanobis metric it becomes an ellipsoid
the algorithm distinguishes between two types of special data points target neighbors and impostors
target neighbors are selected before learning
each instance formula has exactly formula different target neighbors within formula which all share the same class label formula
the target neighbors are the data points that should become nearest neighbors under the learned metric
let us denote the set of target neighbors for a data point formula as formula
an impostor of a data point formula is another data point formula with a different class label i
formula which is one of the nearest neighbors of formula
during learning the algorithm tries to minimize the number of impostors for all data instances in the training set
large margin nearest neighbors optimizes the matrix formula with the help of semidefinite programming
the objective is twofold for every data point formula the target neighbors should be close and the impostors should be far away
figure  shows the effect of such an optimization on an illustrative example
the learned metric causes the input vector formula to be surrounded by training instances of the same class
if it was a test point it would be classified correctly under the formula nearest neighbor rule
the first optimization goal is achieved by minimizing the average distance between instances and their target neighborsthe second goal is achieved by constraining impostors formula to be one unit further away than target neighbors formula and therefore pushing them out of the local neighborhood of formula
the resulting inequality constraint can be stated asthe margin of exactly one unit fixes the scale of the matrix formula
any alternative choice formula would result in a rescaling of formula by a factor of formula
the final optimization problem becomeshere the slack variables formula absorb the amount of violations of the impostor constraints
their overall sum is minimized
the last constraint ensures that formula is positive semidefinite
the optimization problem is an instance of semidefinite programming sdp
although sdps tend to suffer from high computational complexity this particular sdp instance can be solved very efficiently due to the underlying geometric properties of the problem
in particular most impostor constraints are naturally satisfied and do not need to be enforced during runtime
a particularly well suited solver technique is the working set method which keeps a small set of constraints that are actively enforced and monitors the remaining likely satisfied constraints only occasionally to ensure correctness
lmnn was extended to multiple local metrics in the  paper
this extension significantly improves the classification error but involves a more expensive optimization problem
in their  publication in the journal of machine learning research weinberger and saul derive an efficient solver for the semidefinite program
it can learn a metric for the mnist handwritten digit data set in several hours involving billions of pairwise constraints
an open source matlab implementation is freely available at the authors web page
kumal et al
extended the algorithm to incorporate local invariances to multivariate polynomial transformations and improved regularization
in computational learning theory machine learning and theory of computation rademacher complexity named after hans rademacher measures richness of a class of realvalued functions with respect to a probability distribution
given a set formula the rademacher complexity of a is defined as followswhere formula are independent random variables drawn from the rademacher distribution i
formula for formula
given a sample formula and a class formula of realvalued functions defined on a domain space formula the empirical rademacher complexity of formula given formula is defined asthis can also be written using the previous definitionwhere formula denotes function composition i
let formula be a probability distribution over formula
the rademacher complexity of the function class formula with respect to formula for sample size formula iswhere the above expectation is taken over an identically independently distributed i
sample formula generated according to formula
formula contains a single vector e
formula
thenthe same is true for every singleton hypothesis class
formula contains two vectors e
formula
thenthe rademacher complexity can be used to derive datadependent upperbounds on the learnability of function classes
intuitively a functionclass with smaller rademacher complexity is easier to learn
in machine learning it is desired to have a training set that represents the true distribution of samples
this can be quantified using the notion of representativeness
denote by p the probability distribution from which the samples are drawn
denote by formula the set of hypotheses potential classifiers and denote by formula the corresponding set of error functions i
for every formula there is a function formula that maps each training sample featureslabel to the error of the classifier formula on that sample for example if we do binary classification and the error function is the simple  loss then formula is a function that returns  for each training sample on which formula is correct and  for each training sample on which formula is wrong
definethe representativeness of the sample formula with respect to formula and formula is defined assmaller representativeness is better since it means that the empirical error of a classifier on the training set is not much lower than its true error
the expected representativeness of a sample can be bounded by the expected rademacher complexity of the function classwhen the rademacher complexity is small it is possible to learn the hypothesis class h using empirical risk minimization
for example with binary error function for every formula with probability at least formula for every hypothesis formulasince smaller rademacher complexity is better it is useful to have upper bounds on the rademacher complexity of various function sets
the following rules can be used to upperbound the rademacher complexity of a set formula
if all vectors in formula are translated by a constant vector formula then rada does not change
if all vectors in formula are multiplied by a scalar formula then rada is multiplied by formula
radab  rada  radb
kakadetewari lemma if all vectors in formula are operated by a lipschitz function then rada is at most multiplied by the lipschitz constant of the function
in particular if all vectors in formula are operated by a contraction mapping then rada strictly decreases
the rademacher complexity of the convex hull of formula equals rada
massart lemma the rademacher complexity of a finite set grows logarithmically with the set size
formally let formula be a set of formula vectors in formula and let formula be the mean of the vectors in formula
thenin particular if formula is a set of binary vectors the norm is at most formula solet formula be a set family whose vc dimension is formula
it is known that the growth function of formula is bounded asthis means that for every set formula with at most formula elements formula
the setfamily formula can be considered as a set of binary vectors over formula
substituting this in massarts lemma giveswith more advanced techniques dudleys entropy bound and hausslers upper bound one can show for example that there exists a constant formula such that any class of formulaindicator functions with vapnikchervonenkis dimension formula has rademacher complexity upperbounded by formula
the following bounds are related to linear operations on formula  a constant set of formula vectors in formula
define formula the set of dotproducts of the vectors in formula with vectors in the unit ball
then
define formula the set of dotproducts of the vectors in formula with vectors in the unit ball of the norm
thenthe following bound relates the rademacher complexity of a set formula to its external covering number  the number of balls of a given radius formula whose union contains formula
the bound is attributed to dudley
suppose formula is a set of vectors whose length norm is at most formula
then for every integer formulain particular if formula lies in a ddimensional subspace of formula thensubstituting this in the previous bound gives the following bound on the rademacher complexitygaussian complexity is a similar complexity with similar physical meanings and can be obtained from the rademacher complexity using the random variables formula instead of formula where formula are gaussian i
random variables with zeromean and variance  i
formula
aiva artificial intelligence virtual artist is a deep learning algorithm applied to music composition
in june  it became the first system of algorithmic composition to be registered as a composer in an authors right society sacem
created in february  aiva specializes in classical and symphonic music composition
it became the worlds first virtual composer to be recognized by a music society sacem
by reading a large collection of existing works of classical music written by human composers such as bach beethoven mozart aiva is capable of understanding concepts of music theory and composing on its own
the algorithm aiva is based on deep learning and reinforcement learning architecturesaiva is a published composer its first studio album genesis was released in november  and counts  original and  orchestrated works composed by aiva
the tracks were recorded by human musiciansolivier hecho as the conductor of the aiva sinfonietta orchestra and eric breton as a pianist
track listingavignon symphonic orchestra orap also performed aivas compositions in april
this is the preview of the score op
n for piano solo a little chamber music composed by aiva
in probability and statistics base rate generally refers to the base class probabilities unconditioned on featural evidence frequently also known as prior probabilities
for example if it were the case that  of the public were medical professionals and  of the public were not medical professionals then the base rate of medical professionals is simply
in the sciences including medicine the base rate is critical for comparison
it may at first seem impressive that  people beat their winter cold while using treatment x until we look at the entire treatment x population and find that the base rate of success is only  i
people tried the treatment but the other  people never really beat their winter cold
the treatments effectiveness is clearer when such base rate information i
people
out of how many is available
note that controls may likewise offer further information for comparison maybe the control groups who were using no treatment at all had their own base rate success of
controls thus indicate that treatment x makes things worse despite that initial proud claim about  people
the normative method for integrating base rates prior probabilities and featural evidence likelihoods is given by bayes rule
a large number of psychological studies have examined a phenomenon called baserate neglect or base rate fallacy in which category base rates are not integrated with featural evidence in the normative manner
mathematician keith devlin provides an illustration of the risks of this he asks us to imagine that there is a type of cancer that afflicts  of all people
a doctor then says there is a test for that cancer which is about  reliable
he also says that the test provides a positive result for  of people who have the cancer but it also results in a false positive for  of people  who do not have the cancer
now if we test positive we may be tempted to think it is  likely that we have the cancer
devlin explains that in fact our odds are less than
what is missing from the jumble of statistics is the most relevant base rate information
we should ask the doctor out of the number of people who test positive this is the base rate group that we care about how many have the cancer in assessing the probability that a given individual is a member of a particular class we must account for other information besides the base rate
in particular we must account for featural evidence
for example when we see a person wearing a white doctors coat and stethoscope and prescribing medication we have evidence which may allow us to conclude that the probability of this particular individual being a medical professional is considerably greater than the category base rate of
these datasets are used for machinelearning research and have been cited in peerreviewed academic journals
datasets are an integral part of the field of machine learning
major advances in this field can result from advances in learning algorithms such as deep learning computer hardware and lessintuitively the availability of highquality training datasets
highquality labeled training datasets for supervised and semisupervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data
although they do not need to be labeled highquality datasets for unsupervised learning can also be difficult and costly to produce
datasets consisting primarily of images or videos for tasks such as object detection facial recognition and multilabel classification
in computer vision face images have been used extensively to develop facial recognition systems face detection and many other projects that use images of faces
datasets consisting primarily of text for tasks such as natural language processing sentiment analysis translation and cluster analysis
datasets of sounds and sound features
datasets containing electric signal information requiring some sort of signal processing for further analysis
datasets from physical systemsdatasets from biological systems
this section includes datasets that deals with structured data
datasets consisting of rows of observations and columns of attributes characterizing those observations
typically used for regression analysis or classification but other types of algorithms can also be used
this section includes datasets that do not fit in the above categories
as datasets come in myriad formats and can sometimes be difficult to use there has been considerable work put into curating and standardizing the format of datasets to make them easier to use for machine learning research
in machine learning the study and construction of algorithms that can learn from and make predictions on data is a common task
such algorithms work by making datadriven predictions or decisions through building a mathematical model from input data
the data used to build the final model usually comes from multiple datasets
in particular three data sets are commonly used in different stages of the creation of the model
the model is initially fit on a training dataset that is a set of examples used to fit the parameters e
weights of connections between neurons in artificial neural networks of the model
the model e
a neural net or a naive bayes classifier is trained on the training dataset using a supervised learning method e
gradient descent or stochastic gradient descent
in practice the training dataset often consist of pairs of an input vector and the corresponding answer vector or scalar which is commonly denoted as the target
the current model is run with the training dataset and produces a result which is then compared with the target for each input vector in the training dataset
based on the result of the comparison and the specific learning algorithm being used the parameters of the model are adjusted
the model fitting can include both variable selection and parameter estimation
successively the fitted model is used to predict the responses for the observations in a second dataset called the validation dataset
the validation dataset provides an unbiased evaluation of a model fit on the training dataset while tuning the models hyperparameters e
the number of hidden units in a neural network
validation datasets can be used for regularization by early stopping stop training when the error on the validation dataset increases as this is a sign of overfitting to the training dataset
this simple procedure is complicated in practice by the fact that the validation datasets error may fluctuate during training producing multiple local minima
this complication has led to the creation of many adhoc rules for deciding when overfitting has truly begun
finally the test dataset is a dataset used to provide an unbiased evaluation of a final model fit on the training dataset
a training dataset is a dataset of examples used for learning that is to fit the parameters e
weights of for example a classifier
most approaches that search through training data for empirical relationships tend to overfit the data meaning that they can identify apparent relationships in the training data that do not hold in general
a test dataset is a dataset that is independent of the training dataset but that follows the same probability distribution as the training dataset
if a model fit to the training dataset also fits the test dataset well minimal overfitting has taken place see figure below
a better fitting of the training dataset as opposed to the test dataset usually points to overfitting
a test set is therefore a set of examples used only to assess the performance i
generalization of a fully specified classifier
a validation dataset is a set of examples used to tune the hyperparameters i
the architecture of a classifier
it is sometimes also called the development set or the dev set
in artificial neural networks a hyperparameter is for example the number of hidden units
it as well as the testing set as mentioned above should follow the same probability distribution as the training dataset
in order to avoid overfitting when any classification parameter needs to be adjusted it is necessary to have a validation dataset in addition to the training and test datasets
for example if the most suitable classifier for the problem is sought the training dataset is used to train the candidate algorithms the validation dataset is used to compare their performances and decide which one to take and finally the test dataset is used to obtain the performance characteristics such as accuracy sensitivity specificity fmeasure and so on
the validation dataset functions as a hybrid it is training data used by testing but neither as part of the lowlevel training nor as part of the final testing
the basic process of using a validation dataset for model selection as part of training dataset validation dataset and test dataset isan application of this process is in early stopping where the candidate models are successive iterations of the same network and training stops when the error on the validation set grows choosing the previous model the one with minimum error
most simply part of the training dataset can be set aside and used as a validation set this is known as the holdout method
common proportions are  trainingvalidation
alternatively the hold out process can be repeated repeatedly partitioning the original training dataset into a training dataset and a validation dataset this is known as crossvalidation
these repeated partitions can be done in various ways such as dividing into  equal datasets and using them as trainingvalidation and then validationtraining or repeatedly selecting a random subset as a validation dataset
another example of parameter adjustment is hierarchical classification sometimes referred to as instance space decomposition  which splits a complete multiclass problem into a set of smaller classification problems
it serves for learning more accurate concepts due to simpler classification boundaries in subtasks and individual feature selection procedures for subtasks
when doing classification decomposition the central choice is the order of combination of smaller classification steps called the classification path
depending on the application it can be derived from the confusion matrix and uncovering the reasons for typical errors and finding ways to prevent the system make those in the future
for example on the validation set one can see which classes are most frequently mutually confused by the system and then the instance space decomposition is done as follows firstly the classification is done among well recognizable classes and the difficult to separate classes are treated as a single joint class and finally as a second classification step the joint class is classified into the two initially mutually confused classes
semisupervised learning is a class of supervised learning tasks and techniques that also make use of unlabeled data for training  typically a small amount of labeled data with a large amount of unlabeled data
semisupervised learning falls between unsupervised learning without any labeled training data and supervised learning with completely labeled training data
many machinelearning researchers have found that unlabeled data when used in conjunction with a small amount of labeled data can produce considerable improvement in learning accuracy
the acquisition of labeled data for a learning problem often requires a skilled human agent e
to transcribe an audio segment or a physical experiment e
determining the d structure of a protein or determining whether there is oil at a particular location
the cost associated with the labeling process thus may render a fully labeled training set infeasible whereas acquisition of unlabeled data is relatively inexpensive
in such situations semisupervised learning can be of great practical value
semisupervised learning is also of theoretical interest in machine learning and as a model for human learning
as in the supervised learning framework we are given a set of formula independently identically distributed examples formula with corresponding labels formula
additionally we are given formula unlabeled examples formula
semisupervised learning attempts to make use of this combined information to surpass the classification performance that could be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning
semisupervised learning may refer to either transductive learning or inductive learning
the goal of transductive learning is to infer the correct labels for the given unlabeled data formula only
the goal of inductive learning is to infer the correct mapping from formula to formula
intuitively we can think of the learning problem as an exam and labeled data as the few example problems that the teacher solved in class
the teacher also provides a set of unsolved problems
in the transductive setting these unsolved problems are a takehome exam and you want to do well on them in particular
in the inductive setting these are practice problems of the sort you will encounter on the inclass exam
it is unnecessary and according to vapniks principle imprudent to perform transductive learning by way of inferring a classification rule over the entire input space however in practice algorithms formally designed for transduction or induction are often used interchangeably
in order to make any use of unlabeled data we must assume some structure to the underlying distribution of data
semisupervised learning algorithms make use of at least one of the following assumptions
points which are close to each other are more likely to share a label
this is also generally assumed in supervised learning and yields a preference for geometrically simple decision boundaries
in the case of semisupervised learning the smoothness assumption additionally yields a preference for decision boundaries in lowdensity regions so that there are fewer points close to each other but in different classes
the data tend to form discrete clusters and points in the same cluster are more likely to share a label although data sharing a label may be spread across multiple clusters
this is a special case of the smoothness assumption and gives rise to feature learning with clustering algorithms
the data lie approximately on a manifold of much lower dimension than the input space
in this case we can attempt to learn the manifold using both the labeled and unlabeled data to avoid the curse of dimensionality
then learning can proceed using distances and densities defined on the manifold
the manifold assumption is practical when highdimensional data are being generated by some process that may be hard to model directly but which only has a few degrees of freedom
for instance human voice is controlled by a few vocal folds and images of various facial expressions are controlled by a few muscles
we would like in these cases to use distances and smoothness in the natural space of the generating problem rather than in the space of all possible acoustic waves or images respectively
the heuristic approach of selftraining also known as selflearning or selflabeling is historically the oldest approach to semisupervised learning with examples of applications starting in the s see for instance scudder
the transductive learning framework was formally introduced by vladimir vapnik in the s
interest in inductive learning using generative models also began in the s
a probably approximately correct learning bound for semisupervised learning of a gaussian mixture was demonstrated by ratsaby and venkatesh in
semisupervised learning has recently become more popular and practically relevant due to the variety of problems for which vast quantities of unlabeled data are availablee
text on websites protein sequences or images
for a review of recent work see a survey article by zhu
generative approaches to statistical learning first seek to estimate formula  the distribution of data points belonging to each class
the probability formula that a given point formula has label formula is then proportional to formula by bayes rule
semisupervised learning with generative models can be viewed either as an extension of supervised learning classification plus information about formula or as an extension of unsupervised learning clustering plus some labels
generative models assume that the distributions take some particular form formula parameterized by the vector formula
if these assumptions are incorrect the unlabeled data may actually decrease the accuracy of the solution relative to what would have been obtained from labeled data alone
however if the assumptions are correct then the unlabeled data necessarily improves performance
the unlabeled data are distributed according to a mixture of individualclass distributions
in order to learn the mixture distribution from the unlabeled data it must be identifiable that is different parameters must yield different summed distributions
gaussian mixture distributions are identifiable and commonly used for generative models
the parameterized joint distribution can be written as formula by using the chain rule
each parameter vector formula is associated with a decision function formula
the parameter is then chosen based on fit to both the labeled and unlabeled data weighted by formulaanother major class of methods attempts to place boundaries in regions where there are few data points labeled or unlabeled
one of the most commonly used algorithms is the transductive support vector machine or tsvm which despite its name may be used for inductive learning as well
whereas support vector machines for supervised learning seek a decision boundary with maximal margin over the labeled data the goal of tsvm is a labeling of the unlabeled data such that the decision boundary has maximal margin over all of the data
in addition to the standard hinge loss formula for labeled data a loss function formula is introduced over the unlabeled data by letting formula
tsvm then selects formula from a reproducing kernel hilbert space formula by minimizing the regularized empirical riskan exact solution is intractable due to the nonconvex term formula so research has focused on finding useful approximations
other approaches that implement lowdensity separation include gaussian process models information regularization and entropy minimization of which tsvm is a special case
graphbased methods for semisupervised learning use a graph representation of the data with a node for each labeled and unlabeled example
the graph may be constructed using domain knowledge or similarity of examples two common methods are to connect each data point to its formula nearest neighbors or to examples within some distance formula
the weight formula of an edge between formula and formula is then set to formula
within the framework of manifold regularizationthe graph serves as a proxy for the manifold
a term is added to the standard tikhonov regularization problem to enforce smoothness of the solution relative to the manifold in the intrinsic space of the problem as well as relative to the ambient input space
the minimization problem becomeswhere formula is a reproducing kernel hilbert space and formula is the manifold on which the data lie
the regularization parameters formula and formula control smoothness in the ambient and intrinsic spaces respectively
the graph is used to approximate the intrinsic regularization term
defining the graph laplacian formula where formula and formula the vector formula we havethe laplacian can also be used to extend the supervised learning algorithms regularized least squares and support vector machines svm to semisupervised versions laplacian regularized least squares and laplacian svm
some methods for semisupervised learning are not intrinsically geared to learning from both unlabeled and labeled data but instead make use of unlabeled data within a supervised learning framework
for instance the labeled and unlabeled examples formula may inform a choice of representation distance metric or kernel for the data in an unsupervised first step
then supervised learning proceeds from only the labeled examples
selftraining is a wrapper method for semisupervised learning
first a supervised learning algorithm is trained based on the labeled data only
this classifier is then applied to the unlabeled data to generate more labeled examples as input for the supervised learning algorithm
generally only the labels the classifier is most confident of are added at each step
cotraining is an extension of selftraining in which multiple classifiers are trained on different ideally disjoint sets of features and generate labeled examples for one another
human responses to formal semisupervised learning problems have yielded varying conclusions about the degree of influence of the unlabeled data for a summary see
more natural learning problems may also be viewed as instances of semisupervised learning
much of human concept learning involves a small amount of direct instruction e
parental labeling of objects during childhood combined with large amounts of unlabeled experience e
observation of objects without naming or counting them or at least without feedback
human infants are sensitive to the structure of unlabeled natural categories such as images of dogs and cats or male and female faces
more recent work has shown that infants and children take into account not only the unlabeled examples available but the sampling process from which labeled examples arise
machine learning control mlc is a subfield of machine learning intelligent control and control theorywhich solves optimal control problems with methods of machine learning
key applications are complex nonlinear systemsfor which linear control theory methods are not applicable
four types of problems are commonly encountered
mlc comprises for instance neural network control genetic algorithm based control genetic programming controlreinforcement learning control and has methodolical overlaps with other datadriven controllike artificial intelligence and robot control
mlc has been successfully appliedto many nonlinear control problemsexploring unknown and often unexpected actuation mechanisms
example applications includeas for all general nonlinear methodsmlc comes with no guaranteed convergence optimality or robustness for a range of operating conditions
a learning automaton is one type of machine learning algorithm studied since s
learning automata select their current action based on past experiences from the environment
it will fall into the range of reinforcement learning if the environment is stochastic and markov decision process mdp is used
research in learning automata can be traced back to the work of michael lvovitch tsetlin in the early s in the soviet union
together with some colleagues he published a collection of papers on how to use matrices to describe automata functions
additionally tsetlin worked on reasonable and collective automata behaviour and on automata games
learning automata were also investigated by researches in the united states in the s
however the term learning automaton was not used until narendra and thathachar introduced it in a survey paper in
a learning automaton is an adaptive decisionmaking unit situated in a random environment that learns the optimal action through repeated interactions with its environment
the actions are chosen according to a specific probability distribution which is updated based on the environment response the automaton obtains by performing a particular action
with respect to the field of reinforcement learning learning automata are characterized as policy iterators
in contrast to other reinforcement learners policy iterators directly manipulate the policy
another example for policy iterators are evolutionary algorithms
formally narendra and thathachar define a stochastic automaton to consist ofin their paper they investigate only stochastic automata with rs and g being bijective allowing them to confuse actions and states
the states of such an automaton correspond to the states of a discretestate discreteparameter markov process
at each time step t
the automaton reads an input from its environment updates pt to pt by a randomly chooses a successor state according to the probabilities pt and outputs the corresponding action
the automatons environment in turn reads the action and sends the next input to the automaton
frequently the input set x     is used with  and  corresponding to a nonpenalty and a penalty response of the environment respectively in this case the automaton should learn to minimize the number of penalty responses and the feedback loop of automaton and environment is called a pmodel
more generally a qmodel allows an arbitrary finite input set x and an smodel uses the interval  of real numbers as x
finite actionset learning automata fala are a class of learning automata for which the number of possible actions is finite or in more mathematical terms for which the size of the actionset is finite
in statistical classification including machine learning two main approaches are called the generative approach and the discriminative approach
these compute classifiers by different approaches differing in the degree of statistical modelling
terminology is inconsistent but three major types can be distinguished following
given an observable variable x and a target variable y a generative model is a statistical model of the joint probability distribution on xy formula a discriminative model is a model of the conditional probability of the target y given an observation x symbolically formula and classifiers computed without using a probability model are also referred to loosely as discriminative
the distinction between these last two classes is not consistently made refers to these three classes as generative learning conditional learning and discriminative learning but only distinguishes two classes calling them generative classifiers joint distribution and discriminative classifiers conditional distribution or no distribution not distinguishing between the latter two classes
analogously a classifier based on a generative model is a generative classifier while a classifier based on a discriminative model is a discriminative classifier though this term also refers to classifiers that are not based on a model
standard examples of each all of which are linear classifiers are generative classifiers naive bayes classifier and linear discriminant analysis discriminative model logistic regression nonmodel classifier perceptron and support vector machine
in application to classification one wishes to go from an observation x to a label y or probability distribution on labels
one can compute this directly without using a probability distribution distributionfree classifier one can estimate the probability of a label given an observation formula discriminative model and base classification on that or one can estimate the joint distribution formula generative model from that compute the conditional probability formula and then base classification on that
these are increasingly indirect but increasingly probabilistic allowing more domain knowledge and probability theory to be applied
in practice different approaches are used depending on the particular problem and hybrids can combine strengths of multiple approaches
an alternative division defines these symmetrically as a generative model is a model of the conditional probability of the observable x given a target y symbolically formula while a discriminative model is a model of the conditional probability of the target y given an observation x symbolically formula
regardless of precise definition the terminology is because a generative model can be used to generate random instances outcomes either of an observation and target formula or of an observation x given a target value y
while a discriminative model or discriminative classifier without a model can be used to discriminate the value of the target variable y given an observation x
the difference between discriminate distinguish and classify is subtle and these are not consistently distinguished so the term discriminative classifier because a pleonasm meaning that it does nothing other than classify equivalently discriminate inputs
in application to classification the observable x is frequently a continuous variable the target y is generally a discrete variable consisting of a finite set of labels and the conditional probability formula can also be interpreted as a nondeterministic target function formula considering x as inputs and y as outputs
given a finite set of labels the two definitions of generative model are closely related
a model of the conditional distribution formula is a model of the distribution of each label and a model of the joint distribution is equivalent to a model of the distribution of label values formula together with the distribution of observations given a label formula symbolically formula thus while a model of the joint probability distribution is more information than a model of the distribution of label but without their relative frequencies it is a relatively small step hence these are not always distinguished
given a model of the joint distribution formula the distribution of the individual variables can be computed as the marginal distributions formula and formula considering x as continuous hence integrating over it and y as discrete hence summing over it and either conditional distribution can be computed from the definition of conditional probability formula and formula
given a model of one conditional probability and estimated probability distributions for the variables x and y denoted formula and formula one can estimate the opposite conditional probability using bayes rulefor example given a generative model for formula one can estimateand given a discriminative model for formula one can estimatenote that bayes rule computing one conditional probability in terms of the other and the definition of conditional probability computing conditional probability in terms of the joint distribution are frequently conflated as well
a generative algorithm models how the data was generated in order to categorize a signal
it asks the question based on my generation assumptions which category is most likely to generate this signal a discriminative algorithm does not care about how the data was generated it simply categorizes a given signal
so discriminative algorithms try to learn formula directly from the data and then try to classify data
on the other hand generative algorithms try to learn formula which can be transformed into formula later to classify the data
one of the advantages of generative algorithms is that you can use formula to generate new data similar to existing data
on the other hand discriminative algorithms generally give better performance in classification tasks
despite the fact that discriminative models do not need to model the distribution of the observed variables they cannot generally express complex relationships between the observed and target variables
they dont necessarily perform better than generative models at classification and regression tasks
the two classes are seen as complementary or as different views of the same procedure
types of generative models areif the observed data are truly sampled from the generative model then fitting the parameters of the generative model to maximize the data likelihood is a common method
however since most statistical models are only approximations to the true distribution if the models application is to infer about a subset of variables conditional on known values of others then it can be argued that the approximation makes more assumptions than are necessary to solve the problem at hand
in such cases it can be more accurate to model the conditional density functions directly using a discriminative model see below although applicationspecific details will ultimately dictate which approach is most suitable in any particular case
suppose the input data is formula the set of labels for formula is formula and there are the following  data pointsformulafor the above data estimating the joint probability distribution formula from the empirical measure will be the followingwhile formula will be following gives an example in which a table of frequencies of english word pairs is used to generate a sentence beginning with representing and speedily is an good which is not proper english but which will increasingly approximate it as the table is moved from word pairs to word triplets etc
the curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in highdimensional spaces often with hundreds or thousands of dimensions that do not occur in lowdimensional settings such as the threedimensional physical space of everyday experience
the expression was coined by richard e
bellman when considering problems in dynamic optimization
there are multiple phenomena referred to by this name in domains such as numerical analysis sampling combinatorics machine learning data mining and databases
the common theme of these problems is that when the dimensionality increases the volume of the space increases so fast that the available data become sparse
this sparsity is problematic for any method that requires statistical significance
in order to obtain a statistically sound and reliable result the amount of data needed to support the result often grows exponentially with the dimensionality
also organizing and searching data often relies on detecting areas where objects form groups with similar properties in high dimensional data however all objects appear to be sparse and dissimilar in many ways which prevents common data organization strategies from being efficient
these effects are also used for simplification of machine learning algorithms in high dimension that is blessing of dimensionality
the blessings of dimensionality and the curse of dimensionality are recognised as two complementary influential principles in highdimensional data analysis
in some problems each variable can take one of several discrete values or the range of possible values is divided to give a finite number of possibilities
taking the variables together a huge number of combinations of values must be considered
this effect is also known as the combinatorial explosion
even in the simplest case of formula binary variables the number of possible combinations already is formula exponential in the dimensionality
naively each additional dimension doubles the effort needed to try all combinations
there is an exponential increase in volume associated with adding extra dimensions to a mathematical space
for example  evenly spaced sample points suffice to sample a unit interval a dimensional cube with no more than
distance between points an equivalent sampling of a dimensional unit hypercube with a lattice that has a spacing of
between adjacent points would require  sample points
in general with a spacing distance of  the dimensional hypercube appears to be a factor of  larger than the dimensional hypercube which is the unit interval
in the above example n when using a sampling distance of
the dimensional hypercube appears to be  larger than the unit interval
this effect is a combination of the combinatorics problems above and the distance function problems explained below
when solving dynamic optimization problems by numerical backward induction the objective function must be computed for each combination of values
this is a significant obstacle when the dimension of the state variable is large
in machine learning problems that involve learning a stateofnature from a finite number of data samples in a highdimensional feature space with each feature having a range of possible values typically an enormous amount of training data is required to ensure that there are several samples with each combination of values
a typical rule of thumb is that there should be at least  training examples for each dimension in the representation
with a fixed number of training samples the predictive power of a classifier or regressor first increases as number of dimensionsfeatures used is increased but then decreases which is known as hughes phenomenon or peaking phenomena
when a measure such as a euclidean distance is defined using many coordinates there is little difference in the distances between different pairs of samples
one way to illustrate the vastness of highdimensional euclidean space is to compare the proportion of an inscribed hypersphere with radius formula and dimension formula to that of a hypercube with edges of length formula
the volume of such a sphere is formula where formula is the gamma function while the volume of the cube is formula
as the dimension formula of the space increases the hypersphere becomes an insignificant volume relative to that of the hypercube
this can clearly be by comparing the proportions as the dimension formula goes to infinityfurthermore the distance between the center and the corners is formula which increases without bound for fixed r
in this sense nearly all of the highdimensional space is far away from the centre
to put it another way the highdimensional unit hypercube can be said to consist almost entirely of the corners of the hypercube with almost no middle
this also helps to understand the chisquared distribution
indeed the noncentral chisquared distribution associated to a random point in the interval   is the same as the distribution of the lengthsquared of a random point in the dcube
by the law of large numbers this distribution concentrates itself in a narrow band around d times the standard deviation squared  of the original derivation
this illuminates the chisquared distribution and also illustrates that most of the volume of the dcube concentrates near the surface of a sphere of radius
a further development of this phenomenon is as follows
any fixed distribution on  induces a product distribution on points in
for any fixed n it turns out that the minimum and the maximum distance between a random reference point q and a list of n random data points p
p become indiscernible compared to the minimum distancethis is often cited as distance functions losing their usefulness for the nearestneighbor criterion in featurecomparison algorithms for example in high dimensions
however recent research has shown this to only hold in the artificial scenario when the onedimensional distributions  are independent and identically distributed
when attributes are correlated data can become easier and provide higher distance contrast and the signaltonoise ratio was found to play an important role thus feature selection should be used
the effect complicates nearest neighbor search in high dimensional space
it is not possible to quickly reject candidates by using the difference in one coordinate as a lower bound for a distance based on all the dimensions
however it has recently been observed that the mere number of dimensions does not necessarily result in difficulties since relevant additional dimensions can also increase the contrast
in addition for the resulting ranking it remains useful to discern close and far neighbors
irrelevant noise dimensions however reduce the contrast in the manner described above
in time series analysis where the data are inherently highdimensional distance functions also work reliably as long as the signaltonoise ratio is high enough
another effect of high dimensionality on distance functions concerns knearest neighbor knn graphs constructed from a data set using a distance function
as the dimension increases the indegree distribution of the knn digraph becomes skewed with a peak on the right because of the emergence of a disproportionate number of hubs that is datapoints that appear in many more knn lists of other datapoints than the average
this phenomenon can have a considerable impact on various techniques for classification including the knn classifier semisupervised learning and clustering and it also affects information retrieval
in a recent survey zimek et al
identified the following problems when searching for anomalies in highdimensional datamany of the analyzed specialized methods tackle one or another of these problems but there remain many open research questions
multiplicative weight update method is a metaalgorithm
it is an algorithmic technique which maintains a distribution on a certain set of interest and updates it iteratively by multiplying the probability mass of elements by suitably chosen factors based on feedback obtained by running another algorithm on the distribution
it was discovered repeatedly in very diverse fields such as machine learning adaboost winnow hedge optimization solving linear programs theoretical computer science devising fast algorithm for lps and sdps and game theory
multiplicative weights implies the iterative rule used in algorithms derived from the multiplicative weight update method
it is given with different names in the different fields where it was discovered or rediscovered
the earliest known version of this technique was in an algorithm named fictitious play which was proposed in game theory in the early s
grigoriadis and khachiyan applied a randomized variant of fictitious play to solve twoplayer zerosum games efficiently using the multiplicative weights algorithm
in this case player allocates higher weight to the actions that had a better outcome and choose his strategy relying on these weights
in machine learning littlestone applied the earliest form of the multiplicative weights update rule in his famous winnow algorithm which is similar to minsky and paperts earlier perceptron learning algorithm
later he generalized the winnow algorithm to weighted majority algorithm
freund and schapire followed his steps and generalized the winnow algorithm in the form of hedge algorithm
the multiplicative weights algorithm is also widely applied in computational geometry such as clarksons algorithm for linear programming lp with a bounded number of variables in linear time
later bronnimann and goodrich employed analogous methods to find set covers for hypergraphs with small vc dimension
in operation research and online statistical decision making problem field the weighted majority algorithm and its more complicated versions have been found independently
in computer science field some researchers have previously observed the close relationships between multiplicative update algorithms used in different contexts
young discovered the similarities between fast lp algorithms and raghavans method of pessimistic estimators for derandomization of randomized rounding algorithms klivans and servedio linked boosting algorithms in learning theory to proofs of yaos xor lemma garg and khandekar defined a common framework for convex optimization problems that contains gargkonemann and plotkinshmoystardos as subcases
a binary decision needs to be made based on n experts opinions to attain an associated payoff
in the first round all experts opinions have the same weight
the decision maker will make the first decision based on the majority of the experts prediction
then in each successive round the decision maker will repeatedly update the weight of each experts opinion depending on the correctness of his prior predictions
real life examples includes predicting if it is rainy tomorrow or if the stock market will go up or go down
given a sequential game played between an adversary and an aggregator who is advised by n experts the goal is for the aggregator to make as few mistakes as possible
assume there is an expert among the n experts who always gives the correct prediction
in the halving algorithm only the consistent experts are retained
experts who make mistake will all be dismissed
for every decision the aggregator decides by taking a majority vote among the remaining experts
therefore every time the aggregator makes a mistake at least half of the remaining experts are dismissed
the aggregator makes at most mistakes
unlike halving algorithm which dismisses experts who have made mistakes weighted majority algorithm discounts their advice
given the same expert advice setup suppose we have n decisions and we need to select one decision for each loop
in each loop every decision incurs a cost
all costs will be revealed after making the choice
the cost is  if the expert is correct and  otherwise
this algorithms goal is to limit its cumulative losses to roughly the same as the best of experts
the very first algorithm that makes choice based on majority vote every iteration does not work since the majority of the experts can be wrong consistently every time
the weighted majority algorithm corrects above trivial algorithm by keeping a weight of experts instead of fixing the cost at either  or
this would make fewer mistakes compared to halving algorithm
if formula the weight of the experts advice will remain the same
when formula increases the weight of the experts advice will decrease
note that some researchers fix formula in weighted majority algorithm
after formula steps let formula be the number of mistakes of expert i and formula be the number of mistakes our algorithm has made
then we have the following bound for every formulain particular this holds for i which is the best expert
since the best expert will have the least formula it will give the best bound on the number of mistakes made by the algorithm as a whole
given the same setup with n experts
consider the special situation where the proportions of experts predicting positive and negative counting the weights are both close to
then there might be a tie
following the weight update rule in weighted majority algorithm the predictions made by the algorithm would be randomized
the algorithm calculates the probabilities of experts predicting positive or negatives and then makes a random decision based on the computed fractionpredict where the number of mistakes made by the randomized weighted majority algorithm is bounded as where formula and formula
note that only the learning algorithm is randomized
the underlying assumption is that the examples and experts predictions are not random
the only randomness is the randomness where the learner makes his own prediction
in this randomized algorithm formula if formula
compared to weighted algorithm this randomness halved the number of mistakes the algorithm is going to make
however it is important to note that in some research people define formula in weighted majority algorithm and allow formula in randomized weighted majority algorithm
the multiplicative weights method is usually used to solve a constrained optimization problem
let each expert be the constraint in the problem and the events represent the points in the area of interest
the punishment of the expert corresponds to how well its corresponding constraint is satisfied on the point represented by an event
suppose we were given the distribution formula on experts
let formula  payoff matrix of a finite twoplayer zerosum game with formula rows
when the row player formula uses plan formula and the column player formula uses plan formula the payoff of player formula is formulaformula assuming formula
if player formula chooses action formula from a distribution formula over the rows then the expected result for player formula selecting action formula is formula
to maximize formula player formula is should choose plan formula
similarly the expected payoff for player formula is formula
choosing plan formula would minimize this payoff
by john von neumanns minmax theorem we obtainwhere p and i changes over the distributions over rows q and j changes over the columns
then let formula denote the common value of above quantities also named as the value of the game
let formula be an error parameter
to solve the zerosum game bounded by additive error of formulaso there is an algorithm solving zerosum game up to an additive factor of  using oformula calls to oracle with an additional processing time of on per callin machine learning littlestone and warmuth generalized the winnow algorithm to the weighted majority algorithm
later freund and schapire generalized it in the form of hedge algorithm
adaboost algorithm formulated by yoav freund and robert schapire also employed the multiplicative weight update method
based on current knowledge in algorithms multiplicative weight update method was first used in littlestones winnow algorithm
it is used in machine learning to solve a linear program
given formula labeled examples formula where formula are feature vectors and formula are their labels
the aim is to find nonnegative weights such that for all examples the sign of the weighted combination of the features matches its labels
that is require that formula for all formula
without loss of generality assume the total weight is  so that they form a distribution
thus for notational convenience redefine formula to be formula the problem reduces to finding a solution to the following lpthis is general form of lp
the hedge algorithm is similar to the weighted majority algorithm
however their exponential update rules are different
it is generally used to solve the problem of binary allocation in which we need to allocate different portion of resources into n different options
the loss with every option is available at the end of every iteration
the goal is to reduce the total loss suffered for a particular allocation
the allocation for the following iteration is then revised based on the total loss suffered in the current iteration using multiplicative update
assume the learning rate formula and for formula formula is picked by hedge
then for all experts formulainitialization fix an formula
for each expert associate the weight formula for ttthis algorithm maintains a set of weights formula over the training examples
on every iteration formula a distribution formula is computed by normalizing these weights
this distribution is fed to the weak learner weaklearn which generates a hypothesis formula that hopefully has small error with respect to the distribution
using the new hypothesis formula adaboost generates the next weight vector formula
the process repeats
after t such iterations the final hypothesis formula is the output
the hypothesis formula combines the outputs of the t weak hypotheses using a weighted majority vote
given a formula matrix formula and formula is there a formula such that formulausing the oracle algorithm in solving zerosum problem with an error parameter formula the output would either be a point formula such that formula or a proof that formula does not exist i
there is no solution to this linear system of inequalities
given vector formula solves the following relaxed problemif there exists a x satisfying  then x satisfies  for all formula
the contrapositive of this statement is also true
suppose if oracle returns a feasible solution for a formula the solution formula it returns has bounded width formula
so if there is a solution to  then there is an algorithm that its output x satisfies the system  up to an additive error of formula
the algorithm makes at most formula calls to a widthbounded oracle for the problem
the contrapositive stands true as well
the multiplicative updates is applied in the algorithm in this case
in operations research and online statistical decision making problem field the weighted majority algorithm and its more complicated versions have been found independently
the multiplicative weights algorithm is also widely applied in computational geometry such as clarksons algorithm for linear programming lp with a bounded number of variables in linear time
later bronnimann and goodrich employed analogous methods to find set covers for hypergraphs with small vc dimension
nearest neighbor search nns as a form of proximity search is the optimization problem of finding the point in a given set that is closest or most similar to a given point
closeness is typically expressed in terms of a dissimilarity function the less similar the objects the larger the function values
formally the nearestneighbor nn search problem is defined as follows given a set s of points in a space m and a query point qm find the closest point in s to q
donald knuth in vol
of the art of computer programming  called it the postoffice problem referring to an application of assigning to a residence the nearest post office
a direct generalization of this problem is a knn search where we need to find the k closest points
most commonly m is a metric space and dissimilarity is expressed as a distance metric which is symmetric and satisfies the triangle inequality
even more common m is taken to be the ddimensional vector space where dissimilarity is measured using the euclidean distance manhattan distance or other distance metric
however the dissimilarity function can be arbitrary
one example are asymmetric bregman divergences for which the triangle inequality does not hold
the nearest neighbor search problem arises in numerous fields of application includingvarious solutions to the nns problem have been proposed
the quality and usefulness of the algorithms are determined by the time complexity of queries as well as the space complexity of any search data structures that must be maintained
the informal observation usually referred to as the curse of dimensionality states that there is no generalpurpose exact solution for nns in highdimensional euclidean space using polynomial preprocessing and polylogarithmic search time
the simplest solution to the nns problem is to compute the distance from the query point to every other point in the database keeping track of the best so far
this algorithm sometimes referred to as the naive approach has a running time of odn where n is the cardinality of s and d is the dimensionality of m
there are no search data structures to maintain so linear search has no space complexity beyond the storage of the database
naive search can on average outperform space partitioning approaches on higher dimensional spaces
since the s branch and bound methodology has been applied to the problem
in the case of euclidean space this approach is known as spatial index or spatial access methods
several spacepartitioning methods have been developed for solving the nns problem
perhaps the simplest is the kd tree which iteratively bisects the search space into two regions containing half of the points of the parent region
queries are performed via traversal of the tree from the root to a leaf by evaluating the query point at each split
depending on the distance specified in the query neighboring branches that might contain hits may also need to be evaluated
for constant dimension query time average complexity is ologn in the case of randomly distributed points worst case complexity is oknkalternatively the rtree data structure was designed to support nearest neighbor search in dynamic context as it has efficient algorithms for insertions and deletions such as the r tree
rtrees can yield nearest neighbors not only for euclidean distance but can also be used with other distances
in case of general metric space branch and bound approach is known under the name of metric trees
particular examples include vptree and bktree
using a set of points taken from a dimensional space and put into a bsp tree and given a query point taken from the same space a possible solution to the problem of finding the nearest pointcloud point to the query point is given in the following description of an algorithm
strictly speaking no such point may exist because it may not be unique
but in practice usually we only care about finding any one of the subset of all pointcloud points that exist at the shortest distance to a given query point
the idea is for each branching of the tree guess that the closest point in the cloud resides in the halfspace containing the query point
this may not be the case but it is a good heuristic
after having recursively gone through all the trouble of solving the problem for the guessed halfspace now compare the distance returned by this result with the shortest distance from the query point to the partitioning plane
this latter distance is that between the query point and the closest possible point that could exist in the halfspace not searched
if this distance is greater than that returned in the earlier result then clearly there is no need to search the other halfspace
if there is such a need then you must go through the trouble of solving the problem for the other half space and then compare its result to the former result and then return the proper result
the performance of this algorithm is nearer to logarithmic time than linear time when the query point is near the cloud because as the distance between the query point and the closest pointcloud point nears zero the algorithm needs only perform a lookup using the query point as a key to get the correct result
an approximation algorithm is allowed to return a point whose distance from the query is at most formula times the distance from the query to its nearest points
the appeal of this approach is that in many cases an approximate nearest neighbor is almost as good as the exact one
in particular if the distance measure accurately captures the notion of user quality then small differences in the distance should not matter
locality sensitive hashing lsh is a technique for grouping points in space into buckets based on some distance metric operating on the points
points that are close to each other under the chosen metric are mapped to the same bucket with high probability
the cover tree has a theoretical bound that is based on the datasets doubling constant
the bound on search time is oclogn where c is the expansion constant of the dataset
in the special case where the data is a dense d map of geometric points the projection geometry of the sensing technique can be used to dramatically simplify the search problem
this approach requires that the d data is organized by a projection to a two dimensional grid and assumes that the data is spatially smooth across neighboring grid cells with the exception of object boundaries
these assumptions are valid when dealing with d sensor data in applications such as surveying robotics and stereo vision but may not hold for unorganized data in general
in practice this technique has an average search time of o or ok for the knearest neighbor problem when applied to real world stereo vision data
in high dimensional spaces tree indexing structures become useless because an increasing percentage of the nodes need to be examined anyway
to speed up linear search a compressed version of the feature vectors stored in ram is used to prefilter the datasets in a first run
the final candidates are determined in a second stage using the uncompressed data from the disk for distance calculation
the vafile approach is a special case of a compression based search where each feature component is compressed uniformly and independently
the optimal compression technique in multidimensional spaces is vector quantization vq implemented through clustering
the database is clustered and the most promising clusters are retrieved
huge gains over vafile treebased indexes and sequential scan have been observed
also note the parallels between clustering and lsh
one possible way to solve nns is to construct a graph formula where every point formula is uniquely associated with vertex formula
the search of the point in the set s closest to the query q takes the form of the search of vertex in the graph formula
one of the basic vertex search algorithms in graphs with metric objects is the greedy search algorithm
it starts from the random vertex formula
the algorithm computes a distance value from the query q to each vertex from the neighborhood formula of the current vertex formula and then selects a vertex with the minimal distance value
if the distance value between the query and the selected vertex is smaller than the one between the query and the current element then the algorithm moves to the selected vertex and it becomes new current vertex
the algorithm stops when it reaches a local minimum a vertex whose neighborhood does not contain a vertex that is closer to the query than the vertex itself
this idea was exploited in multiple publications including the seminal paper by arya and mount in the voronet system for the plane in the raynet system for the formula in the metrized small world algorithm for the general metric space
this work was preceded by a pioneering paper by toussaint where he introduced a concept of a relative neighborhood graph
there are numerous variants of the nns problem and the two most wellknown are the knearest neighbor search and the approximate nearest neighbor search
knearest neighbor search identifies the top k nearest neighbors to the query
this technique is commonly used in predictive analytics to estimate or classify a point based on the consensus of its neighbors
knearest neighbor graphs are graphs in which every point is connected to its k nearest neighbors
in some applications it may be acceptable to retrieve a good guess of the nearest neighbor
in those cases we can use an algorithm which doesnt guarantee to return the actual nearest neighbor in every case in return for improved speed or memory savings
often such an algorithm will find the nearest neighbor in a majority of cases but this depends strongly on the dataset being queried
algorithms that support the approximate nearest neighbor search include localitysensitive hashing best bin first and balanced boxdecomposition tree based search
nearest neighbor distance ratio do not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depending on the distance to the previous neighbor
it is used in cbir to retrieve pictures through a query by example using the similarity between local features
more generally it is involved in several matching problems
fixedradius near neighbors is the problem where one wants to efficiently find all points given in euclidean space within a given fixed distance from a specified point
the data structure should work on a distance which is fixed however the query point is arbitrary
for some applications e
entropy estimation we may have n datapoints and wish to know which is the nearest neighbor for every one of those n points
this could of course be achieved by running a nearestneighbor search once for every point but an improved strategy would be an algorithm that exploits the information redundancy between these n queries to produce a more efficient search
as a simple example when we find the distance from point x to point y that also tells us the distance from point y to point x so the same calculation can be reused in two different queries
given a fixed dimension a semidefinite positive norm thereby including every l norm and n points in this space the nearest neighbour of every point can be found in onlogn time and the m nearest neighbours of every point can be found in omnlogn time
coupled pattern learner cpl is a machine learning algorithm which couples the semisupervised learning of categories and relations to forestall the problem of semantic drift associated with bootstrap learning methods
semisupervised learning approaches using a small number of labeled examples with many unlabeled examples are usually unreliable as they produce an internally consistent but incorrect set of extractions
cpl solves this problem by simultaneously learning classifiers for many different categories and relations in the presence of an ontology defining constraints that couple the training of these classifiers
it was introduced by andrew carlson justin betteridge estevam r
hruschka jr
and tom m
mitchell in
cpl is an approach to semisupervised learning that yields more accurate results by coupling the training of many information extractors
basic idea behind cpl is that semisupervised training of a single type of extractor such as coach is much more difficult than simultaneously training many extractors that cover a variety of interrelated entity and relation types
using prior knowledge about the relationships between these different entities and relations cpl makes unlabeled data as a useful constraint during training
for e
coachx implies personx and not sportx
cpl primarily relies on the notion of coupling the learning of multiple functions so as to constrain the semisupervised learning problem
cpl constrains the learned function in two ways
each predicate p in the ontology has a list of other samearity predicates with which p is mutually exclusive
if a is mutually exclusive with predicate b as positive instances and patterns become negative instances and negative patterns for b
for example if city having an instance boston and a pattern mayor of arg is mutually exclusive with scientist then boston and mayor of arg will become a negative instance and a negative pattern respectively for scientist
further some categories are declared to be a subset of another category
for e
athlete is a subset of person
this is a type checking information used to couple the learning of relations and categories
for example the arguments of the ceoof relation are declared to be of the categories person and company
cpl does not promote a pair of noun phrases as an instance of a relation unless the two noun phrases are classified as belonging to the correct argument types
following is a quick summary of the cpl algorithm
a large corpus of partofspeech tagged sentences and an initial ontology with predefined categories relations mutually exclusive relationships between samearity predicates subset relationships between some categories seed instances for all predicates and seed patterns for the categories
cpl finds new candidate instances by using newly promoted patterns to extract the noun phrases that cooccur with those patterns in the text corpus
cpl extractscandidate instances and patterns are filtered to maintain high precision and to avoid extremely specific patterns
an instance is only considered for assessment if it cooccurs with at least two promoted patterns in the text corpus and if its cooccurrence count with all promoted patterns is at least three times greater than its cooccurrence count with negative patterns
cpl ranks candidate instances using the number of promoted patterns that they cooccur with so that candidates that occur with more patterns are ranked higher
patterns are ranked using an estimate of the precision of each pattern
cpl ranks the candidates according to their assessment scores and promotes at most  instances and  patterns for each predicate
instances and patterns are only promoted if they cooccur with at least two promoted patterns or instances respectively
metabootstrap learner mbl was also proposed by the authors of cpl in
metabootstrap learner couples the training of multiple extraction techniques with a multiview constraint which requires the extractors to agree
it makes addition of coupling constraints on top of existing extraction algorithms while treating them as black boxes feasible
mbl assumes that the errors made by different extraction techniques are independent
following is a quick summary of mbl
subordinate algorithms used with mbl do not promote any instance on their own they report the evidence about each candidate to mbl and mbl is responsible for promoting instances
in their paper authors have presented results showing the potential of cpl to contribute new facts to existing repository of semantic knowledge freebase
logic learning machine llm is a machine learning method based on the generation of intelligible rules
llm is an efficient implementation of the switching neural network snn paradigm developed by marco muselli senior researcher at the italian national research council cnrieiit in genoa
logic learning machine is implemented in the rulex suite
llm has been employed in different fields including orthopaedic patient classification dna microarray analysis and clinical decision support system
the switching neural network approach was developed in the s to overcome the drawbacks of the most commonly used machine learning methods
in particular black box methods such as multilayer perceptron and support vector machine had good accuracy but could not provide deep insight into the studied phenomenon
on the other hand decision trees were able to describe the phenomenon but often lacked accuracy
switching neural networks made use of boolean algebra to build sets of intelligible rules able to obtain very good performance
in  an efficient version of switching neural network was developed and implemented in the rulex suite with the name logic learning machine
also a llm version devoted to regression problems was developed
like other machine learning methods llm uses data to build a model able to perform a good forecast about future behaviors
llm starts from a table including a target variable output and some inputs and generates a set of rules that return the output value formula corresponding to a given configuration of inputs
a rule is written in the formwhere consequence contains the output value whereas premise includes one or more conditions on the inputs
according to the input type conditions can have different formsa possible rule is therefore in the formaccording to the output type different versions of logic learning machine have been developed
in machine learning early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method such as gradient descent
such methods update the learner so as to make it better fit the training data with each iteration
up to a point this improves the learners performance on data outside of the training set
past that point however improving the learners fit to the training data comes at the expense of increased generalization error
early stopping rules provide guidance as to how many iterations can be run before the learner begins to overfit
early stopping rules have been employed in many different machine learning methods with varying amounts of theoretical foundation
this section presents some of the basic machinelearning concepts required for a description of early stopping methods
machine learning algorithms train a model based on a finite set of training data
during this training the model is evaluated based on how well it predicts the observations contained in the training set
in general however the goal of a machine learning scheme is to produce a model that generalizes that is that predicts previously unseen observations
overfitting occurs when a model fits the data in the training set well while incurring larger generalization error
regularization in the context of machine learning refers to the process of modifying a learning algorithm so as to prevent overfitting
this generally involves imposing some sort of smoothness constraint on the learned model
this smoothness may be enforced explicitly by fixing the number of parameters in the model or by augmenting the cost function as in tikhonov regularization
tikhonov regularization along with principal component regression and many other regularization schemes fall under the umbrella of spectral regularization regularization characterized by the application of a filter
early stopping also belongs to this class of methods
gradient descent methods are firstorder iterative optimization methods
each iteration updates an approximate solution to the optimization problem by taking a step in the direction of the negative of the gradient of the objective function
by choosing the stepsize appropriately such a method can be made to converge to a local minimum of the objective function
gradient descent is used in machinelearning by defining a loss function that reflects the error of the learner on the training set and then minimizing that function
earlystopping can be used to regularize nonparametric regression problems encountered in machine learning
for a given input space formula output space formula and samples drawn from an unknown probability measure formula on formula the goal of such problems is to approximate a regression function formula given bywhere formula is the conditional distribution at formula induced by formula
one common choice for approximating the regression function is to use functions from a reproducing kernel hilbert space
these spaces can be infinite dimensional in which they can supply solutions that overfit training sets of arbitrary size
regularization is therefore especially important for these methods
one way to regularize nonparametric regression problems is to apply an early stopping rule to an iterative procedure such as gradient descent
the early stopping rules proposed for these problems are based on analysis of upper bounds on the generalization error as a function of the iteration number
they yield prescriptions for the number of iterations to run that can be computed prior to starting the solution process
let formula and formula
given a set of samplesdrawn independently from formula minimize the functionalwhere formula is a member of the reproducing kernel hilbert space formula
that is minimize the expected risk for a leastsquares loss function
since formula depends on the unknown probability measure formula it cannot be used for computation
instead consider the following empirical risklet formula and formula be the tth iterates of gradient descent applied to the expected and empirical risks respectively where both iterations are initialized at the origin and both use the step size formula
the formula form the population iteration which converges to formula but cannot be used in computation while the formula form the sample iteration which usually converges to an overfitting solution
we want to control the difference between the expected risk of the sample iteration and the minimum expected risk that is the expected risk of the regression functionthis difference can be rewritten as the sum of two terms the difference in expected risk between the sample and population iterations and that between the population iteration and the regression functionthis equation presents a biasvariance tradeoff which is then solved to give an optimal stopping rule that may depend on the unknown probability distribution
that rule has associated probabilistic bounds on the generalization error
for the analysis leading to the early stopping rule and bounds the reader is referred to the original article
in practice datadriven methods e
crossvalidation can be used to obtain an adaptive stopping rule
boosting refers to a family of algorithms in which a set of weak learners learners that are only slightly correlated with the true process are combined to produce a strong learner
it has been shown for several boosting algorithms including adaboost that regularization via early stopping can provide guarantees of consistency that is that the result of the algorithm approaches the true solution as the number of samples goes to infinity
boosting methods have close ties to the gradient descent methods described above can be regarded as a boosting method based on the formula loss lboost
these early stopping rules work by splitting the original training set into a new training set and a validation set
the error on the validation set is used as a proxy for the generalization error in determining when overfitting has begun
these methods are most commonly employed in the training of neural networks
prechelt gives the following summary of a naive implementation of holdoutbased early stopping as followsmore sophisticated forms use crossvalidation multiple partitions of the data into training set and validation set instead of a single partition into a training set and validation set
even this simple procedure is complicated in practice by the fact that the validation error may fluctuate during training producing multiple local minima
this complication has led to the creation of many adhoc rules for deciding when overfitting has truly begun
document classification or document categorization is a problem in library science information science and computer science
the task is to assign a document to one or more classes or categories
this may be done manually or intellectually or algorithmically
the intellectual classification of documents has mostly been the province of library science while the algorithmic classification of documents is mainly in information science and computer science
the problems are overlapping however and there is therefore interdisciplinary research on document classification
the documents to be classified may be texts images music etc
each kind of document possesses its special classification problems
when not otherwise specified text classification is implied
documents may be classified according to their subjects or according to other attributes such as document type author printing year etc
in the rest of this article only subject classification is considered
there are two main philosophies of subject classification of documents the contentbased approach and the requestbased approach
contentbased classification is classification in which the weight given to particular subjects in a document determines the class to which the document is assigned
it is for example a common rule for classification in libraries that at least  of the content of a book should be about the class to which the book is assigned
in automatic classification it could be the number of times given words appears in a document
requestoriented classification or indexing is classification in which the anticipated request from users is influencing how documents are being classified
the classifier asks himself under which descriptors should this entity be found and think of all the possible queries and decide for which ones the entity at hand is relevant soergel  p
requestoriented classification may be classification that is targeted towards a particular audience or user group
for example a library or a database for feminist studies may classifyindex documents differently when compared to a historical library
it is probably better however to understand requestoriented classification as policybased classification the classification is done according to some ideals and reflects the purpose of the library or database doing the classification
in this way it is not necessarily a kind of classification or indexing based on user studies
only if empirical data about use or users are applied should requestoriented classification be regarded as a userbased approach
sometimes a distinction is made between assigning documents to classes classification versus assigning subjects to documents subject indexing but as frederick wilfrid lancaster has argued this distinction is not fruitful
these terminological distinctions he writes are quite meaningless and only serve to cause confusion lancaster  p
the view that this distinction is purely superficial is also supported by the fact that a classification system may be transformed into a thesaurus and vice versa cf
aitchison   broughton  riesthuis  bliedung
therefore is the act of labeling a document say by assigning a term from a controlled vocabulary to a document at the same time to assign that document to the class of documents indexed by that term all documents indexed or classified as x belong to the same class of documents
automatic document classification tasks can be divided into three sorts supervised document classification where some external mechanism such as human feedback provides information on the correct classification for documents unsupervised document classification also known as document clustering where the classification must be done entirely without reference to external information and semisupervised document classification where parts of the documents are labeled by the external mechanism
there are several software products under various license models available
automatic document classification techniques includeclassification techniques have been applied to
nativelanguage identification nli is the task of determining an authors native language l based only on their writings in a second language l
nli works through identifying languageusage patterns that are common to specific l groups and then applying this knowledge to predict the native language of previously unseen texts
this is motivated in part by applications in secondlanguage acquisition language teaching and forensic linguistics amongst others
nli works under the assumption that an authors l will dispose them towards particular language production patterns in their l as influenced by their native language
this relates to crosslinguistic influence cli a key topic in the field of secondlanguage acquisition sla that analyzes transfer effects from the l on later learned languages
using largescale english data nli methods achieve over  accuracy in predicting the native language of texts written by authors from  different l backgrounds
this can be compared to a baseline of  for choosing randomly
this identification of lspecific features has been used to study language transfer effects in secondlanguage acquisition
this is useful for developing pedagogical material teaching methods lspecific instructions and generating learner feedback that is tailored to their native language
nli methods can also be applied in forensic linguistics as a method of performing authorship profiling in order to infer the attributes of an author including their linguistic background
this is particularly useful in situations where a text e
an anonymous letter is the key piece of evidence in an investigation and clues about the native language of a writer can help investigators in identifying the source
this has already attracted interest and funding from intelligence agencies
natural language processing methods are used to extract and identify language usage patterns common to speakers of an lgroup
this is done using language learner data usually from a learner corpus
next machine learning is applied to train classifiers like support vector machines for predicting the l of unseen texts
a range of ensemble based systems have also been applied to the task and shown to improve performance over single classifier systems
various linguistic feature types have been applied for this task
these include syntactic features such as constituent parses grammatical dependencies and partofspeech tags
surface level lexical features such as character word and lemma ngrams have also been found to be quite useful for this task
however it seems that character ngrams are the single best feature for the task
the building educational applications bea workshop at naacl  hosted the inaugural nli shared task
the competition resulted in  entries from teams across the globe  of which also published a paper describing their systems and approaches
statistical relational learning srl is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty which can be dealt with using statistical methods and complex relational structure
note that srl is sometimes called relational machine learning rml in the literature
typically the knowledge representation formalisms developed in srl use a subset of firstorder logic to describe relational properties of a domain in a general manner universal quantification and draw upon probabilistic graphical models such as bayesian networks or markov networks to model the uncertainty some also build upon the methods of inductive logic programming
significant contributions to the field have been made since the late s
as is evident from the characterization above the field is not strictly limited to learning aspects it is equally concerned with reasoning specifically probabilistic inference and knowledge representation
therefore alternative terms that reflect the main foci of the field include statistical relational learning and reasoning emphasizing the importance of reasoning and firstorder probabilistic languages emphasizing the key properties of the languages with which models are represented
a number of canonical tasks are associated with statistical relational learning the most common ones beingone of the fundamental design goals of the representation formalisms developed in srl is to abstract away from concrete entities and to represent instead general principles that are intended to be universally applicable
since there are countless ways in which such principles can be represented many representation formalisms have been proposed in recent years
in the following some of the more common ones are listed in alphabetical order
multilinear principal component analysis mpca is a multilinear extension of principal component analysis pca
mpca is employed in the analysis of nway arrays i
a cube or hypercube of numbers also informally referred to as a data tensor
nway arrays may be decomposed analyzed or modeled by the origin of mpca can be traced back to the tucker decomposition and peter kroonenbergs mmode pcamode pca work
in  de lathauwer et al
restated tucker and kroonenbergs work in clear and concise numerical computational terms in their siam paper entitled multilinear singular value decomposition hosvd and in their paper on the best rank and rankr r
r  approximation of higherorder tensors
circa  vasilescu reframed the data analysis recognition and synthesis problems as multilinear tensor problems based on the insight that most observed data are the compositional consequence of several causal factors of data formation and are well suited for multimodal data tensor analysis
the power of the tensor framework was showcased by analyzing human motion joint angles facial images or textures in terms of their causal factors of data formation in the following works human motion signaturescvpr  icpr  face recognition  tensorfaceseccv  cvpr  etc
and computer graphics  tensortextures siggraph
historically mpca has been referred to as mmode pca a terminology which was coined by peter kroonenberg in
in  vasilescu and terzopoulos introduced the multilinear pca terminology as a way to better differentiate between linear and multilinear tensor decomposition as well as to better differentiate between the work that computed nd order statistics associated with each data tensor modeaxis and subsequent work on multilinear independent component analysis that computed higher order statistics associated with each tensor modeaxis
multilinear pca may be applied to compute the causal factors of data formation or as signal processing tool on data tensors whose individual observation have either been vectorized or whose observations are treated as matrix and concatenated into a data tensor
mpca computes a set of orthonormal matrices associated with each mode of the data tensor which are analogous to the orthonormal row and column space of a matrix computed by the matrix svd
this transformation aims to capture as high a variance as possible accounting for as much of the variability in the data associated with each data tensor modeaxis
the mpca solution follows the alternating least square als approach
it is iterative in nature
as in pca mpca works on centered data
centering is a little more complicated for tensors and it is problem dependent
mpca features supervised mpca feature selection is used in object recognition while unsupervised mpca feature selection is employed in visualization task
various extensions of mpca have been developed
the following outline is provided as an overview of and topical guide to machine learningmachine learning  subfield of computer science more particularly soft computing that evolved from the study of pattern recognition and computational learning theory in artificial intelligence
in  arthur samuel defined machine learning as a field of study that gives computers the ability to learn without being explicitly programmed
machine learning explores the study and construction of algorithms that can learn from and make predictions on data
such algorithms operate by building a model from an example training set of input observations in order to make datadriven predictions or decisions expressed as outputs rather than following strictly static program instructions
subfields of machine learningcrossdisciplinary fields involving machine learningapplications of machine learningmachine learning hardwaremachine learning tools  listmachine learning frameworkproprietary machine learning frameworksopen source machine learning frameworksmachine learning library  machine learning algorithmmachine learning method  listdimensionality reductionensemble learningmeta learningreinforcement learningsupervised learningbayesian statisticsdecision tree algorithmlinear classifierunsupervised learningartificial neural networkassociation rule learninghierarchical clusteringcluster analysisanomaly detectionsemisupervised learningdeep learningmachine learning researchhistory of machine learningmachine learning projectsmachine learning organizationsbooks about machine learning
knowledge integration is the process of synthesizing multiple knowledge models or representations into a common model representation
compared to information integration which involves merging information having different schemas and representation models knowledge integration focuses more on synthesizing the understanding of a given subject from different perspectives
for example multiple interpretations are possible of a set of student grades typically each from a certain perspective
an overall integrated view and understanding of this information can be achieved if these interpretations can be put under a common model say a student performance index
the webbased inquiry science environment wise from the university of california at berkeley has been developed along the lines of knowledge integration theory
knowledge integration has also been studied as the process of incorporating new information into a body of existing knowledge with an interdisciplinary approach
this process involves determining how the new information and the existing knowledge interact how existing knowledge should be modified to accommodate the new information and how the new information should be modified in light of the existing knowledge
a learning agent that actively investigates the consequences of new information can detect and exploit a variety of learning opportunities e
to resolve knowledge conflicts and to fill knowledge gaps
by exploiting these learning opportunities the learning agent is able to learn beyond the explicit content of the new information
the machine learning program ki developed by murray and porter at the university of texas at austin was created to study the use of automated and semiautomated knowledge integration to assist knowledge engineers constructing a large knowledge base
a possible technique which can be used is semantic matching
more recently a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on minimal mappings
minimal mappings are high quality mappings such that i all the other mappings can be computed from them in time linear in the size of the input graphs and ii none of them can be dropped without losing property i
the university of waterloo operates a bachelor of knowledge integration undergraduate degree program as an academic major or minor
the program started in
quantum machine learning is an emerging interdisciplinary research area at the intersection of quantum physics and machine learning
one can distinguish four different ways of merging the two parent disciplines
quantum machine learning algorithms can use the advantages of quantum computation in order to improve classical methods of machine learning for example by developing efficient implementations of expensive classical algorithms on a quantum computer
on the other hand one can apply classical methods of machine learning to analyse quantum systems
most generally one can consider situations wherein both the learning device and the system under study are fully quantum
a related branch of research explores methodological and structural similarities between certain physical systems and learning systems in particular neural networks which has revealed for example that certain mathematical and numerical techniques from quantum physics carry over to classical deep learning
quantumenhanced machine learning refers to quantum algorithms that solve tasks in machine learning thereby improving a classical machine learning method
such algorithms typically require one to encode the given classical dataset into a quantum computer so as to make it accessible for quantum information processing
after this quantum information processing routines can be applied and the result of the quantum computation is read out by measuring the quantum system
for example the outcome of the measurement of a qubit could reveal the result of a binary classification task
while many proposals of quantum machine learning algorithms are still purely theoretical and require a fullscale universal quantum computer to be tested others have been implemented on smallscale or special purpose quantum devices
one line of approaches is based on the idea of amplitude encoding that is to associate the amplitudes of a quantum state with the inputs and outputs of computations
since a state of formula qubits is described by formula complex amplitudes this information encoding can allow for an exponentially compact representation
intuitively this corresponds to associating a discrete probability distribution over binary random variables with a classical vector
the goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose resources grow polynomially in the number of qubits formula which amounts to a logarithmic growth in the number of amplitudes and thereby the dimension of the input
many quantum machine learning algorithms in this category are based on variations of the quantum algorithm for linear systems of equations which under specific conditions performs a matrix inversion using an amount of physical resources growing only logarithmically in the dimensions of the matrix
one of these conditions is that a hamiltonian which entrywise corresponds to the matrix can be simulated efficiently which is known to be possible if the matrix is sparse or low rank
for reference any known classical algorithm for matrix inversion requires a number of operations that grows at least quadratically in the dimension of the matrix
quantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a linear system of equations for example in leastsquares linear regression the leastsquares version of support vector machines and gaussian processes
a crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation which often requires one to initialise a quantum system in a state whose amplitudes reflect the features of the entire dataset
although efficient methods for state preparation are known for specific cases this step easily hides the complexity of the task
another approach to improving classical machine learning with quantum information processing uses amplitude amplification methods based on grovers search algorithm which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms
these quantum routines can be employed for learning algorithms that translate into an unstructured search task as can be done for instance in the case of the kmedians and the knearest neighbors algorithms
another application is a quadratic speedup in the training of perceptron
amplitude amplification is often combined with quantum walks to achieve the same quadratic speedup
quantum walks have been proposed to enhance googles pagerank algorithm as well as the performance of reinforcement learning agents in the projective simulation framework
reinforcement learning is a third branch of machine learning distinct from supervised and unsupervised learning which also admits quantum enhancements
in quantumenhanced reinforcement learning a quantum agent interacts with a classical environment and occasionally receives rewards for its actions which allows the agent to adapt its behaviourin other words to learn what to do in order to gain more rewards
in some situations either because of the quantum processing capability of the agent or due to the possibility to probe the environment in superpositions a quantum speedup may be achieved
implementations of these kinds of protocols in superconducting circuits and in systems of trapped ions have been proposed
sampling from highdimensional probability distributions is at the core of a wide spectrum of computational techniques with important applications across science engineering and society
examples include deep learning probabilistic programming and other machine learning and artificial intelligence applications
a computationally hard problem which is key for some relevant machine learning tasks is the estimation of averages over probabilistic models defined in terms of a boltzmann distribution
sampling from generic probabilistic models is hard algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become
even though quantum annealers like those produced by dwave systems were designed for challenging combinatorial optimization problems it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects
some research groups have recently explored the use of quantum annealing hardware for training boltzmann machines and deep neural networks
the standard approach to training boltzmann machines relies on the computation of certain averages that can be estimated by standard sampling techniques such as markov chain monte carlo algorithms
another possibility is to rely on a physical process like quantum annealing that naturally generates samples from a boltzmann distribution
the objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset
the dwave x system hosted at nasa ames research center has been recently used for the learning of a special class of restricted boltzmann machines that can serve as a building block for deep learning architectures
complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks
the same device was later used to train a fully connected boltzmann machine to generate reconstruct and classify downscaled lowresolution handwritten digits among other synthetic datasets
in both cases the models trained by quantum annealing had a similar or better performance in terms of quality
the ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications
experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward
inspired by the success of boltzmann machines based on classical boltzmann distribution a new machine learning approach based on quantum boltzmann distribution of a transversefield ising hamiltonian was recently proposed
due to the noncommutative nature of quantum mechanics the training process of the quantum boltzmann machine can become nontrivial
this problem was to some extent circumvented by introducing bounds on the quantum probabilities allowing the authors to train the model efficiently by sampling
it is possible that a specific type of quantum boltzmann machine has been trained in the dwave x by using a learning rule analogous to that of classical boltzmann machines
quantum annealing is not the only technology for sampling
in a prepare and measure scenario a universal quantum computer prepares a thermal state which is then sampled by measurements
this can reduce the time required to train a deep restricted boltzmann machine and provide a richer and more comprehensive framework for deep learning than classical computing
the same quantum methods also permit efficient training of full boltzmann machines and multilayer fully connected models and do not have wellknown classical counterparts
relying on an efficient thermal state preparation protocol starting from an arbitrary state quantumenhanced markov logic networks exploit the symmetries and the locality structure of the probabilistic graphical model generated by a firstorder logic template
this provides an exponential reduction in computational complexity in probabilistic inference and while the protocol relies on a universal quantum computer under mild assumptions it can be embedded on contemporary quantum annealing hardware
there are quantum analogues or generalisations of classical neural nets which are known as quantum neural networks
hidden quantum markov models hqmms are a quantumenhanced version of classical hidden markov models hmms which are typically used to model sequential data in various fields like robotics and natural language processing
unlike the approach taken by other quantumenhanced machine learning algorithms hqmms can be viewed as models inspired by quantum mechanics that can be run on classical computers as well
where classical hmms use probability vectors to represent hidden belief states hqmms use the quantum analogue density matrices
recent work has shown that these models can be successfully learned by maximizing the loglikelihood of the given data via classical optimization and there is some empirical evidence that these models can better model sequential data compared to classical hmms in practice although further work is needed to determine exactly when and how these benefits are derived
additionally since classical hmms are a particular kind of bayes net an exciting aspect of hqmms is that the techniques used show how we can perform quantumanalogous bayesian inference which should allow for the general construction of the quantum versions of probabilistic graphical models
quantum learning theory pursues a mathematical analysis of the quantum generalizations of classical learning models and of the possible speedups or other improvements that they may provide
the framework is very similar to that of classical computational learning theory but the learner in this case is a quantum information processing device while the data may be either classical or quantum
quantum learning theory should be contrasted with the quantumenhanced machine learning discussed above where the goal was to consider specific problems and to use quantum protocols to improve the time complexity of classical algorithms for these problems
although quantum learning theory is still under development partial results in this direction have been obtained
the starting point in learning theory is typically a concept class a set of possible concepts
usually a concept is a function on some domain such as formula
for example the concept class could be the set of disjunctive normal form dnf formulas on n bits or the set of boolean circuits of some constant depth
the goal for the learner is to learnexactly or approximately an unknown target concept from this concept class
the learner may be actively interacting with the target concept or passively receiving samples from it
in active learning a learner can make membership queries to the target concept c asking for its value cx on inputs x chosen by the learner
the learner then has to reconstruct the exact target concept with high probability
in the model of quantum exact learning the learner can make membership queries in quantum superposition
if the complexity of the learner is measured by the number of membership queries it makes then quantum exact learners can be polynomially more efficient than classical learners for some concept classes but not more
if complexity is measured by the amount of timethe learner uses then there are concept classes that can be learned efficiently byquantum learners but not by classical learners under plausible complexitytheoretic assumptions
a natural model of passive learning is valiants probably approximately correct pac learning
here the learner receives random examples xcx where x is distributed according to some unknown distribution d
the learners goal is to output a hypothesis function h such that hxcx with high probability when x is drawn according to d
the learner has to be able to produce such an approximately correct h for every d and every target concept c in its concept class
we can consider replacing the random examples by potentially more powerful quantum examples formula
in the pac model and the related agnostic model this doesnt significantly reduce the number of examples needed for every concept class classical andquantum sample complexity are the same up to constant factors
however for learning under somefixed distribution d quantum examples can be very helpful for example for learning dnf underthe uniform distribution
when considering time complexity there exist concept classes that can be paclearned efficiently by quantum learners even from classical examples but not by classical learners again under plausible complexitytheoretic assumptions
this passive learning type is also the most common scheme in supervised learning a learning algorithm typically takes the training examples fixed without the ability to query the label of unlabelled examples
outputting a hypothesis h is a step of induction
classically an inductive model splits into a training and an application phase the model parameters are estimated in the training phase and the learned model is applied an arbitrary many times in the application phase
in the asymptotic limit of the number of applications this splitting of phases is also present with quantum resources
the term quantum machine learning is also used for approaches that apply classical methods of machine learning to the study of quantum systems
a prime example is the use of classical learning techniques to process large amounts of experimental data in order to characterize an unknown quantum system for instance in the context of quantum information theory and for the development of quantum technologies but there are also more exotic applications
the ability to experimentally control and prepare increasingly complex quantum systems brings with it a growing need to turn large and noisy data sets into meaningful information
this is a problem that has already been studied extensively in the classical setting and consequently many existing machine learning techniques can be naturally adapted to more efficiently address experimentally relevant problems
for example bayesian methods and concepts of algorithmic learning can be fruitfully applied to tackle quantum state classification hamiltonian learning and the characterization of an unknown unitary transformation
other problems that have been addressed with this approach are given in the following listhowever the characterization of quantum states and processes is not the only application of classical machine learning techniques
some additional applications includein the most general case of quantum machine learning both the learning device and the system under study as well as their interaction are fully quantum
this section gives a few examples of results on this topic
one class of problem that can benefit from the fully quantum approach is that of learning unknown quantum states processes or measurements in the sense that one can subsequently reproduce them on another quantum system
for example one may wish to learn a measurement that discriminates between two coherent states given not a classical description of the states to be discriminated but instead a set of example quantum systems prepared in these states
the naive approach would be to first extract a classical description of the states and then implement an ideal discriminating measurement based on this information
this would only require classical learning
however one can show that a fully quantum approach is strictly superior in this case
this also relates to work on quantum pattern matching
the problem of learning unitary transformations can be approached in a similar way
going beyond the specific problem of learning states and transformations the task of clustering also admits a fully quantum version wherein both the oracle which returns the distance between datapoints and the information processing device which runs the algorithm are quantum
finally a general framework spanning supervised unsupervised and reinforcement learning in the fully quantum setting was introduced in where it was also shown that the possibility of probing the environment in superpositions permits a quantum speedup in reinforcement learning
the earliest experiments were conducted using the adiabatic dwave quantum computer for instance to detect cars in digital images using regularized boosting with a nonconvex objective function in a demonstration in
many experiments followed on the same architecture and leading tech companies have shown interest in the potential of quantum machine learning for future technological implementations
in  google research nasa and the universities space research association launched the quantum artificial intelligence lab which explores the use of the adiabatic dwave quantum computer
a more recent example trained a probabilistic generative models with arbitrary pairwise connectivity showing that their model is capable of generating handwritten digits as well as reconstructing noisy images of bars and stripes and handwritten digits
using a different annealing technology based on nuclear magnetic resonance nmr a quantum hopfield network was implemented in  that mapped the input data and memorized data to hamiltonians allowing the use of adiabatic quantum computation
nmr technology also enables universal quantum computing and it was used for the first experimental implementation of a quantum support vector machine to distinguish hand written number  and  on a liquidstate quantum computer in
the training data involved the preprocessing of the image which maps them to normalized dimensional vectors to represent the images as the states of a qubit
the two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image
once the vectors are defined on the feature space the quantum support vector machine was implemented to classify the unknown input vector
the readout avoids costly quantum tomography by reading out the final state in terms of direction updown of the nmr signal
photonic implementations are attracting more attention not the least because they do not require extensive cooling
simultaneous spoken digit and speaker recognition and chaotic timeseries prediction were demonstrated at data rates beyond  gigabyte per second in
using nonlinear photonics to implement an alloptical linear classifier a perceptron model was capable of learning the classification boundary iteratively from training data through a feedback rule
a core building block in many learning algorithms is to calculate the distance between two vectors this was first experimentally demonstrated for up to eight dimensions using entangled qubits in a photonic quantum computer in
recently based on a neuromimetic approach a novel ingredient has been added to the field of quantum machine learning in the form of a socalled quantum memristor a quantized model of the standard classical memristor
this device can be constructed by means of a tunable resistor weak measurements on the system and a classical feedforward mechanism
an implementation of a quantum memristor in superconducting circuits has been proposed and an experiment with quantum dots performed
a quantum memristor would implement nonlinear interactions in the quantum dynamics which would aid the search for a fully functional quantum neural network
the accuracy paradox for predictive analytics states that predictive models with a given level of accuracy may have greater predictive power than models with higher accuracy
it may be better to avoid the accuracy metric in favor of other metrics such as precision and recall
accuracy is often the starting point for analyzing the quality of a predictive model as well as an obvious criterion for prediction
accuracy measures the ratio of correct predictions to the total number of cases evaluated
it may seem obvious that the ratio of correct predictions to cases should be a key metric
a predictive model may have high accuracy but be useless
in an example predictive model for an insurance fraud application all cases that are predicted as highrisk by the model will be investigated
to evaluate the performance of the model the insurance company has created a sample data set of  claims
all  cases in the validation sample have been carefully checked and it is known which cases are fraudulent
a table of confusion assists analyzing the quality of the model
the definition of accuracy the table of confusion for model m and the calculation of accuracy for model m is shown below
formulawhereformula  definition of accuracytable  table of confusion for fraud model m
formulaformula  accuracy for model mwith an accuracy of
model m appears to perform fairly well
the paradox lies in the fact that accuracy can be easily improved to
by always predicting no fraud
the table of confusion and the accuracy for this trivial always predict negative model m and the accuracy of this model are shown below
table  table of confusion for fraud model m
formulaformula  accuracy for model mmodel mreduces the rate of inaccurate predictions from  to
this is an apparent improvement of
the new model m shows fewer incorrect predictions and markedly improved accuracy as compared to the original model m but is obviously useless
the alternative model m does not offer any value to the company for preventing fraud
the less accurate model is more useful than the more accurate model
caution is advised when using accuracy in the evaluation of predictive models it is appropriate only if the cost of a false positive false alarm is equal to the cost of a false negative missed prediction
otherwise a more appropriate loss function should be determined
a constrained conditional model ccm is a machine learning and inference framework that augments the learning of conditional probabilistic or discriminative models with declarative constraints
the constraint can be used as a way to incorporate expressive prior knowledge into the model and bias the assignments made by the learned model to satisfy these constraints
the framework can be used to support decisions in an expressive output space while maintaining modularity and tractability of training and inference
models of this kind have recently attracted much attention within the natural language processing nlp community
formulating problems as constrained optimization problems over the output of learned models has several advantages
it allows one to focus on the modeling of problems by providing the opportunity to incorporate domainspecific knowledge as global constraints using a first order language
using this declarative framework frees the developer from low level feature engineering while capturing the problems domainspecific properties and guarantying exact inference
from a machine learning perspective it allows decoupling the stage of model generation learning from that of the constrained inference stage thus helping to simplify the learning stage while improving the quality of the solutions
for example in the case of generating compressed sentences rather than simply relying on a language model to retain the most commonly used ngrams in the sentence constraints can be used to ensure that if a modifier is kept in the compressed sentence its subject will also be kept
making decisions in many domains such as natural language processing and computer vision problems often involves assigning values to sets of interdependent variables where the expressive dependency structure can influence or even dictate what assignments are possible
these settings are applicable not only to structured learning problems such as semantic role labeling but also for cases that require making use of multiple prelearned components such as summarization textual entailment and question answering
in all these cases it is natural to formulate the decision problem as a constrained optimization problem with an objective function that is composed of learned models subject to domain or problemspecific constraints
constrained conditional models form a learning and inference framework that augments the learning of conditional probabilistic or discriminative models with declarative constraints written for example using a firstorder representation as a way to support decisions in an expressive output space while maintaining modularity and tractability of training and inference
these constraints can express either hard restrictions completely prohibiting some assignments or soft restrictions penalizing unlikely assignments
in most applications of this framework in nlp following integer linear programming ilp was used as the inference framework although other algorithms can be used for that purpose
given a set of feature functions formula and a set of constraints formula defined over an input structure formula and an output structure formula a constraint conditional model is characterized by two weight vectors w and formula and is defined as the solution to the following optimization problemeach constraint formula is a boolean mapping indicating if the joint assignment formula violates a constraint and formula is the penalty incurred for violating the constraints
constraints assigned an infinite penalty are known as hard constraints and represent unfeasible assignments to the optimization problem
the objective function used by ccms can be decomposed and learned in several ways ranging from a complete joint training of the model along with the constraints to completely decoupling the learning and the inference stage
in the latter case several local models are learned independently and the dependency between these models is considered only at decision time via a global decision process
the advantages of each approach are discussed in which studies the two training paradigms  local models li learning  inference and  global model ibt inference based training and shows both theoretically and experimentally that while ibt joint training is best in the limit under some conditions basically good components li can generalize better
the ability of ccm to combine local models is especially beneficial in cases where joint learning is computationally intractable or when training data are not available for joint learning
this flexibility distinguishes ccm from the other learning frameworks that also combine statistical information with declarative constraints such as markov logic network that emphasize joint training
ccm can help reduce supervision by using domain knowledge expressed as constraints to drive learning
these settings were studied in codl and show that by incorporating domain knowledge the performance of the learned model improves significantly
ccms have also been applied to latent learning frameworks where the learning problem is defined over a latent representation layer
since the notion of a correct representation is inherently illdefined no goldstandard labeled data regarding the representation decision is available to the learner
identifying the correct or optimal learning representation is viewed as a structured prediction process and therefore modeled as a ccm
this problem was covered in several papers in both supervised and unsupervised settings
in all cases research showed that explicitly modeling the interdependencies between representation decisions via constraints results in an improved performance
the advantages of the ccm declarative formulation and the availability of offtheshelf solvers have led to a large variety of natural language processing tasks being formulated within the framework including semantic role labeling syntactic parsing coreference resolution summarization transliteration natural language generation and joint information extraction
most of these works use an integer linear programming ilp solver to solve the decision problem
although theoretically solving an integer linear program is exponential in the size of the decision problem in practice using stateoftheart solvers and approximate inference techniques large scale problems can be solved efficiently
the key advantage of using an ilp solver for solving the optimization problem defined by a constrained conditional model is the declarative formulation used as input for the ilp solver consisting of a linear objective function and a set of linear constraints
inductive programming ip is a special area of automatic programming covering research from artificial intelligence and programming which addresses learning of typically declarative logic or functional and often recursive programs from incomplete specifications such as inputoutput examples or constraints
depending on the programming language used there are several kinds of inductive programming
inductive functional programming which uses functional programming languages such as lisp or haskell and most especially inductive logic programming which uses logic programming languages such as prolog and other logical representations such as description logics have been more prominent but other programming language paradigms have also been used such as constraint programming or probabilistic programming
inductive programming incorporates all approaches which are concerned with learning programs or algorithms from incomplete formal specifications
possible inputs in an ip system are a set of training inputs and corresponding outputs or an output evaluation function describing the desired behavior of the intended program traces or action sequences which describe the process of calculating specific outputs constraints for the program to be induced concerning its time efficiency or its complexity various kinds of background knowledge such as standard data types predefined functions to be used program schemes or templates describing the data flow of the intended program heuristics for guiding the search for a solution or other biases
output of an ip system is a program in some arbitrary programming language containing conditionals and loop or recursive control structures or any other kind of turingcomplete representation language
in many applications the output program must be correct with respect to the examples and partial specification and this leads to the consideration of inductive programming as a special area inside automatic programming or program synthesis usually opposed to deductive program synthesis where the specification is usually complete
in other cases inductive programming is seen as a more general area where any declarative programming or representation language can be used and we may even have some degree of error in the examples as in general machine learning the more specific area of structure mining or the area of symbolic artificial intelligence
a distinctive feature is the number of examples or partial specification needed
typically inductive programming techniques can learn from just a few examples
the diversity of inductive programming usually comes from the applications and the languages that are used apart from logic programming and functional programming other programming paradigms and representation languages have been used or suggested in inductive programming such as functional logic programming constraint programming probabilistic programming abductive logic programming modal logic action languages agent languages and many types of imperative languages
research on the inductive synthesis of recursive functional programs started in the early s and was brought onto firm theoretical foundations with the seminal thesis system of summers and work of biermann
these approaches were split into two phases first inputoutput examples are transformed into nonrecursive programs traces using a small set of basic operators second regularities in the traces are searched for and used to fold them into a recursive program
the main results until the mid s are surveyed by smith
due to limited progress with respect to the range of programs that could be synthesized research activities decreased significantly in the next decade
the advent of logic programming brought a new elan but also a new direction in the early s especially due to the mis system of shapiro eventually spawning the new field of inductive logic programming ilp
the early works of plotkin and his relative least general generalization rlgg had an enormous impact in inductive logic programming
most of ilp work addresses a wider class of problems as the focus is not only on recursive logic programs but on machine learning of symbolic hypotheses from logical representations
however there were some encouraging results on learning recursive prolog programs such as quicksort from examples together with suitable background knowledge for example with golem
but again after initial success the community got disappointed by limited progress about the induction of recursive programs with ilp less and less focusing on recursive programs and leaning more and more towards a machine learning setting with applications in relational data mining and knowledge discovery
in parallel to work in ilp koza proposed genetic programming in the early s as a generateandtest based approach to learning programs
the idea of genetic programming was further developed into the inductive programming system adate and the systematicsearchbased system magichaskeller
here again functional programs are learned from sets of positive examples together with an output evaluation fitness function which specifies the desired inputoutput behavior of the program to be learned
the early work in grammar induction also known as grammatical inference is related to inductive programming as rewriting systems or logic programs can be used to represent production rules
in fact early works in inductive inference considered grammar induction and lisp program inference as basically the same problem
the results in terms of learnability were related to classical concepts such as identificationinthelimit as introduced in the seminal work of gold
more recently the language learning problem was addressed by the inductive programming community
in the recent years the classical approaches have been resumed and advanced with great success
therefore the synthesis problem has been reformulated on the background of constructorbased term rewriting systems taking into account modern techniques of functional programming as well as moderate use of searchbased strategies and usage of background knowledge as well as automatic invention of subprograms
many new and successful applications have recently appeared beyond program synthesis most especially in the area of data manipulation programming by example and cognitive modelling see below
other ideas have also been explored with the common characteristic of using declarative languages for the representation of hypotheses
for instance the use of higherorder features schemes or structured distances have been advocated for a better handling of recursive data types and structures abstraction has also been explored as a more powerful approach to cumulative learning and function invention
one powerful paradigm that has been recently used for the representation of hypotheses in inductive programming generally in the form of generative models is probabilistic programming and related paradigms such as stochastic logic programs and bayesian logic programming
the first workshop on approaches and applications of inductive programming aaip held in conjunction with icml  identified all applications where learning of programs or recursive rules are called for
first in the domain of software engineering where structural learning software assistants and software agents can help to relieve programmers from routine tasks give programming support for end users or support of novice programmers and programming tutor systems
further areas of application are language learning learning recursive control rules for aiplanning learning recursive concepts in webmining or for dataformat transformations
since then these and many other areas have shown to be successful application niches for inductive programming such as enduser programming the related areas of programming by example and programming by demonstration and intelligent tutoring systems
other areas where inductive inference has been recently applied are knowledge acquisition artificial general intelligence reinforcement learning and theory evaluation and cognitive science in general
there may also be prospective applications in intelligent agents games robotics personalisation ambient intelligence and human interfaces
grammar induction or grammatical inference is the process in machine learning of learning a formal grammar usually as a collection of rewrite rules or productions or alternatively as a finite state machine or automaton of some kind from a set of observations thus constructing a model which accounts for the characteristics of the observed objects
more generally grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings trees and graphs
grammatical inference has often been very focused on the problem of learning finite state machines of various types see the article induction of regular languages for details on these approaches since there have been efficient algorithms for this problem since the s
since the beginning of the century these approaches have been extended to the problem of inference of contextfree grammars and richer formalisms such as multiple contextfree grammars and parallel multiple contextfree grammars
other classes of grammars for which grammatical inference has been studied are contextual grammars and pattern languages
the simplest form of learning is where the learning algorithm merely receives a set of examples drawn from the language in question the aim is to learn the language from examples of it and rarely from counterexamples that is example that do not belong to the language
however other learning models have been studied
one frequently studied alternative is the case where the learner can ask membership queries as in the exact query learning model or minimally adequate teacher model introduced by angluin
there is a wide variety of methods for grammatical inference
two of the classic sources are and
also devote a brief section to the problem and cite a number of references
the basic trialanderror method they present is discussed below
for approaches to infer subclasses of regular languages in particular see induction of regular languages
a more recent textbook is de la higuera  which covers the theory of grammatical inference of regular languages and finite state automata
dulizia ferri and grifoni provide a survey that explores grammatical inference methods for natural languages
the method proposed in section
of suggests successively guessing grammar rules productions and testing them against positive and negative observations
the rule set is expanded so as to be able to generate each positive example but if a given rule set also generates a negative example it must be discarded
this particular approach can be characterized as hypothesis testing and bears some similarity to mitchels version space algorithm
the text provide a simple example which nicely illustrates the process but the feasibility of such an unguided trialanderror approach for more substantial problems is dubious
grammatical induction using evolutionary algorithms is the process of evolving a representation of the grammar of a target language through some evolutionary process
formal grammars can easily be represented as tree structures of production rules that can be subjected to evolutionary operators
algorithms of this sort stem from the genetic programming paradigm pioneered by john koza
other early work on simple formal languages used the binary string representation of genetic algorithms but the inherently hierarchical structure of grammars couched in the ebnf language made trees a more flexible approach
koza represented lisp programs as trees
he was able to find analogues to the genetic operators within the standard set of tree operators
for example swapping subtrees is equivalent to the corresponding process of genetic crossover where substrings of a genetic code are transplanted into an individual of the next generation
fitness is measured by scoring the output from the functions of the lisp code
similar analogues between the tree structured lisp representation and the representation of grammars as trees made the application of genetic programming techniques possible for grammar induction
in the case of grammar induction the transplantation of subtrees corresponds to the swapping of production rules that enable the parsing of phrases from some language
the fitness operator for the grammar is based upon some measure of how well it performed in parsing some group of sentences from the target language
in a tree representation of a grammar a terminal symbol of a production rule corresponds to a leaf node of the tree
its parent nodes corresponds to a nonterminal symbol e
a noun phrase or a verb phrase in the rule set
ultimately the root node might correspond to a sentence nonterminal
like all greedy algorithms greedy grammar inference algorithms make in iterative manner decisions that seem to be the best at that stage
the decisions made usually deal with things like the creation of new rules the removal of existing rules the choice of a rule to be applied or the merging of some existing rules
because there are several ways to define the stage and the best there are also several greedy grammar inference algorithms
these contextfree grammar generating algorithms make the decision after every read symbolthese contextfree grammar generating algorithms first read the whole given symbolsequence and then start to make decisionsa more recent approach is based on distributional learning
algorithms using these approaches have been applied to learning contextfree grammars and mildly contextsensitive languages and have been proven to be correct and efficient for large subclasses of these grammars
angluin defines a pattern to be a string of constant symbols from  and variable symbols from a disjoint set
the language of such a pattern is the set of all its nonempty ground instances i
all strings resulting from consistent replacement of its variable symbols by nonempty strings of constant symbols
a pattern is called descriptive for a finite input set of strings if its language is minimal with respect to set inclusion among all pattern languages subsuming the input set
angluin gives a polynomial algorithm to compute for a given input string set all descriptive patterns in one variable x
to this end she builds an automaton representing all possibly relevant patterns using sophisticated arguments about word lengths which rely on x being the only variable the state count can be drastically reduced
erlebach et al
give a more efficient version of angluins pattern learning algorithm as well as a parallelized version
arimura et al
show that a language class obtained from limited unions of patterns can be learned in polynomial time
pattern theory formulated by ulf grenander is a mathematical formalism to describe knowledge of the world as patterns
it differs from other approaches to artificial intelligence in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns rather it prescribes a vocabulary to articulate and recast the pattern concepts in precise language
in addition to the new algebraic vocabulary its statistical approach was novel in its aim tobroad in its mathematical coverage pattern theory spans algebra and statistics as well as local topological and global entropic properties
the principle of grammar induction has been applied to other aspects of natural language processing and has been applied among many other problems to natural language understanding examplebased translation morpheme analysis and place name derivations
grammar induction has also been used for lossless data compression and statistical inference via minimum message length mml and minimum description length mdl principles
grammar induction has also been used in some probabilistic models of language acquisition
action model learning sometimes abbreviated action learning is an area of machine learning concerned with creation and modification of software agents knowledge about effects and preconditions of the actions that can be executed within its environment
this knowledge is usually represented in logicbased action description language and used as the input for automated planners
learning action models is important when goals change
when an agent acted for a while it can use its accumulated knowledge about actions in the domain to make better decisions
thus learning action models differs from reinforcement learning
it enables reasoning about actions instead of expensive trials in the world
action model learning is a form of inductive reasoning where new knowledge is generated based on agents observations
it differs from standard supervised learning in that correct inputoutput pairs are never presented nor imprecise action models explicitly corrected
usual motivation for action model learning is the fact that manual specification of action models for planners is often a difficult time consuming and errorprone task especially in complex environments
given a training set formula consisting of examples formula where formula are observations of a world state from two consecutive time steps formula and formula is an action instance observed in time step formula the goal of action model learning in general is to construct an action model formula where formula is a description of domain dynamics in action description formalism like strips adl or pddl and formula is a probability function defined over the elements of formula
however many state of the art action learning methods assume determinism and do not induce formula
in addition to determinism individual methods differ in how they deal with other attributes of domain e
partial observability or sensoric noise
recent action learning methods take various approaches and employ a wide variety of tools from different areas of artificial intelligence and computational logic
as an example of a method based on propositional logic we can mention slaf simultaneous learning and filtering algorithm which uses agents observations to construct a long propositional formula over time and subsequently interprets it using a satisfiability sat solver
another technique in which learning is converted into a satisfiability problem weighted maxsat in this case and sat solvers are used is implemented in arms actionrelation modeling system
two mutually similar fully declarative approaches to action learning were based on logic programming paradigm answer set programming asp and its extension reactive asp
in another example bottomup inductive logic programming approach was employed
several different solutions are not directly logicbased
for example the action model learning using a perceptron algorithm or the multi level greedy search over the space ofpossible action models
in the older paper from  the action model learning was studied as an extension of reinforcement learning
most action learning research papers are published in journals and conferences focused on artificial intelligence in general e
journal of artificial intelligence research jair artificial intelligence applied artificial intelligence aai or aaai conferences
despite mutual relevance of the topics action model learning is usually not addressed on planning conferences like icaps
in the regulation of algorithms particularly artificial intelligence and its subfield of machine learning a right to explanation or right to an explanation is a right to be given an explanation for an output of the algorithm
such rights primarily refer to individual rights to be given an explanation for decisions that significantly affect an individual particularly legally or financially
for example a person who applies for a loan and is denied may ask for an explanation which could be credit bureau x reports that you declared bankruptcy last year this is the main factor in considering you too likely to default and thus we will not give you the loan you applied for
some such legal rights already exist while the scope of a general right to explanation is a matter of ongoing debate
credit score in the united states  more generally credit actions  have a wellestablished right to explanation
under the equal credit opportunity act regulation b of the code of federal regulationstitle  chapter x part
creditors are required to notify applicants of action taken in certain circumstances and such notifications must provide specific reasons as detailed in
bthe official interpretation of this section details what types of statements are acceptable
credit agencies and data analysis firms such as fico comply with this regulation by providing a list of reasons generally at most  per interpretation of regulations consisting of a numeric as identifier and an associated explanation identifying the main factors affecting a credit score
an example might bethe european union general data protection regulation enacted  taking effect  extends the automated decisionmaking rights in the  data protection directive to provide a legally disputed form of a right to an explanation stated as such in recital  the data subject should have the right
to obtain an explanation of the decision reached
in fullhowever the extent to which the regulations themselves provide a right to explanation is heavily debated
there are two main strands of criticism
there are significant legal issues with the right as found in article   as recitals are not binding and the right to an explanation is not mentioned in the binding articles of the text having been removed during the legislative process
in addition there are significant restrictions on the types of automated decisions that are covered  which must be both solely based on automated processing and have legal or similarly significant effect  which limits their applicability in many of the cases of algorithmic controversy that have been picked up in the media
a second source of such a right has been pointed to in article  the right of access by the data subject
this restates a similar provision from the  data protection directive allowing the data subject access to meaningful information about the logic involved in the same significant solely automated decisionmaking found in article
yet this too suffers from alleged challenges that relate to the timing of when this right can be drawn upon as well as practical challenges that mean it may not be binding in many cases of public concern
in france the  loi pour une rpublique numrique digital republic act or loi numrique amends the countrys administrative code to introduce a new provision for the explanation of decisions made by public sector bodies about individuals
it notes that where there is a decision taken on the basis of an algorithmic treatment the rules that define that treatment and its principal characteristics must be communicated to the citizen upon request where there is not an exclusion e
for national security or defence
these should include the followingscholars have noted that this right while limited to administrative decisions goes beyond the gdpr right to explicitly apply to decision support rather than decisions solely based on automated processing as well as provides a framework for explaining specific decisions
indeed the gdpr automated decisionmaking rights in the european union one of the places a right to an explanation has been sought within find their origins in french law in the late s
some argue that a right to explanation is at best unnecessary at worst harmful and threatens to stifle innovation
specific criticisms include favoring human decisions over machine decisions being redundant with existing laws and focusing on process over outcome
more fundamentally many algorithms used in machine learning are not easily explainable
for example the output of a deep neural network depends on many layers of computations connected in a complex way and no one input or computation may be a dominant factor
the field of explainable ai seeks to provide better explanations from existing algorithms and algorithms that are more easily explainable but it is a young and active field
similarly human decisions often cannot be easily explained they may be based on intuition or a gut feeling that is hard to put into words
requiring machines to meet a higher standard than humans is thus arguably unreasonable
category utility is a measure of category goodness defined in and
it attempts to maximize both the probability that two objects in the same category have attribute values in common and the probability that objects from different categories have different attribute values
it was intended to supersede more limited measures of category goodness such as cue validity   and collocation index
it provides a normative informationtheoretic measure of the predictive advantage gained by the observer who possesses knowledge of the given category structure i
the class labels of instances over the observer who does not possess knowledge of the category structure
in this sense the motivation for the category utility measure is similar to the information gain metric used in decision tree learning
in certain presentations it is also formally equivalent to the mutual information as discussed below
a review of category utility in its probabilistic incarnation with applications to machine learning is provided in
the probabilitytheoretic definition of category utility given in and is as followswhere formula is a sizeformula set of formulaary features and formula is a set of formula categories
the term formula designates the marginal probability that feature formula takes on value formula and the term formula designates the categoryconditional probability that feature formula takes on value formula given that the object in question belongs to category formula
the motivation and development of this expression for category utility and the role of the multiplicand formula as a crude overfitting control is given in the above sources
loosely  the term formula is the expected number of attribute values that can be correctly guessed by an observer using a probabilitymatching strategy together with knowledge of the category labels while formula is the expected number of attribute values that can be correctly guessed by an observer the same strategy but without any knowledge of the category labels
their difference therefore reflects the relative advantage accruing to the observer by having knowledge of the category structure
the informationtheoretic definition of category utility for a set of entities with sizeformula binary feature set formula and a binary category formula is given in as followswhere formula is the prior probability of an entity belonging to the positive category formula in the absence of any feature information formula is the conditional probability of an entity having feature formula given that the entity belongs to category formula formula is likewise the conditional probability of an entity having feature formula given that the entity belongs to category formula and formula is the prior probability of an entity possessing feature formula in the absence of any category information
the intuition behind the above expression is as follows the term formula represents the cost in bits of optimally encoding or transmitting feature information when it known that the objects to be described belong to category formula
similarly the term formula represents the cost in bits of optimally encoding or transmitting feature information when it known that the objects to be described belong to category formula
the sum of these two terms in the brackets is therefore the weighted average of these two costs
the final term formula represents the cost in bits of optimally encoding or transmitting feature information when no category information is available
the value of the category utility will in the above formulation be negative
it is mentioned in and that the category utility is equivalent to the mutual information
here we provide a simple demonstration of the nature of this equivalence
let us assume a set of entities each having the same formula features i
feature set formula with each feature variable having cardinality formula
that is each feature has the capacity to adopt any of formula distinct values which need not be ordered all variables can be nominal for the special case formula these features would be considered binary but more generally for any formula the features are simply mary
for our purposes without loss of generality we can replace feature set formula with a single aggregate variable formula that has cardinality formula and adopts a unique value formula corresponding to each feature combination in the cartesian product formula
ordinality does not matter because the mutual information is not sensitive to ordinality
in what follows a term such as formula or simply formula refers to the probability with which formula adopts the particular value formula
using the aggregate feature variable formula replaces multiple summations and simplifies the presentation to follow
we assume also a single category variable formula which has cardinality formula
this is equivalent to a classification system in which there are formula nonintersecting categories
in the special case of formula we have the twocategory case discussed above
from the definition of mutual information for discrete variables the mutual information formula between the aggregate feature variable formula and the category variable formula is given bywhere formula is the prior probability of feature variable formula adopting value formula formula is the marginal probability of category variable formula adopting value formula and formula is the joint probability of variables formula and formula simultaneously adopting those respective values
in terms of the conditional probabilities this can be rewritten or defined asif we will rewrite the original definition of the category utility from above with formula we havethis equation clearly has the same form as the blue equation expressing the mutual information between the feature set and the category variable the difference is that the sum formula in the category utility equation runs over independent binary variables formula whereas the sum formula in the mutual information runs over values of the single formulaary variable formula
the two measures are actually equivalent then only when the features formula are independent and assuming that terms in the sum corresponding to formula are also added
like the mutual information the category utility is not sensitive to any ordering in the feature or category variable values
that is as far as the category utility is concerned the category set codice is not qualitatively different from the category set codice since the formulation of the category utility does not account for any ordering of the class variable
similarly a feature variable adopting values codice is not qualitatively different from a feature variable adopting values codice
as far as the category utility or mutual information are concerned all category and feature variables are nominal variables
for this reason category utility does not reflect any gestalt aspects of category goodness that might be based on such ordering effects
one possible adjustment for this insensitivity to ordinality is given by the weighting scheme described in the article for mutual information
this section provides some background on the origins of and need for formal measures of category goodness such as the category utility and some of the history that lead to the development of this particular metric
at least since the time of aristotle there has been a tremendous fascination in philosophy with the nature of concepts and universals
what kind of entity is a concept such as horse such abstractions do not designate any particular individual in the world and yet we can scarcely imagine being able to comprehend the world without their use
does the concept horse therefore have an independent existence outside of the mind if it does then what is the locus of this independent existence the question of locus was an important issue on which the classical schools of plato and aristotle famously differed
however they remained in agreement that universals did indeed have a mindindependent existence
there was therefore always a fact to the matter about which concepts and universals exist in the world
in the late middle ages perhaps beginning with occam although porphyry also makes a much earlier remark indicating a certain discomfort with the status quo however the certainty that existed on this issue began to erode and it became acceptable among the socalled nominalists and empiricists to consider concepts and universals as strictly mental entities or conventions of language
on this view of conceptsthat they are purely representational constructsa new question then comes to the fore why do we possess one set of concepts rather than another what makes one set of concepts good and another set of concepts bad this is a question that modern philosophers and subsequently machine learning theorists and cognitive scientists have struggled with for many decades
one approach to answering such questions is to investigate the role or purpose of concepts in cognition
thus we ask what are concepts good for in the first place the answer provided by and many others is that classification conception is a precursor to induction by imposing a particular categorization on the universe an organism gains the ability to deal with physically nonidentical objects or situations in an identical fashion thereby gaining substantial predictive leverage
as j
mill puts it from this base mill reaches the following conclusion which foreshadows much subsequent thinking about category goodness including the notion of category utilityone may compare this to the category utility hypothesis proposed by  a category is useful to the extent that it can be expected to improve the ability of a person to accurately predict the features of instances of that category
mill here seems to be suggesting that the best category structure is one in which object features properties are maximally informative about the objects class and simultaneously the object class is maximally informative about the objects features
in other words a useful classification scheme is one in which we can use category knowledge to accurately infer object properties and we can use property knowledge to accurately infer object classes
one may also compare this idea to aristotles criterion of counterpredication for definitional predicates as well as to the notion of concepts described in formal concept analysis
a variety of different measures have been suggested with an aim of formally capturing this notion of category goodness the best known of which is probably the cue validity
cue validity of a feature formula with respect to category formula is defined as the conditional probability of the category given the feature  formula or as the deviation of the conditional probability from the category base rate  formula
clearly these measures quantify only inference from feature to category i
cue validity but not from category to feature i
the category validity formula
also while the cue validity was originally intended to account for the demonstrable appearance of basic categories in human cognitioncategories of a particular level of generality that are evidently preferred by human learnersa number of major flaws in the cue validity quickly emerged in this regard  and others
one attempt to address both problems by simultaneously maximizing both feature validity and category validity was made by in defining the collocation index as the product formula but this construction was fairly ad hoc see
the category utility was introduced as a more sophisticated refinement of the cue validity which attempts to more rigorously quantify the full inferential power of a class structure
as shown above on a certain view the category utility is equivalent to the mutual information between the feature variable and the category variable
it has been suggested that categories having the greatest overall category utility are those that are not only those best in a normative sense but also those human learners prefer to use e
basic categories
other related measures of category goodness are cohesion  and salience
in machine learning instancebased learning sometimes called memorybased learning is a family of learning algorithms that instead of performing explicit generalization compares new problem instances with instances seen in training which have been stored in memory
it is called instancebased because it constructs hypotheses directly from the training instances themselves
this means that the hypothesis complexity can grow with the data in the worst case a hypothesis is a list of n training items and the computational complexity of classifying a single new instance is on
one advantage that instancebased learning has over other methods of machine learning is its ability to adapt its model to previously unseen data
instancebased learners may simply store a new instance or throw an old instance away
examples of instancebased learning algorithm are the knearest neighbor algorithm kernel machines and rbf networks
these store a subset of their training set when predicting a valueclass for a new instance they compute distances or similarities between this instance and the training instances to make a decision
to battle the memory complexity of storing all training instances as well as the risk of overfitting to noise in the training set instance reduction algorithms have been proposed
gagliardi applies this family of classifiers in medical field as secondopinion diagnostic tools and as tools for the knowledge extraction phase in the process of knowledge discovery in databases
one of these classifiers called prototype exemplar learning classifier pelc is able to extract a mixture of abstracted prototypical cases that are syndromes and selected atypical clinical cases
the center for biological  computational learning is a research lab at the massachusetts institute of technology
cbcl was established in  with support from the national science foundation
it is based in the department of brain  cognitive sciences at mit and is associated with the mcgovern institute for brain research and the mit computer science and artificial intelligence laboratory
it was founded with the belief that learning is at the very core of the problem of intelligence both biological and artificial
learning is thus the gateway to understanding how the human brain works and for making intelligent machines
cbcl studies the problem of learning within a multidisciplinary approach
its main goal is to nurture serious research on the mathematics the engineering and the neuroscience of learning
research is focused on the problem of learning in theory engineering applications and neuroscience
in computational neuroscience the center has developed a model of the ventral stream in the visual cortex which accounts for much of the physiological data and psychophysical experiments in difficult object recognition tasks
the model performs at the level of the best computer vision systems
tomaso poggio director of cbcl
bing predicts is a prediction engine developed by microsoft that uses machine learning from data on trending social media topics and sentiment towards those topics along with trending searches on bing
it predicts the outcomes of political elections popular reality shows and major sporting events
predictions can be accessed through the bing search engine
the idea for a prediction engine was first suggested by walter sun development manager for the core ranking team at bing when he noticed that school districts were more frequently searched before a major weather event in the area was forecasted because searchers wanted to find out if a closing or delay was caused
he concluded that the time and location of major weather events could accurately be predicted without referring to a weather forecast by observing major increases in search frequency of school districts in the area
this inspired bing to use its search data to infer outcomes of certain events such as winners of reality shows
bing predicts launched on april
the first reality shows to be featured on bing predicts were the voice american idol and dancing with the stars
the prediction accuracy for bing predicts is  for american idol and  for the voice
bing predicts also predicts the outcomes of major political elections in the united states
bing predicts had  accuracy for the  united states senate elections  accuracy for the  united states house of representatives elections and an  accuracy for the  united states gubernatorial elections
bing predicts is also making predictions for the results of the  united states presidential primaries
it has also done predictions in sports including a perfect  for  in the  world cup leading to positive press such as a business insider story on its successes and a pc world article on how microsoft ceo satya nadella did well in his march madness bracket entry
in the fields of machine learning the theory of computation and random matrix theory a probability distribution over vectors is said to be in isotropic position if its covariance matrix is equal to the identity matrix
let formula be a distribution over vectors in the vector space formula
then formula is in isotropic position if for vector formula sampled from the distributiona set of vectors is said to be in isotropic position if the uniform distribution over that set is in isotropic position
in particular every orthonormal set of vectors is isotropic
as a related definition a convex body formula in formula is called isotropic if it has volume formula center of mass at the origin and there is a constant formula such thatfor all vectors formula in formula here formula stands for the standard euclidean norm
in theoretical computer science a pattern language is a formal language that can be defined as the set of all particular instances of a string of constants and variables
pattern languages were introduced by dana angluin in the context of machine learning
given a finite set  of constant symbols and a countable set x of variable symbols disjoint from  a pattern is a finite nonempty string of symbols from x
the length of a pattern p denoted by p is just the number of its symbols
the set of all patterns containing exactly n distinct variables each of which may occur several times is denoted by p the set of all patterns at all by p
a substitution is a mapping f p  p such thatif p  fq for some patterns p q  p and some substitution f then p is said to be less general than q written pqin that case necessarily p  q holds
for a pattern p its language is defined as the set of all less general patterns that are built from constants only formally lp   s    s  p  where  denotes the set of all finite nonempty strings of symbols from
for example using the constants       and the variables x   x y z
the pattern xxx p and xxy p has length  and  respectively
an instance of the former pattern is zzz and zzz it is obtained by the substitution that maps x to z and to z respectively and each other symbol to itself
both zzz and zzz are also instances of xxy
in fact lxxx is a subset of lxxy
the language of the pattern x and x is the set of all bit strings which denote an even and odd binary number respectively
the language of xx is the set of all strings obtainable by concatenating a bit string with itself e
lxx
the problem of deciding whether s  lp for an arbitrary string s   and pattern p is npcomplete see pictureand so is hence the problem of deciding p  q for arbitrary patterns p q
the class of pattern languages is not closed under
the class of pattern languages is closed under
if p q  p are patterns containing exactly one variable then p  q if and only if lp  lqthe same equivalence holds for patterns of equal length
for patterns of different length the above example p  xxx and q  xxy shows that lp  lq may hold without implying p  q
however any two patterns p and q of arbitrary lengths generate the same language if and only if they are equal up to consistent variable renaming
each pattern p is a common generalization of all strings in its generated language lp modulo associativity of
in a refined chomsky hierarchy the class of pattern languages is a proper superclass and subclass of the singleton and the indexed languages respectively but incomparable to the language classes in between due to the latter the pattern language class is not explicitly shown in the table below
the class of pattern languages is incomparable with the class of finite languages with the class of regular languages and with the class of contextfree languageseach singleton language is trivially a pattern language generated by a pattern without variables
each pattern language can be produced by an indexed grammarfor example using    a b c  and x   x y the pattern a x b y c x a y b is generated by a grammar with nonterminal symbols n   s s s   x terminal symbols t   index symbols f   a b c a b c  start symbol s and the following production rulesan example derivation is
in a similar way an index grammar can be constructed from any pattern
given a sample set s of strings a pattern p is called descriptive of s if s  lp but not s  lq  lp for any other pattern q
given any sample set s a descriptive pattern for s can be computed by based on this algorithm the class of pattern languages can be identified in the limit from positive examples
in computational learning theory in mathematics given a class of concepts c a subclass d is reachable if there exists a partial approximation s of some concept such that d contains exactly those concepts in c that are extensions to s i
dcs
in the field of machine learning and specifically the problem of statistical classification a confusion matrix also known as an error matrix is a specific table layout that allows visualization of the performance of an algorithm typically a supervised learning one in unsupervised learning it is usually called a matching matrix
each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class or vice versa
the name stems from the fact that it makes it easy to see if the system is confusing two classes i
commonly mislabeling one as another
it is a special kind of contingency table with two dimensions actual and predicted and identical sets of classes in both dimensions each combination of dimension and class is a variable in the contingency table
if a classification system has been trained to distinguish between cats dogs and rabbits a confusion matrix will summarize the results of testing the algorithm for further inspection
assuming a sample of  animals   cats  dogs and  rabbits the resulting confusion matrix could look like the table belowin predictive analytics a table of confusion sometimes also called a confusion matrix is a table with two rows and two columns that reports the number of false positives false negatives true positives and true negatives
this allows more detailed analysis than mere proportion of correct classifications accuracy
accuracy is not a reliable metric for the real performance of a classifier because it will yield misleading results if the data set is unbalanced that is when the numbers of observations in different classes vary greatly
for example if there were  cats and only  dogs in the data set a particular classifier might classify all the observations as cats
the overall accuracy would be  but in more detail the classifier would have a  recognition rate sensitivity for the cat class but a  recognition rate for the dog class
f score is even more unreliable in such cases and here would yield over
whereas informedness removes such bias and yields  as the probability of an informed decision for any form of guessing here alway guessing cat
assuming the confusion matrix above its corresponding table of confusion for the cat class would bethe final table of confusion would contain the average values for all classes combined
let us define an experiment from p positive instances and n negative instances for some condition
the four outcomes can be formulated in a  confusion matrix as follows
in probability theory the multiarmed bandit problem sometimes called the kref namedoi
aref or narmed bandit problem is a problem in which a fixed limited set of resources must be allocated between competing alternative choices in a way that maximizes their expected gain when each choices properties are only partially known at the time of allocation and may become better understood as time passes or by allocating resources to the choice
the name comes from imagining a gambler at a row of slot machines sometimes known as onearmed bandits who has to decide which machines to play how many times to play each machine and in which order to play them and whether to continue with the current machine or try a different machine
the multiarmed bandit problem also falls into the broad category of stochastic scheduling
in the problem each machine provides a random reward from a probability distribution specific to that machine
the objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls
the crucial tradeoff the gambler faces at each trial is between exploitation of the machine that has the highest expected payoff and exploration to get more information about the expected payoffs of the other machines
the tradeoff between exploration and exploitation is also faced in reinforcement learning
in practice multiarmed bandits have been used to model problems such as managing research projects in a large organization like a science foundation or a pharmaceutical company
in early versions of the problem the gambler begins with no initial knowledge about the machines
herbert robbins in  realizing the importance of the problem constructed convergent population selection strategies in some aspects of the sequential design of experiments
a theorem the gittins index first published by john c
gittins gives an optimal policy for maximizing the expected discounted reward
the multiarmed bandit problem models an agent that simultaneously attempts to acquire new knowledge called exploration and optimize his or her decisions based on existing knowledge called exploitation
the agent attempts to balance these competing tasks in order to maximize his total value over the period of time considered
there are many practical applications of the bandit model for examplein these practical examples the problem requires balancing reward maximization based on the knowledge already acquired with attempting new actions to further increase knowledge
this is known as the exploitation vs
exploration tradeoff in reinforcement learning
the model has also been used to control dynamic allocation of resources to different projects answering the question of which project to work on given uncertainty about the difficulty and payoff of each possibility
originally considered by allied scientists in world war ii it proved so intractable that according to peter whittle the problem was proposed to be dropped over germany so that german scientists could also waste their time on it
the version of the problem now commonly analyzed was formulated by herbert robbins in
the multiarmed bandit short bandit or mab can be seen as a set of real distributions formula each distribution being associated with the rewards delivered by one of the formula levers
let formula be the mean values associated with these reward distributions
the gambler iteratively plays one lever per round and observes the associated reward
the objective is to maximize the sum of the collected rewards
the horizon formula is the number of rounds that remain to be played
the bandit problem is formally equivalent to a onestate markov decision process
the regret formula after formula rounds is defined as the expected difference between the reward sum associated with an optimal strategy and the sum of the collected rewards formula where formula is the maximal reward mean formula and formula is the reward in round t
a zeroregret strategy is a strategy whose average regret per round formula tends to zero with probability  when the number of played rounds tends to infinity
intuitively zeroregret strategies are guaranteed to converge to a not necessarily unique optimal strategy if enough rounds are played
a common formulation is the binary multiarmed bandit or bernoulli multiarmed bandit which issues a reward of one with probability formula and otherwise a reward of zero
another formulation of the multiarmed bandit has each arm representing an independent markov machine
each time a particular arm is played the state of that machine advances to a new one chosen according to the markov state evolution probabilities
there is a reward depending on the current state of the machine
in a generalisation called the restless bandit problem the states of nonplayed arms can also evolve over time
there has also been discussion of systems where the number of choices about which arm to play increases over time
computer science researchers have studied multiarmed bandits under worstcase assumptions obtaining algorithms to minimize regret in both finite and infinite asymptotic time horizons for both stochastic and nonstochastic arm payoffs
a major breakthrough was the construction of optimal population selection strategies or policies that possess uniformly maximum convergence rate to the population with highest mean in the work described below
in the paper asymptotically efficient adaptive allocation rules lai and robbins following papers of robbins and his coworkers going back to robbins in the year  constructed convergent population selection policies that possess the fastest rate of convergence to the population with highest mean for the case that the population reward distributions are the oneparameter exponential family
then in katehakis and robbins simplifications of the policy and the main proof were given for the case of normal populations with known variances
the next notable progress was obtained by burnetas and katehakis in the paper optimal adaptive policies for sequential allocation problems where index based policies with uniformly maximum convergence rate were constructed under more general conditions that include the case in which the distributions of outcomes from each population depend on a vector of unknown parameters
burnetas and katehakis  also provided an explicit solution for the important case in which the distributions of outcomes follow arbitrary i
nonparametric discrete univariate distributions
later in optimal adaptive policies for markov decision processes burnetas and katehakis studied the much larger model of markov decision processes under partial information where the transition law andor the expected one period rewards may depend on unknown parameters
in this work the explicit form for a class of adaptive policies that possess uniformly maximum convergence rate properties for the total expected finite horizon reward were constructed under sufficient assumptions of finite stateaction spaces and irreducibility of the transition law
a main feature of these policies is that the choice of actions at each state and time period is based on indices that are inflations of the righthand side of the estimated average reward optimality equations
these inflations have recently been called the optimistic approach in the work of tewari and bartlett ortner filippi capp and garivier and honda and takemura
many strategies exist which provide an approximate solution to the bandit problem and can be put into the four broad categories detailed below
semiuniform strategies were the earliest and simplest strategies discovered to approximately solve the bandit problem
all those strategies have in common a greedy behavior where the best lever based on previous observations is always pulled except when a uniformly random action is taken
probability matching strategies reflect the idea that the number of pulls for a given lever should match its actual probability of being the optimal lever
probability matching strategies are also known as thompson sampling or bayesian bandits and are surprisingly easy to implement if you can sample from the posterior for the mean value of each alternative
probability matching strategies also admit solutions to socalled contextual bandit problems
pricing strategies establish a price for each lever
for example as illustrated with the poker algorithm the price can be the sum of the expected reward plus an estimation of extra future rewards that will gain through the additional knowledge
the lever of highest price is always pulled
these strategies minimize the assignment of any patient to an inferior arm physicians duty
in a typical case they minimize expected successes lost esl that is the expected number of favorable outcomes that were missed because of assignment to an arm later proved to be inferior
another version minimizes resources wasted on any inferior more expensive treatment
a particularly useful version of the multiarmed bandit is the contextual multiarmed bandit problem
in this problem in each iteration an agent has to choose between arms
before making the choice the agent sees a ddimensional feature vector context vectorassociated with the current iteration
the learner uses these context vectors along with the rewards of the arms played in the past to make the choice of the arm to play inthe current iteration
over time the learners aim is to collect enough information about how the context vectors and rewards relate to each other so that it can predict the next best arm to play by looking at the feature vectors
many strategies exist that provide an approximate solution to the contextual bandit problem and can be put into two broad categories detailed below
in practice there is usually a cost associated with the resource consumed by each action and the total cost is limited by a budget in many applications such as crowdsourcing and clinical trials
constrained contextual bandit ccb is such a model that considers both the time and budget constraints in a multiarmed bandit setting
badanidiyuru et al
first studied contextual bandits with budget constraints also referred to as resourceful contextual bandits and show that a formula regret is achievable
however their work focuses on a finite set of policies and the algorithm is computationally inefficient
a simple algorithm with logarithmic regret is proposed inanother variant of the multiarmed bandit problem is called the adversarial bandit first introduced by auer and cesabianchi
in this variant at each iteration an agent chooses an arm and an adversary simultaneously chooses the payoff structure for each arm
this is one of the strongest generalizations of the bandit problem as it removes all assumptions of the distribution and a solution to the adversarial bandit problem is a generalized solution to the more specific bandit problems
in the original specification and in the above variants the bandit problem is specified with a discrete and finite number of arms often indicated by the variable formula
in the infinite armed case introduced by agarwal  the arms are a continuous variable in formula dimensions
garivier and moulines derive some of the first results with respect to bandit problems where the underlying model can change during play
a number of algorithms were presented to deal with this case including discounted ucb and slidingwindow ucb
another work by burtini et al
introduces a weighted least squares thompson sampling approach wlsts which proves beneficial in both the known and unknown nonstationary cases
in the known nonstationary case the authors in produce an alternative solution a variant of ucb named adjusted upper confidence bound aucb which assumes a stochastic model and provide upperbounds of the regret
many variants of the problem have been proposed in recent years
the dueling bandit variant was introduced by yue et al
to model the explorationversusexploitation tradeoff for relative feedback
in this variant the gambler is allowed to pull two levers at the same time but they only get a binary feedback telling which lever provided the best reward
the difficulty of this problem stems from the fact that the gambler has no way of directly observing the reward of their actions
the earliest algorithms for this problem are interleavefiltering beatthemean
the relative feedback of dueling bandits can also lead to voting paradoxes
a solution is to take the condorcet winner as a reference
more recently researchers have generalized algorithms from traditional mab to dueling bandits relative upper confidence bounds rucb relative exponential weighing rex copeland confidence bounds ccb relative minimum empirical divergence rmed and double thompson sampling dts
the collaborative filtering bandits i
cofiba was introduced by li and karatzoglou and gentile sigir  where the classical collaborative filtering and contentbased filtering methods try to learn a static recommendation model given training data
these approaches are far from ideal in highly dynamic recommendation domains such as news recommendation and computational advertisement where the set of items and users is very fluid
in this work they investigate an adaptive clustering technique for content recommendation based on explorationexploitation strategies in contextual multiarmed bandit settings
their algorithm cofiba pronounced as coffee bar takes into account the collaborative effects that arise due to the interaction of the users with the items by dynamically grouping users based on the items under consideration and at the same time grouping items based on the similarity of the clusterings induced over the users
the resulting algorithm thus takes advantage of preference patterns in the data in a way akin to collaborative filtering methods
they provide an empirical analysis on mediumsize realworld datasets showing scalability and increased prediction performance as measured by clickthrough rate over stateoftheart methods for clustering bandits
they also provide a regret analysis within a standard linear stochastic noise setting
the combinatorial multiarmed bandit cmab problem arises when instead of a single discrete variable to choose from an agent needs to choose values for a set of variables
assuming each variable is discrete the number of possible choices per iteration is exponential in the number of variables
several cmab settings have been studied in the literature from settings where the variables are binary to more general setting where each variable can take an arbitrary set of values
in machine learning manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset
in many machine learning problems the data to be learned do not cover the entire input space
for example a facial recognition system may not need to classify any possible image but only the subset of images that contain faces
the technique of manifold learning assumes that the relevant subset of data comes from a manifold a mathematical structure with useful properties
the technique also assumes that the function to be learned is smooth data with different labels are not likely to be close together and so the labeling function should not change quickly in areas where there are likely to be many data points
because of this assumption a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not using an extension of the technique of tikhonov regularization
manifold regularization algorithms can extend supervised learning algorithms in semisupervised learning and transductive learning settings where unlabeled data are available
the technique has been used for applications including medical imaging geographical imaging and object recognition
manifold regularization is a type of regularization a family of techniques that reduces overfitting and ensures that a problem is wellposed by penalizing complex solutions
in particular manifold regularization extends the technique of tikhonov regularization as applied to reproducing kernel hilbert spaces rkhss
under standard tikhonov regularization on rkhss a learning algorithm attempts to learn a function formula from among a hypothesis space of functions formula
the hypothesis space is an rkhs meaning that it is associated with a kernel formula and so every candidate function formula has a norm formula which represents the complexity of the candidate function in the hypothesis space
when the algorithm considers a candidate function it takes its norm into account in order to penalize complex functions
formally given a set of labeled training data formula with formula and a loss function formula a learning algorithm using tikhonov regularization will attempt to solve the expressionwhere formula is a hyperparameter that controls how much the algorithm will prefer simpler functions to functions that fit the data better
manifold regularization adds a second regularization term the intrinsic regularizer to the ambient regularizer used in standard tikhonov regularization
under the manifold assumption in machine learning the data in question do not come from the entire input space formula but instead from a nonlinear manifold formula
the geometry of this manifold the intrinsic space is used to determine the regularization norm
there are many possible choices for formula
many natural choices involve the gradient on the manifold formula which can provide a measure of how smooth a target function is
a smooth function should change slowly where the input data are dense that is the gradient formula should be small where the marginal probability density formula the probability density of a randomly drawn data point appearing at formula is large
this gives one appropriate choice for the intrinsic regularizerin practice this norm cannot be computed directly because the marginal distribution formula is unknown but it can be estimated from the provided data
in particular if the distances between input points are interpreted as a graph then the laplacian matrix of the graph can help to estimate the marginal distribution
suppose that the input data include formula labeled examples pairs of an input formula and a label formula and formula unlabeled examples inputs without associated labels
define formula to be a matrix of edge weights for a graph where formula is a distance measure between the data points formula and formula
define formula to be a diagonal matrix with formula and formula to be the laplacian matrix formula
then as the number of data points formula increases formula converges to the laplacebeltrami operator formula which is the divergence of the gradient formula
then if formula is a vector of the values of formula at the data formula the intrinsic norm can be estimatedas the number of data points formula increases this empirical definition of formula converges to the definition when formula is known
using the weights formula and formula for the ambient and intrinsic regularizers the final expression to be solved becomesas with other kernel methods formula may be an infinitedimensional space so if the regularization expression cannot be solved explicitly it is impossible to search the entire space for a solution
instead a representer theorem shows that under certain conditions on the choice of the norm formula the optimal solution formula must be a linear combination of the kernel centered at each of the input points for some weights formulausing this result it is possible to search for the optimal solution formula by searching the finitedimensional space defined by the possible choices of formula
manifold regularization can extend a variety of algorithms that can be expressed using tikhonov regularization by choosing an appropriate loss function formula and hypothesis space formula
two commonly used examples are the families of support vector machines and regularized least squares algorithms
regularized least squares includes the ridge regression algorithm the related algorithms of lasso and elastic net regularization can be expressed as support vector machines
the extended versions of these algorithms are called laplacian regularized least squares abbreviated laprls and laplacian support vector machines lapsvm respectively
regularized least squares rls is a family of regression algorithms algorithms that predict a value formula for its inputs formula with the goal that the predicted values should be close to the true labels for the data
in particular rls is designed to minimize the mean squared error between the predicted values and the true labels subject to regularization
ridge regression is one form of rls in general rls is the same as ridge regression combined with the kernel method
the problem statement for rls results from choosing the loss function formula in tikhonov regularization to be the mean squared errorthanks to the representer theorem the solution can be written as a weighted sum of the kernel evaluated at the data pointsand solving for formula giveswhere formula is defined to be the kernel matrix with formula and formula is the vector of data labels
adding a laplacian term for manifold regularization gives the laplacian rls statementthe representer theorem for manifold regularization again givesand this yields an expression for the vector formula
letting formula be the kernel matrix as above formula be the vector of data labels and formula be the formula block matrix formulawith a solution oflaprls has been applied to problems including sensor networksmedical imagingobject detectionspectroscopydocument classificationdrugprotein interactionsand compressing images and videos
support vector machines svms are a family of algorithms often used for classifying data into two or more groups or classes
intuitively an svm draws a boundary between classes so that the closest labeled examples to the boundary are as far away as possible
this can be directly expressed as a linear program but it is also equivalent to tikhonov regularization with the hinge loss function formulaadding the intrinsic regularization term to this expression gives the lapsvm problem statementagain the representer theorem allows the solution to be expressed in terms of the kernel evaluated at the data pointsformula can be found by writing the problem as a linear program and solving the dual problem
again letting formula be the kernel matrix and formula be the block matrix formula the solution can be shown to bewhere formula is the solution to the dual problemand formula is defined bylapsvm has been applied to problems including geographical imagingmedical imagingface recognitionmachine maintenanceand braincomputer interfaces
the stochastic block model is a generative model for random graphs
this model tends to produce graphs containing communities subsets characterized by being connected with one another with particular edge densities
for example edges may be more common within communities than between communities
the stochastic block model is important in statistics machine learning and network science where it serves as a useful benchmark for the task of recovering community structure in graph data
the stochastic block model takes the following parametersthe edge set is then sampled at random as follows any two vertices formula and formula are connected by an edge with probability formula
if the probability matrix is a constant in the sense that formula for all formula then the result is the erdsrnyi model formula
this case is degeneratethe partition into communities becomes irrelevantbut it illustrates a close relationship to the erdsrnyi model
the planted partition model is the special case that the values of the probability matrix formula are a constant formula on the diagonal and another constant formula off the diagonal
thus two vertices within the same community share an edge with probability formula while two vertices in different communities share an edge with probability formula
sometimes it is this restricted model that is called the stochastic block model
the case where formula is called an assortative model while the case formula is called dissortative
returning to the general stochastic block model a model is called strongly assortative if formula whenever formula all diagonal entries dominate all offdiagonal entries
a model is called weakly assortative if formula whenever formula each diagonal entry is only required to dominate the rest of its own row and column
dissortative forms of this terminology exist by reversing all inequalities
algorithmic recovery is often easier against block models with assortative or dissortative conditions of this form
much of the literature on algorithmic community detection addresses three statistical tasks detection partial recovery and exact recovery
the goal of detection algorithms is simply to determine given a sampled graph whether the graph has latent community structure
more precisely a graph might be generated with some known prior probability from a known stochastic block model and otherwise from a similar erdosrenyi model
the algorithmic task is to correctly identify which of these two underlying models generated the graph
in partial recovery the goal is to approximately determine the latent partition into communities in the sense of finding a partition that is correlated with the true partition significantly better than a random guess
in exact recovery the goal is to recover the latent partition into communities exactly
the community sizes and probability matrix may be known or unknown
stochastic block models exhibit a sharp threshold effect reminiscent of percolation thresholds
suppose that we allow the size formula of the graph to grow keeping the community sizes in fixed proportions
if the probability matrix remains fixed tasks such as partial and exact recovery become feasible for all nondegenerate parameter settings
however if we scale down the probability matrix at a suitable rate as formula increases we observe a sharp phase transition for certain settings of the parameters it will become possible to achieve recovery with probability tending to  whereas on the opposite side of the parameter threshold the probability of recovery tends to  no matter what algorithm is used
for partial recovery the appropriate scaling is to take formula for fixed formula resulting in graphs of constant average degree
in the case of two equalsized communities in the assortative planted partition model with probability matrixformulapartial recovery is feasible with probability formula whenever formula whereas any estimator fails partial recovery with probability formula whenever formula
for exact recovery the appropriate scaling is to take formula resulting in graphs of logarithmic average degree
here a similar threshold exists for the assortative planted partition model with formula equalsized communities the threshold lies at formula
in fact the exact recovery threshold is known for the fully general stochastic block model
in principle exact recovery can be solved in its feasible range using maximum likelihood but this amounts to solving a constrained or regularized cut problem such as minimum bisection that is typically npcomplete
hence no known efficient algorithms will correctly compute the maximumlikelihood estimate in the worst case
however a wide variety of algorithms perform well in the average case and many highprobability performance guarantees have been proven for algorithms in both the partial and exact recovery settings
successful algorithms include spectral clustering of the vertices semidefinite programming and forms of belief propagation among others
several variants of the model exist
one minor tweak allocates vertices to communities randomly according to a categorical distribution rather than in a fixed partition
more significant variants include the censored block model and the mixedmembership block model
wordvec is a group of related models that are used to produce word embeddings
these models are shallow twolayer neural networks that are trained to reconstruct linguistic contexts of words
wordvec takes as its input a large corpus of text and produces a vector space typically of several hundred dimensions with each unique word in the corpus being assigned a corresponding vector in the space
word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space
wordvec was created by a team of researchers led by tomas mikolov at google
the algorithm has been subsequently analysed and explained by other researchers
embedding vectors created using the wordvec algorithm have many advantages compared to earlier algorithms such as latent semantic analysis
wordvec can utilize either of two model architectures to produce a distributed representation of words continuous bagofwords cbow or continuous skipgram
in the continuous bagofwords architecture the model predicts the current word from a window of surrounding context words
the order of context words does not influence prediction bagofwords assumption
in the continuous skipgram architecture the model uses the current word to predict the surrounding window of context words
the skipgram architecture weighs nearby context words more heavily than more distant context words
according to the authors note cbow is faster while skipgram is slower but does a better job for infrequent words
results of wordvec training can be sensitive to parametrization
the following are some important parameters in wordvec training
a wordvec model can be trained with hierarchical softmax andor negative sampling
to approximate the conditional loglikelihood a model seeks to maximize the hierarchical softmax method uses a huffman tree to reduce calculation
the negative sampling method on the other hand approaches the maximization problem by minimizing the loglikelihood of sampled negative instances
according to the authors hierarchical softmax works better for infrequent words while negative sampling works better for frequent words and better with low dimensional vectors
as training epochs increase hierarchical softmax stops being useful
high frequency words often provide little information
words with frequency above a certain threshold may be subsampled to increase training speed
quality of word embedding increases with higher dimensionality
but after reaching some point marginal gain will diminish
typically the dimensionality of the vectors is set to be between  and
the size of the context window determines how many words before and after a given word would be included as context words of the given word
according to the authors note the recommended value is  for skipgram and  for cbow
an extension of wordvec to construct embeddings from entire documents rather than the individual words has been proposed
this extension is called paragraphvec or docvec and has been implemented in the c python and javascala tools see below with the java and python versions also supporting inference of document embeddings on new unseen documents
an extension of word vectors for ngrams in biological sequences e
dna rna and proteins for bioinformatics applications have been proposed by asgari and mofrad
named biovectors biovec to refer to biological sequences in general with proteinvectors protvec for proteins aminoacid sequences and genevectors genevec for gene sequences this representation can be widely used in applications of machine learning in proteomics and genomics
the results suggest that biovectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns
an extension of word vectors for creating a dense vector representation of unstructured radiology reports has been proposed by banerjee et
al
one of the biggest challenges with wordvec is how to handle unknown or outofvocabulary oov words and morphologically similar words
this can particularly be an issue in domains like medicine where synonyms and related words can be used depending on the preferred style of radiologist and words may have been used infrequently in a large corpus
if the wordvec model has not encountered a particular word before it will be forced to use a random vector which is generally far from its ideal representation
iwe combines wordvec with a semantic dictionary mapping technique to tackle the major challenges of information extraction from clinical texts which include ambiguity of free text narrative style lexical variations use of ungrammatical and telegraphic phases arbitrary ordering of words and frequent appearance of abbreviations and acronyms
of particular interest the iwe model trained on the one institutional dataset successfully translated to a different institutional dataset which demonstrates good generalizability of the approach across institutions
the reasons for successful word embedding learning in the wordvec framework are poorly understood
goldberg and levy point out that the wordvec objective function causes words that occur in similar contexts to have similar embeddings as measured by cosine similarity and note that this is in line with j
firths distributional hypothesis
however they note that this explanation is very handwavy and argue that a more formal explanation would be preferable
levy et al
show that much of the superior performance of wordvec or similar embeddings in downstream tasks is not a result of the models per se but of the choice of specific hyperparameters
transferring these hyperparameters to more traditional approaches yields similar performances in downstream tasks
the word embedding approach is able to capture multiple different degrees of similarity between words
mikolov et al
found that semantic and syntactic patterns can be reproduced using vector arithmetic
patterns such as man is to woman as brother is to sister can be generated through algebraic operations on the vector representations of these words such that the vector representation of brother  man  woman produces a result which is closest to the vector representation of sister in the model
such relationships can be generated for a range of semantic relations such as countrycapital as well as syntactic relations e
present tensepast tensemikolov et al
develop an approach to assessing the quality of a wordvec model which draws on the semantic and syntactic patterns discussed above
they developed a set of  semantic relations and  syntactic relations which they use as a benchmark to test the accuracy of a model
when assessing the quality of a vector model a user may draw on this accuracy test which is implemented in wordvec or develop their own test set which is meaningful to the corpora which make up the model
this approach offers a more challenging test than simply arguing that the words most similar to a given test word are intuitively plausible
the use of different model parameters and different corpus sizes can greatly affect the quality of a wordvec model
accuracy can be improved in a number of ways including the choice of model architecture cbow or skipgram increasing the training data set increasing the number of vector dimensions and increasing the window size of words considered by the algorithm
each of these improvements comes with the cost of increased computational complexity and therefore increased model generation time
in models using large corpora and a high number of dimensions the skipgram model yields the highest overall accuracy and consistently produces the highest accuracy on semantic relationships as well as yielding the highest syntactic accuracy in most cases
however the cbow is less computationally expensive and yields similar accuracy results
accuracy increases overall as the number of words used increases and as the number of dimensions increases
mikolov et al
report that doubling the amount of training data results in an increase in computational complexity equivalent to doubling the number of vector dimensions
altszyler et al
studied wordvec performance in two semantic tests for different corpus size
they found that wordvec has a steep learning curve outperforming another wordembedding technique lsa when it is trained with medium to large corpus size more than  million words
however with a small training corpus lsa showed better performance
additionally they show that the best parameter setting depends on the task and the training corpus
nevertheless for skipgram models trained in medium size corpora with  dimensions a window size of  and  negative samples seems to be a good parameter setting
in statistical learning theory a learnable function class is a set of functions for which an algorithm can be devised to asymptotically minimize the expected risk uniformly over all probability distributions
the concept of learnable classes are closely related to regularization in machine learning and provides large sample justifications for certain learning algorithms
let formula be the sample space where formula are the labels and formula are the covariates predictors
formula is a collection of mappings functions under consideration to link formula to formula
formula is a pregiven loss function usually nonnegative
given a probability distribution formula on formula define the expected risk formula to bethe general goal in statistical learning is to find the function in formula that minimizes the expected risk
that is to find solutions to the following problembut in practice the distribution formula is unknown and any learning task can only be based on finite samples
thus we seek instead to find an algorithm that asymptotically minimizes the empirical risk i
to find a sequence of functions formula that satisfiesone usual algorithm to find such a sequence is through empirical risk minimization
we can make the condition given in the above equation stronger by requiring that the convergence is uniform for all probability distributions
that isthe intuition behind the more strict requirement is as such the rate at which sequence formula converges to the minimizer of the expected risk can be very different for different formula
because in real world the true distribution formula is always unknown we would want to select a sequence that performs well under all cases
however by the no free lunch theorem such a sequence that satisfies  does not exist if formula is too complex
this means we need to be careful and not allow too many functions in formula if we want  to be a meaningful requirement
specifically function classes that ensure the existence of a sequence formula that satisfies  are known as learnable classes
it is worth noting that at least for supervised classification and regression problems if a function class is learnable then the empirical risk minimization automatically satisfies
thus in these settings not only do we know that the problem posed by  is solvable we also immediately have an algorithm that gives the solution
if the true relationship between formula and formula is formula then by selecting the appropriate loss function formula can always be expressed as the minimizer of the expected loss across all possible functions
that ishere we let formula be the collection of all possible functions mapping formula onto formula
formula can be interpreted as the actual data generating mechanism
however the no free lunch theorem tells us that in practice with finite samples we cannot hope to search for the expected risk minimizer over formula
thus we often consider a subset of formula formula to carry out searches on
by doing so we risk that formula might not be an element of formula
this tradeoff can be mathematically expressed asin the above decomposition part formula does not depend on the data and is nonstochastic
it describes how far away our assumptions formula are from the truth formula
formula will be strictly greater than  if we make assumptions that are too strong formula too small
on the other hand failing to put enough restrictions on formula will cause it to be not learnable and part formula will not stochastically converge to
this is the wellknown overfitting problem in statistics and machine learning literature
a good example where learnable classes are used is the socalled tikhonov regularization in reproducing kernel hilbert space rkhs
specifically let formula be an rkhs and formula be the norm on formula given by its inner product
it is shown in that formula is a learnable class for any finite positive formula
the empirical minimization algorithm to the dual form of this problem isthis was first introduced by tikhonov to solve illposed problems
many statistical learning algorithms can be expressed in such a form for example the wellknown ridge regression
the tradeoff between formula and formula in  is geometrically more intuitive with tikhonov regularization in rkhs
we can consider a sequence of formula which are essentially balls in formula with centers at
as formula gets larger formula gets closer to the entire space and formula is likely to become smaller
however we will also suffer smaller convergence rates in formula
the way to choose an optimal formula in finite sample settings is usually through crossvalidation
part formula in  is closely linked to empirical process theory in statistics where the empirical risk formula are known as empirical processes
in this field the function class formula that satisfies the stochastic convergenceare known as uniform glivenkocantelli classes
it has been shown that under certain regularity conditions learnable classes and uniformly glivenkocantelli classes are equivalent
interplay between formula and formula in statistics literature is often known as the biasvariance tradeoff
however note that in the authors gave an example of stochastic convex optimization for general setting of learning where learnability is not equivalent with uniform convergence
the matthews correlation coefficient is used in machine learning as a measure of the quality of binary twoclass classifications introduced by biochemist brian w
matthews in
it takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes
the mcc is in essence a correlation coefficient between the observed and predicted binary classifications it returns a value between  and
a coefficient of  represents a perfect prediction  no better than random prediction and  indicates total disagreement between prediction and observation
the statistic is also known as the phi coefficient
mcc is related to the chisquare statistic for a  contingency tablewhere n is the total number of observations
while there is no perfect way of describing the confusion matrix of true and false positives and negatives by a single number the matthews correlation coefficient is generally regarded as being one of the best such measures
other measures such as the proportion of correct predictions also termed accuracy are not useful when the two classes are of very different sizes
for example assigning every object to the larger set achieves a high proportion of correct predictions but is not generally a useful classification
the mcc can be calculated directly from the confusion matrix using the formulain this equation tp is the number of true positives tn the number of true negatives fp the number of false positives and fn the number of false negatives
if any of the four sums in the denominator is zero the denominator can be arbitrarily set to one this results in a matthews correlation coefficient of zero which can be shown to be the correct limiting value
the original formula as given by matthews wasthis is equal to the formula given above
as a correlation coefficient the matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual
the component regression coefficients of the matthews correlation coefficient are markedness p and youdens j statistic informedness or p
markedness and informedness correspond to different directions of information flow and generalize youdens j statistic the formulap statistics and as their geometric mean the matthews correlation coefficient to more than two classes
some scientists claim the matthews correlation coefficient to be the most informative single score to establish the quality of a binary classifier prediction in a confusion matrix context
let us define an experiment from p positive instances and n negative instances for some condition
the four outcomes can be formulated in a  contingency table or confusion matrix as followsthe matthews correlation coefficient has been generalized to the multiclass case
this generalization was called the formula statistic for k different classes by the author and defined in terms of a formula confusion matrix formulawhen there are more than two labels the mcc will no longer range between  and
instead the minimum value will be between  and  depending on the true distribution
the maximum value is always
as explained by davide chicco is his paper ten quick tips for machine learning in computational biology biodata mining  the matthews correlation coefficient is more informative than other confusion matrix measures such as f score and accuracy in evaluating binary classification problems because it takes into account the balance ratios of the four confusion matrix categories true positives true negatives false positives false negatives
the paper explains for tip
spikeandslab regression is a bayesian variable selection technique that is particularly useful when the number of possible predictors is larger than the number of observations
initially the idea of the spikeandslab model was proposed by mitchell  beauchamp
the approach was further significantly developed by madigan  raftery  and george  mcculloch
the final adjustments to the model were done by ishwaran  rao
suppose we have p possible predictors in some model
vector  has a length equal to p and consists of zeros and ones
this vector indicates whether a particular variable is included in the regression or not
if no specific prior information on initial inclusion probabilities of particular variables is available a bernoulli prior distribution is a common default choice
conditional on a predictor being in the regression we identify a prior distribution for the model coefficient which corresponds to that variable
a common choice on that step is to use a normal prior with mean equal to zero and a large variance calculated based on formula where formula is a design matrix of explanatory variables of the model
a draw of  from its prior distribution is a list of the variables included in the regression
conditional on this set of selected variables we take a draw from the prior distribution of the regression coefficients if    then    and if    then
denotes the subset of  for which
in the next step we calculate a posterior probability distribution for both inclusion and coefficients by applying a standard statistical procedure
all steps of the described algorithm are repeated thousands of times using markov chain monte carlo mcmc technique
as a result we obtain a posterior distribution of  variable inclusion in the model  regression coefficient values and the corresponding prediction of y
the model got its name spikeandslab due to the shape of the two prior distributions
the spike is the probability of a particular coefficient in the model to be zero
the slab is the prior distribution for the regression coefficient values
an advantage of bayesian variable selection techniques is that they are able to make use of prior knowledge about the model
in the absence of such knowledge some reasonable default values can be used for the analyst who prefers simplicity at the cost of some reasonable assumptions useful prior information can be reduced to an expected model size an expected r and a sample size  determining the weight given to the guess at r
some researchers suggest the following default values r
and
parameter of a prior bernoulli distribution
a possible drawback of the spikeandslab model can be its mathematical complexity in comparison to linear regression
a deep understanding of this model requires sound knowledge in stochastic processes
on the other hand some modern statistical software e
r have readytouse solutions for calculating various bayesian variable selection models
in this case it would be enough for a researcher to know the idea of the method required model parameters and input variables
the analysis of the model outcomes distribution of   and corresponding predictions of y can be more challenging in comparison to linear regression case
the spikeandslab model produces inclusion probabilities for each of possible predictors
this can cause difficulties when comparing results to the studies with simple regression usually only regression coefficients with corresponding statistics are available
spikeandslab regression is a part of bayesian structural time series model which is used for feature selection time series forecasting nowcasting inferring causation and other
in machine learning lazy learning is a learning method in which generalization of the training data is delayed until a query is made to the system as opposed to in eager learning where the system tries to generalize the training data before receiving queries
the main advantage gained in employing a lazy learning method such as casebased reasoning is that the target function will be approximated locally such as in the knearest neighbor algorithm
because the target function is approximated locally for each query to the system lazy learning systems can simultaneously solve multiple problems and deal successfully with changes in the problem domain
the disadvantages with lazy learning include the large space requirement to store the entire training dataset
particularly noisy training data increases the case base unnecessarily because no abstraction is made during the training phase
another disadvantage is that lazy learning methods are usually slower to evaluate though this is coupled with a faster training phase
lazy classifiers are most useful for large datasets with few attributes
in artificial intelligence apprenticeship learning or learning from demonstration is the process of learning by observing an expert
it can be viewed as a form of supervised learning where the training dataset consists of task executions by a demonstration teacher
mapping methods try to mimic the expert by forming a direct mapping from the states to the actions
for example in  researchers used such an approach to teach an aibo robot basic soccer skills
system models try to mimic the expert by modeling world dynamics
inverse reinforcement learning irl is the process of deriving a reward function from observed behavior
while ordinary reinforcement learning involves using rewards and punishments to learn behavior in irl the direction is reversed and a robot observes a persons behavior to figure out what goal that behavior seems to be trying to achieve
the irl problem can be defined asgiven  measurements of an agents behaviour over time in a variety of circumstances  measurements of the sensory inputs to that agent  a model of the physical environment including the agents body determine the reward function that the agent is optimizing
irl researcher stuart j
russell proposes that irl might be used to observe humans and attempt to codify their complex ethical values in an effort to create ethical robots that might someday know not to cook your cat without needing to be explicitly told
the scenario can be modeled as a cooperative inverse reinforcement learning game where a person player and a robot player cooperate to secure the persons implicit goals despite these goals not being explicitly known by either the person nor the robot
in  openai and deepmind applied deep learning to the cooperative inverse reinforcement learning in simple domains such as atari games and straightforward robot tasks such as backflips
the human role was limited to answering queries from the robot as to which of two different actions were preferred
the researchers found evidence that the techniques may be economically scalable to modern systems
apprenticeship via inverse reinforcement learning airp was developed by in  pieter abbeel professor in berkeleys eecs department and andrew ng associate professor in stanford universitys computer science department
airp deals with markov decision process where we are not explicitly given a reward function but where instead we can observe an expert demonstrating the task that we want to learn to perform
airp has been used to model reward functions of highly dynamic scenarios where there is no obvious reward function intuitively
take the task of driving for example there are many different objectives working simultaneously  such as maintaining safe following distance a good speed not changing lanes too often etc
this task may seem easy at first glance but a trivial reward function may not converge to the policy wanted
one domain where airp has been used extensively is helicopter control
while simple trajectories can be intuitively derived complicated tasks like aerobatics for shows has been successful
these include aerobatic maneuvers like  inplace flips inplace rolls loops hurricanes and even autorotation landings
this work was developed by pieter abbeel adam coates and andrew ng  autonomous helicopter aerobatics through apprenticeship learningthe system learns rules to associate preconditions and postconditions with each action
in one  demonstration a humanoid learns a generalized plan from only two demonstrations of a repetitive ballcollection task
learning from demonstration is often explained from a perspective that the working robotcontrolsystem is available and the humandemonstrator is using it
and indeed if the software works the human operator takes the robotarm makes a move with it and the robot will reproduce the action later
for example he teaches the robotarm how to put a cup under a coffeemaker and press the startbutton
in the replay phase the robot is imitating this behavior
but that is not how the system works internally it is only what the audience can observe
in reality learning from demonstration is much more complex
for telling the story right we must go back in the year
in this time the robotics expert stefan schaal was working on the sarcos robotarm
the goal was simple solve the pendulum swingup task
the robot itself can execute a movement and as a result the pendulum is moving
the problem is that it is unclear what actions will result into which movement
it is an optimal controlproblem which can be described with mathematical formulas but is hard to solve
the idea from schaal was not to use a bruteforce solver but record the movements of a humandemonstration
the angle of the pendulum is logged over the timeperiod of  seconds at the yaxis
this results into a diagram which produces a pattern
in computer animation the principle is called spline animation
that means on the xaxis the time is given for example
seconds
seconds
seconds while on the yaxis is the variable given
in most cases its the position of an object
in the inverted pendulum it is the angle
the overall task consists of two parts recording the angle over time and reproducing the recorded motion
the reproducing step is surprisingly simple
as an input we know in which time step which angle the pendulum must have
bringing the system to a state is called tracking control or pid control
that means we have a trajectory over time and must find control actions to map the system to this trajectory
other authors call the principle steering behavior  because the aim is to bring a robot to a given line
documenting hate is a project of propublica in collaboration with a number of journalistic academic and computing organizations for systematic tracking of hate crimes and bias incidents
it uses an online form to facilitate reporting of incidents by the general public
since august  it has also used machine learning and natural language processing techniques to monitor and collect news stories about hate crimes and bias incidents
over  news organizations had joined the project
documenting hate was created in response to propublicas dissatisfaction with the quality of reporting and tracking of evidence of hate crimes and bias incidents after the united states presidential election of
the project was launched on  january  after the publication on  november  of a propublica news story about the difficulty of obtaining hard data on hate crimes
on  august  propublica and google announced the creation of the documenting hate news index which uses the google cloud natural language api for automated monitoring and collection of news stories about hate crimes and bias incidents
the api uses machine learning and natural language processing techniques
the findings of the index are integrated with reports from members of the public
the index is a joint project of propublica google news lab and the data visualization studio pitch interactive
thousands of incidents had been reported via documenting hate
over  news organizations had joined the project including the boston globe the new york times vox and the georgetown university hoya
an education reporter for the conservative daily caller has criticized the project for ambiguity in the terms it uses to describe hate crimes and for neglect of hatecrime hoaxes
another daily caller journalist has likewise criticized the documenting hate news index for underrepresentation of conservative outlets among the news sources it monitors
a policy analyst for the center for data innovation an affiliate of the information technology and innovation foundation while supporting propublicas critique of the present state of hatecrime statistics and praising propublica for drawing attention to the problem has argued that a nongovernmental project like documenting hate cannot solve it unaided instead intervention at the federal level is needed
product of experts poe is a machine learning technique
it models a probability distribution by combining the output from several simpler distributions
it was proposed by geoff hinton along with an algorithm for training the parameters of such a system
the core idea is to combine several probability distributions experts by multiplying their density functionsmaking the poe classification similar to an and operation
this allows each expert to make decisions on the basis of a few dimensions without having to cover the full dimensionality of a problem
this is related to but quite different from a mixture model where several probability distributions are combined via an or operation which is a weighted sum of their density functions
an inauthentic text is a computergenerated expository document meant to appear as genuine but which is actually meaningless
frequently they are created in order to be intermixed with genuine documents and thus manipulate the results of search engines as with spam blogs
they are also carried along in email in order to fool spam filters by giving the spam the superficial characteristics of legitimate text
sometimes nonsensical documents are created with computer assistance for humorous effect as with dissociated press or flarf poetry
they have also been used to challenge the veracity of a publicationmit students submitted papers generated by a computer program called scigen to a conference where they were initially accepted
this led the students to claim that the bar for submissions was too low
with the amount of computer generated text outpacing the ability of people to humans to curate it there needs some means of distinguishing between the two
yet automated approaches to determining absolutely whether a text is authentic or not face intrinsic challenges of semantics
noam chomsky coined the phrase colorless green ideas sleep furiously giving an example of grammaticallycorrect but semantically incoherent sentence some will point out that in certain contexts one could give this sentence or any phrase meaning
the first group to use the expression in this regard can be found below from indiana university
their work explains in detail an attempt to detect inauthentic texts and identify pernicious problems of inauthentic texts in cyberspace
the site has a means of submitting text that assesses based on supervised learning whether a corpus is inauthentic or not
many users have submitted incorrect types of data and have correspondingly commented on the scores
this application is meant for a specific kind of data therefore submitting say an email will not return a meaningful score
probability matching is a decision strategy in which predictions of class membership are proportional to the class base rates
thus if in the training set positive examples are observed  of the time and negative examples are observed  of the time then the observer using a probabilitymatching strategy will predict for unlabeled examples a class label of positive on  of instances and a class label of negative on  of instances
the optimal bayesian decision strategy to maximize the number of correct predictions see  in such a case is to always predict positive i
predict the majority category in the absence of other information which has  chance of winning rather than matching which has  of winning where p is the probability of positive realization the result of matching would be formula here formula
the probabilitymatching strategy is of psychological interest because it is frequently employed by human subjects in decision and classification studies where it may be related to thompson sampling
the evaluation of binary classifiers compares two methods of assigning a binary attribute one of which is usually a standard method and the other is being investigated
there are many metrics that can be used to measure the performance of a classifier or predictor different fields have different preferences for specific metrics due to different goals
for example in medicine sensitivity and specificity are often used while in computer science precision and recall are preferred
an important distinction is between metrics that are independent on the prevalence how often each category occurs in the population and metrics that depend on the prevalence  both types are useful but they have very different properties
given a data set a classification the output of a classifier on that set gives two numbers the number of positives and the number of negatives which add up to the total size of the set
to evaluate a classifier one compares its output to another reference classification  ideally a perfect classification but in practice the output of another gold standard test  and cross tabulates the data into a  contingency table comparing the two classifications
one then evaluates the classifier relative to the gold standard by computing summary statistics of these  numbers
generally these statistics will be scale invariant scaling all the numbers by the same factor does not change the output to make them independent of population size which is achieved by using ratios of homogeneous functions most simply homogeneous linear or homogeneous quadratic functions
say we test some people for the presence of a disease
some of these people have the disease and our test correctly says they are positive
they are called true positives tp
some have the disease but the test incorrectly claims they dont
they are called false negatives fn
some dont have the disease and the test says they dont  true negatives tn
finally there might be healthy people who have a positive test result  false positives fp
these can be arranged into a  contingency table confusion matrix conventionally with the test result on the vertical axis and the actual condition on the horizontal axis
these numbers can then be totaled yielding both a grand total and marginal totals
totaling the entire table the number of true positives false negatives true negatives and false positives add up to  of the set
totaling the rows adding horizontally the number of true positives and false positives add up to  of the test positives and likewise for negatives
totaling the columns adding vertically the number of true positives and false negatives add up to  of the condition positives conversely for negatives
the basic marginal ratio statistics are obtained by dividing the  values in the table by the marginal totals either rows or columns yielding  auxiliary  tables for a total of  ratios
these ratios come in  complementary pairs each pair summing to  and so each of these derived  tables can be summarized as a pair of  numbers together with their complements
further statistics can be obtained by taking ratios of these ratios ratios of ratios or more complicated functions
the contingency table and the most common derived ratios are summarized below see sequel for details
note that the columns correspond to the condition actually being positive or negative or classified as such by the gold standard as indicated by the colorcoding and the associated statistics are prevalenceindependent while the rows correspond to the test being positive or negative and the associated statistics are prevalencedependent
there are analogous likelihood ratios for prediction values but these are less commonly used and not depicted above
the fundamental prevalenceindependent statistics are sensitivity and specificity
sensitivity or true positive rate tpr also known as recall is the proportion of people that tested positive and are positive true positive tp of all the people that actually are positive condition positive cp  tp  fn
it can be seen as the probability that the test is positive given that the patient is sick
with higher sensitivity fewer actual cases of disease go undetected or in the case of the factory quality control fewer faulty products go to the market
specificity spc or true negative rate tnr is the proportion of people that tested negative and are negative true negative tn of all the people that actually are negative condition negative cn  tn  fp
as with sensitivity it can be looked at as the probability that the test result is negative given that the patient is not sick
with higher specificity fewer healthy people are labeled as sick or in the factory case fewer good products are discarded
the relationship between sensitivity and specificity as well as the performance of the classifier can be visualized and studied using the receiver operating characteristic roc curve
in theory sensitivity and specificity are independent in the sense that it is possible to achieve  in both such as in the redblue ball example given above
in more practical less contrived instances however there is usually a tradeoff such that they are inversely proportional to one another to some extent
this is because we rarely measure the actual thing we would like to classify rather we generally measure an indicator of the thing we would like to classify referred to as a surrogate marker
the reason why  is achievable in the ball example is because redness and blueness is determined by directly detecting redness and blueness
however indicators are sometimes compromised such as when nonindicators mimic indicators or when indicators are timedependent only becoming evident after a certain lag time
the following example of a pregnancy test will make use of such an indicator
modern pregnancy tests do not use the pregnancy itself to determine pregnancy status rather human chorionic gonadotropin is used or hcg present in the urine of gravid females as a surrogate marker to indicate that a woman is pregnant
because hcg can also be produced by a tumor the specificity of modern pregnancy tests cannot be  in that false positives are possible
also because hcg is present in the urine in such small concentrations after fertilization and early embryogenesis the sensitivity of modern pregnancy tests cannot be  because false negatives are possible
in addition to sensitivity and specificity the performance of a binary classification test can be measured with positive predictive value ppv also known as precision and negative predictive value npv
the positive prediction value answers the question if the test result is positive how well does that predict an actual presence of disease
it is calculated as tptp  fp that is it is the proportion of true positives out of all positive results
the negative prediction value is the same but for negatives naturally
prevalence has a significant impact on prediction values
as an example suppose there is a test for a disease with  sensitivity and  specificity
if  people are tested and the prevalence in the sample is   of them are sick and  of them are healthy
thus about  true positives and  true negatives are likely with  false positives and  false negatives
the positive and negative prediction values would be  so there can be high confidence in the result
however if the prevalence is only  so of the  people only  are really sick then the prediction values change significantly
the likely result is  true positives  false negative  true negatives and  false positives
of the  people tested positive only  really have the disease  that means intuitively that given that a patients test result is positive there is only  chance that they really have the disease
on the other hand given that the patients test result is negative there is only  chance in  or
probability that the patient has the disease despite the test result
there are various relationships between these ratios
if the prevalence sensitivity and specificity are known the positive predictive value can be obtained from the following identityif the prevalence sensitivity and specificity are known the negative predictive value can be obtained from the following identityin addition to the paired metrics there are also single metrics that give a single number to evaluate the test
perhaps the simplest statistic is accuracy or fraction correct fc which measures the fraction of all instances that are correctly categorized it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications tp  tntotal population  tp  tntp  tn  fp  fn
this is often not very useful compared to the marginal ratios as it does not yield useful marginal interpretations due to mixing true positives test positive condition positive and true negatives test negative condition negative  in terms of the condition table it sums the diagonal further it is prevalencedependent
the complement is the fraction incorrect fic fc  fic   or fp  fntp  tn  fp  fn  this is the sum of the antidiagonal divided by the total population
the diagnostic odds ratio dor is a more useful overall metric which can be defined directly as tptnfpfn  tpfnfptn or indirectly as a ratio of ratio of ratios ratio of likelihood ratios which are themselves ratios of true rates or prediction values
this has a useful interpretation  as an odds ratio  and is prevalenceindependent
an fscore is a combination of the precision and the recall providing a single score
there is a oneparameter family of statistics with parameter  which determines the relative weights of precision and recall
the traditional or balanced fscore f score is the harmonic mean of precision and recallnote however that the fscores do not take the true negative rate into account and are more suited to information retrieval and information extraction evaluation where the true negatives are innumerable
instead measures such as the phi coefficient matthews correlation coefficient informedness or cohens kappa may be preferable to assess the performance of a binary classifier
as a correlation coefficient the matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual
the component regression coefficients of the matthews correlation coefficient are markedness deltap and informedness youdens j statistic or deltap
parity learning is a problem in machine learning
an algorithm that solves this problem must find a function  given some samples xx and the assurance that  computes the parity of bits at some fixed locations
the samples are generated using some distribution over the input
the problem is easy to solve using gaussian elimination provided that a sufficient number of samples from a distribution which is not too skewed are provided to the algorithm
in this version the samples may contain some error
instead of samples xx the algorithm is provided with xy where yx with some small probability
the noisy version of the parity learning problem is conjectured to be hard
concept learning also known as category learning concept attainment and concept formation is defined by bruner goodnow  austin  as the search for and listing of attributes that can be used to distinguish exemplars from non exemplars of various categories
more simply put concepts are the mental categories that help us classify objects events or ideas building on the understanding that each object event or idea has a set of common relevant features
thus concept learning is a strategy which requires a learner to compare and contrast groups or categories that contain conceptrelevant features with groups or categories that do not contain conceptrelevant features
concept learning also refers to a learning task in which a human or machine learner is trained to classify objects by being shown a set of example objects along with their class labels
the learner simplifies what has been observed by condensing it in the form of an example
this simplified version of what has been learned is then applied to future examples
concept learning may be simple or complex because learning takes place over many areas
when a concept is difficult it is less likely that the learner will be able to simplify and therefore will be less likely to learn
colloquially the task is known as learning from examples
most theories of concept learning are based on the storage of exemplars and avoid summarization or overt abstraction of any kind
concept learning must be distinguished from learning by reciting something from memory recall or discriminating between two things that differ discrimination
however these issues are closely related since memory recall of facts could be considered a trivial conceptual process where prior exemplars representing the concept are invariant
similarly while discrimination is not the same as initial concept learning discrimination processes are involved in refining concepts by means of the repeated presentation of exemplars
concrete or perceptual concepts vs abstract conceptsdefined or relational and associated conceptscomplex concepts
constructs such as a schema and a script are examples of complex concepts
a schema is an organization of smaller concepts or features and is revised by situational information to assist in comprehension
a script on the other hand is a list of actions that a person follows in order to complete a desired goal
an example of a script would be the process of buying a cd
there are several actions that must occur before the actual act of purchasing the cd and a script provides a sequence of the necessary actions and proper order of these actions in order to be successful in purchasing the cd
discovery  every baby discovers concepts for itself such as discovering that each of its fingers can be individually controlled or that care givers are individuals
although this is perception driven formation of the concept is more than memorizing perceptions
examples  supervised or unsupervised generalizing from examples may lead to learning a new concept but concept formation is more than generalizing from examples
words  hearing or reading new words leads to learning new concepts but forming a new concept is more than learning a dictionary definition
a person may have previously formed a new concept before encountering the word or phrase for it
exemplars comparison and contrast  an efficient way to learn new categories and to induce new categorization rules is by comparing a few example objects while being informed about their categorical relation
comparing two exemplars while being informed that the two are from the same category allows identifying the attributes shared by the category members as it exemplifies variability within this category
on the other hand contrasting two exemplars while being informed that the two are from different categories may allow identifying attributes with diagnostic value
within category comparison and between categories contrast are not similarly useful for category learning hammer et al
and the capacity to use these two forms of comparisonbased learning changes at childhood hammer et al
invention  when prehistoric people who lacked tools used their fingernails to scrape food from killed animals or smashed melons they noticed that a broken stone sometimes had a sharp edge like a fingernail and was therefore suitable for scraping food
inventing a stone tool to avoid broken fingernails was a new concept
in general the theoretical issues underlying concept learning are those underlying induction
these issues are addressed in many diverse publications including literature on subjects like version spaces statistical learning theory pac learning information theory and algorithmic information theory
some of the broad theoretical ideas are also discussed by watanabe  solomonoff ab and rendell  see the reference list below
it is difficult to make any general statements about human or animal concept learning without already assuming a particular psychological theory of concept learning
although the classical views of concepts and concept learning in philosophy speak of a process of abstraction data compression simplification and summarization currently popular psychological theories of concept learning diverge on all these basic points
the history of psychology has seen the rise and fall of many theories about concept learning
classical conditioning as defined by pavlov created the earliest experimental technique
reinforcement learning as described by watson and elaborated by clark hull created a lasting paradigm in behavioral psychology
cognitive psychology emphasized a computer and information flow metaphor for concept formation
neural network models of concept formation and the structure of knowledge have opened powerful hierarchical models of knowledge organization such as george millers wordnet
neural networks are based on computational models of learning using factor analysis or convolution
neural networks also are open to neuroscience and psychophysiological models of learning following karl lashley and donald hebb
rulebased theories of concept learning began with cognitive psychology and early computer models of learning that might be implemented in a high level computer language with computational statements such as ifthen production rules
they take classification data and a rulebased theory as input which are the result of a rulebased learner with the hopes of producing a more accurate model of the data hekenaho
the majority of rulebased models that have been developed are heuristic meaning that rational analyses have not been provided and the models are not related to statistical approaches to induction
a rational analysis for rulebased models could presume that concepts are represented as rules and would then ask to what degree of belief a rational agent should be in agreement with each rule with some observed examples provided goodman griffiths feldman and tenenbaum
rulebased theories of concept learning are focused more so on perceptual learning and less on definition learning
rules can be used in learning when the stimuli are confusable as opposed to simple
when rules are used in learning decisions are made based on properties alone and rely on simple criteria that do not require a lot of memory  rouder and ratcliff
example of rulebased theorya radiologist using rulebased categorization would observewhether specific properties of an xray image meet certaincriteria for example is there an extreme difference in brightnessin a suspicious region relative to other regions a decision isthen based on this property alone
see rouder and ratcliff the prototype view of concept learning holds that people abstract out the central tendency or prototype of the examples experienced and use this as a basis for their categorization decisions
the prototype view of concept learning holds that people categorize based on one or more central examples of a given category followed by a penumbra of decreasingly typical examples
this implies that people do not categorize based on a list of things that all correspond to a definition but rather on a hierarchical inventory based on semantic similarity to the central examples
to illustrate imagine the following mental representations of the category sportsthe first illustration demonstrates a mental representation if we were to categorize by definitiondefinition of sports an athletic activity requiring skill or physical prowess and often of a competitive nature
the second illustration demonstrates a mental representation that prototype theory would predict
baseballbr
footballbr
basketballbr
soccerbr
hockeybr
tennisbr
golfbr
br
bikeracingbr
weightliftingbr
skateboardingbr
snowboardingbr
boxingbr
wrestlingbr
br
fishingbr
huntingbr
hikingbr
skydivingbr
bungeejumpingbr
br
cookingbr
walkingbr
br
gatoradebr
waterbr
proteinbr
dietit is evident that prototype theory hypothesizes a more continuous less discrete way of categorization in which the list of things that match the categorys definition is not limited
exemplar theory is the storage of specific instances exemplars with new objects evaluated only with respect to how closely they resemble specific known members and nonmembers of the category
this theory hypothesizes that learners store examples verbatim
this theory views concept learning as highly simplistic
only individual properties are represented
these individual properties are not abstract and they do not create rules
an example of what exemplar theory might look like is water is wet
it is simply known that some or one or all stored examples of water have the property wet
exemplar based theories have become more empirically popular over the years with some evidence suggesting that human learners use exemplar based strategies only in early learning forming prototypes and generalizations later in life
an important result of exemplar models in psychology literature has been a deemphasis of complexity in concept learning
one of the best known exemplar theories of concept learning is the generalized context model gcm
a problem with exemplar theory is that exemplar models critically depend on two measures similarity between exemplars and having a rule to determine group membership
sometimes it is difficult to attain or distinguish these measures
more recently cognitive psychologists have begun to explore the idea that the prototype and exemplar models form two extremes
it has been suggested that people are able to form a multiple prototype representation besides the two extreme representations
for example consider the category spoon
there are two distinct subgroups or conceptual clusters spoons tend to be either large and wooden or small and made of metal
the prototypical spoon would then be a mediumsize object made of a mixture of metal and wood which is clearly an unrealistic proposal
a more natural representation of the category spoon would instead consist of multiple at least two prototypes one for each cluster
a number of different proposals have been made in this regard anderson  griffiths canini sanborn  navarro  love medin  gureckis  vanpaemel  storms
these models can be regarded as providing a compromise between exemplar and prototype models
the basic idea of explanationbased learning suggests that a new concept is acquired by experiencing examples of it and forming a basic outline
put simply by observing or receiving the qualities of a thing the mind forms a concept which possesses and is identified by those qualities
the original theory proposed by mitchell keller and kedarcabelli in  and called explanationbased generalization is that learning occurs through progressive generalizing
this theory was first developed to program machines to learn
when applied to human cognition it translates as follows the mind actively separates information that applies to more than one thing and enters it into a broader description of a category of things
this is done by identifying sufficient conditions for something to fit in a category similar to schematizing
the revised model revolves around the integration of four mental processes  generalization chunking operationalization and analogy
this particular theory of concept learning is relatively new and more research is being conducted to test it
bayes theorem is important because it provides a powerful tool for understanding manipulating and controlling data that takes a larger view that is not limited to data analysis alone
the approach is subjective and this requires the assessment of prior probabilities making it also very complex
however if bayesians show that the accumulated evidence and the application of bayes law are sufficient the work will overcome the subjectivity of the inputs involved
bayesian inference can be used for any honestly collected data and has a major advantage because of its scientific focus
one model that incorporates the bayesian theory of concept learning is the actr model developed by john r
anderson
the actr model is a programming language that defines the basic cognitive and perceptual operations that enable the human mind by producing a stepbystep simulation of human behavior
this theory exploits the idea that each task humans perform consists of a series of discrete operations
the model has been applied to learning and memory higher level cognition natural language perception and attention humancomputer interaction education and computer generated forces
in addition to john r
anderson joshua tenenbaum has been a contributor to the field of concept learning he studied the computational basis of human learning and inference using behavioral testing of adults children and machines from bayesian statistics and probability theory but also from geometry graph theory and linear algebra
tenenbaum is working to achieve a better understanding of human learning in computational terms and trying to build computational systems that come closer to the capacities of human learners
merrills component display theory cdt is a cognitive matrix that focuses on the interaction between two dimensions the level of performance expected from the learner and the types of content of the material to be learned
merrill classifies a learners level of performance as find use remember and material content as facts concepts procedures and principles
the theory also calls upon four primary presentation forms and several other secondary presentation forms
the primary presentation forms include rules examples recall and practice
secondary presentation forms include prerequisites objectives helps mnemonics and feedback
a complete lesson includes a combination of primary and secondary presentation forms but the most effective combination varies from learner to learner and also from concept to concept
another significant aspect of the cdt model is that it allows for the learner to control the instructional strategies used and adapt them to meet his or her own learning style and preference
a major goal of this model was to reduce three common errors in concept formation overgeneralization undergeneralization and misconception
developmental robotics devrob sometimes called epigenetic robotics is a scientific field which aims at studying the developmental mechanisms architectures and constraints that allow lifelong and openended learning of new skills and new knowledge in embodied machines
as in human children learning is expected to be cumulative and of progressively increasing complexity and to result from selfexploration of the world in combination with social interaction
the typical methodological approach consists in starting from theories of human and animal development elaborated in fields such as developmental psychology neuroscience developmental and evolutionary biology and linguistics then to formalize and implement them in robots sometimes exploring extensions or variants of them
the experimentation of those models in robots allows researchers to confront them with reality and as a consequence developmental robotics also provides feedback and novel hypotheses on theories of human and animal development
developmental robotics is related to but differs from evolutionary robotics er
er uses populations of robots that evolve over time whereas devrob is interested in how the organization of a single robots control system develops through experience over time
devrob is also related to work done in the domains of robotics and artificial life
can a robot learn like a child can it learn a variety of new skills and new knowledge unspecified at design time and in a partially unknown and changing environment how can it discover its body and its relationships with the physical and social environment how can its cognitive capacities continuously develop without the intervention of an engineer once it is out of the factory what can it learn through natural social interactions with humans these are the questions at the center of developmental robotics
alan turing as well as a number of other pioneers of cybernetics already formulated those questions and the general approach in but it is only since the end of the th century that they began to be investigated systematically
because the concept of adaptive intelligent machine is central to developmental robotics it has relationships with fields such as artificial intelligence machine learning cognitive robotics or computational neuroscience
yet while it may reuse some of the techniques elaborated in these fields it differs from them from many perspectives
it differs from classical artificial intelligence because it does not assume the capability of advanced symbolic reasoning and focuses on embodied and situated sensorimotor and social skills rather than on abstract symbolic problems
it differs from traditional machine learning because it targets task independent selfdetermined learning rather than taskspecific inference over spoon fed humanedited sensori data weng et al
it differs from cognitive robotics because it focuses on the processes that allow the formation of cognitive capabilities rather than these capabilities themselves
it differs from computational neuroscience because it focuses on functional modeling of integrated architectures of development and learning
more generally developmental robotics is uniquely characterized by the following three featuresdevelopmental robotics emerged at the crossroads of several research communities including embodied artificial intelligence enactive and dynamical systems cognitive science connectionism
starting from the essential idea that learning and development happen as the selforganized result of the dynamical interactions among brains bodies and their physical and social environment and trying to understand how this self organization can be harnessed to provide taskindependent lifelong learning of skills of increasing complexity developmental robotics strongly interacts with fields such as developmental psychology developmental and cognitive neuroscience developmental biology embryology evolutionary biology and cognitive linguistics
as many of the theories coming from these sciences are verbal andor descriptive this implies a crucial formalization and computational modeling activity in developmental robotics
these computational models are then not only used as ways to explore how to build more versatile and adaptive machines but also as a way to evaluate their coherence and possibly explore alternative explanations for understanding biological development
due to the general approach and methodology developmental robotics projects typically focus on having robots develop the same types of skills as human infants
a first category that is importantly being investigated is the acquisition of sensorimotor skills
these include the discovery of ones own body including its structure and dynamics such as handeye coordination locomotion and interaction with objects as well as tool use with a particular focus on the discovery and learning of affordances
a second category of skills targeted by developmental robots are social and linguistic skills the acquisition of simple social behavioural games such as turntaking coordinated interaction lexicons syntax and grammar and the grounding of these linguistic skills into sensorimotor skills sometimes referred as symbol grounding
in parallel the acquisition of associated cognitive skills are being investigated such as the emergence of the selfnonself distinction the development of attentional capabilities of categorization systems and higherlevel representations of affordances or social constructs of the emergence of values empathy or theories of mind
the sensorimotor and social spaces in which humans and robot live are so large and complex that only a small part of potentially learnable skills can actually be explored and learnt within a lifetime
thus mechanisms and constraints are necessary to guide developmental organisms in their development and control of the growth of complexity
there are several important families of these guiding mechanisms and constraints which are studied in developmental robotics all inspired by human developmentwhile most developmental robotics projects strongly interact with theories of animal and human development the degrees of similarities and inspiration between identified biological mechanisms and their counterpart in robots as well as the abstraction levels of modeling may vary a lot
while some projects aim at modeling precisely both the function and biological implementation neural or morphological models such as in neurorobotics some other projects only focus on functional modeling of the mechanisms and constraints described above and might for example reuse in their architectures techniques coming from applied mathematics or engineering fields
as developmental robotics is a relatively novel research field and at the same time very ambitious many fundamental open challenges remain to be solved
first of all existing techniques are far from allowing realworld highdimensional robots to learn an open ended repertoire of increasingly complex skills over a lifetime period
highdimensional continuous sensorimotor spaces are a major obstacle to be solved
lifelong cumulative learning is another one
actually no experiments lasting more than a few days have been set up so far which contrasts severely with the time period needed by human infants to learn basic sensorimotor skills while equipped with brains and morphologies which are tremendously more powerful than existing computational mechanisms
among the strategies to explore in order to progress towards this target the interaction between the mechanisms and constraints described in the previous section shall be investigated more systematically
indeed they have so far mainly been studied in isolation
for example the interaction of intrinsically motivated learning and socially guided learning possibly constrained by maturation is an essential issue to be investigated
another important challenge is to allow robots to perceive interpret and leverage the diversity of multimodal social cues provided by nonengineer humans during humanrobot interaction
these capacities are so far mostly too limited to allow efficient general purpose teaching from humans
a fundamental scientific issue to be understood and resolved which applied equally to human development is how compositionality functional hierarchies primitives and modularity at all levels of sensorimotor and social structures can be formed and leveraged during development
this is deeply linked with the problem of the emergence of symbols sometimes referred as the symbol grounding problem when it comes to language acquisition
actually the very existence and need for symbols in the brain is actively questioned and alternative concepts still allowing for compositionality and functional hierarchies are being investigated
during biological epigenesis morphology is not fixed but rather develops in constant interaction with the development of sensorimotor and social skills
the development of morphology poses obvious practical problems with robots but it may be a crucial mechanism that should be further explored at least in simulation such as in morphogenetic robotics
another open problem is the understanding of the relation between the key phenomena investigated by developmental robotics e
hierarchical and modular sensorimotor systems intrinsicextrinsicsocial motivations and openended learning and the underlying brain mechanisms
similarly in biology developmental mechanisms operating at the ontogenetic time scale strongly interact with evolutionary mechanisms operating at the phylogenetic time scale as shown in the flourishing evodevo scientific literature
however the interaction of those mechanisms in artificial organisms developmental robots in particular is still vastly understudied
the interaction of evolutionary mechanisms unfolding morphologies and developing sensorimotor and social skills will thus be a highly stimulating topic for the future of developmental robotics
the nsfdarpa funded workshop on development and learning was held april   at michigan state university
it was the first international meeting devoted to computational understanding of mental development by robots and animals
the term by was used since the agents are active during development
the first undergraduate courses in devrob were offered at bryn mawr college and swarthmore college in the spring of  by douglas blank and lisa meeden respectively
the first graduate course in devrob was offered at iowa state university by alexander stoytchev in the fall of
in machine learning and statistics classification is the problem of identifying to which of a set of categories subpopulations a new observation belongs on the basis of a training set of data containing observations or instances whose category membership is known
examples are assigning a given email to the spam or nonspam class and assigning a diagnosis to a given patient based on observed characteristics of the patient gender blood pressure presence or absence of certain symptoms etc
classification is an example of pattern recognition
in the terminology of machine learning classification is considered an instance of supervised learning i
learning where a training set of correctly identified observations is available
the corresponding unsupervised procedure is known as clustering and involves grouping data into categories based on some measure of inherent similarity or distance
often the individual observations are analyzed into a set of quantifiable properties known variously as explanatory variables or features
these properties may variously be categorical e
a b ab or o for blood type ordinal e
large medium or small integervalued e
the number of occurrences of a particular word in an email or realvalued e
a measurement of blood pressure
other classifiers work by comparing observations to previous observations by means of a similarity or distance function
an algorithm that implements classification especially in a concrete implementation is known as a classifier
the term classifier sometimes also refers to the mathematical function implemented by a classification algorithm that maps input data to a category
terminology across fields is quite varied
in statistics where classification is often done with logistic regression or a similar procedure the properties of observations are termed explanatory variables or independent variables regressors etc
and the categories to be predicted are known as outcomes which are considered to be possible values of the dependent variable
in machine learning the observations are often known as instances the explanatory variables are termed features grouped into a feature vector and the possible categories to be predicted are classes
other fields may use different terminology e
in community ecology the term classification normally refers to cluster analysis i
a type of unsupervised learning rather than the supervised learning described in this article
classification and clustering are examples of the more general problem of pattern recognition which is the assignment of some sort of output value to a given input value
other examples are regression which assigns a realvalued output to each input sequence labeling which assigns a class to each member of a sequence of values for example part of speech tagging which assigns a part of speech to each word in an input sentence parsing which assigns a parse tree to an input sentence describing the syntactic structure of the sentence etc
a common subclass of classification is probabilistic classification
algorithms of this nature use statistical inference to find the best class for a given instance
unlike other algorithms which simply output a best class probabilistic algorithms output a probability of the instance being a member of each of the possible classes
the best class is normally then selected as the one with the highest probability
however such an algorithm has numerous advantages over nonprobabilistic classifiersearly work on statistical classification was undertaken by fisher in the context of twogroup problems leading to fishers linear discriminant function as the rule for assigning a group to a new observation
this early work assumed that datavalues within each of the two groups had a multivariate normal distribution
the extension of this same context to more than twogroups has also been considered with a restriction imposed that the classification rule should be linear
later work for the multivariate normal distribution allowed the classifier to be nonlinear several classification rules can be derived based on slight different adjustments of the mahalanobis distance with a new observation being assigned to the group whose centre has the lowest adjusted distance from the observation
unlike frequentist procedures bayesian classification procedures provide a natural way of taking into account any available information about the relative sizes of the subpopulations associated with the different groups within the overall population
bayesian procedures tend to be computationally expensive and in the days before markov chain monte carlo computations were developed approximations for bayesian clustering rules were devised
some bayesian procedures involve the calculation of group membership probabilities these can be viewed as providing a more and more informative outcome of a data analysis than a simple attribution of a single grouplabel to each new observation
classification can be thought of as two separate problems  binary classification and multiclass classification
in binary classification a better understood task only two classes are involved whereas multiclass classification involves assigning an object to one of several classes
since many classification methods have been developed specifically for binary classification multiclass classification often requires the combined use of multiple binary classifiers
most algorithms describe an individual instance whose category is to be predicted using a feature vector of individual measurable properties of the instance
each property is termed a feature also known in statistics as an explanatory variable or independent variable although features may or may not be statistically independent
features may variously be binary e
on or off categorical e
a b ab or o for blood type ordinal e
large medium or small integervalued e
the number of occurrences of a particular word in an email or realvalued e
a measurement of blood pressure
if the instance is an image the feature values might correspond to the pixels of an image if the instance is a piece of text the feature values might be occurrence frequencies of different words
some algorithms work only in terms of discrete data and require that realvalued or integervalued data be discretized into groups e
less than  between  and  or greater than a large number of algorithms for classification can be phrased in terms of a linear function that assigns a score to each possible category k by combining the feature vector of an instance with a vector of weights using a dot product
the predicted category is the one with the highest score
this type of score function is known as a linear predictor function and has the following general formwhere x is the feature vector for instance i  is the vector of weights corresponding to category k and scorex k is the score associated with assigning instance i to category k
in discrete choice theory where instances represent people and categories represent choices the score is considered the utility associated with person i choosing category k
algorithms with this basic setup are known as linear classifiers
what distinguishes them is the procedure for determining training the optimal weightscoefficients and the way that the score is interpreted
examples of such algorithms areexamples of classification algorithms includeclassifier performance depends greatly on the characteristics of the data to be classified
there is no single classifier that works best on all given problems a phenomenon that may be explained by the nofreelunch theorem
various empirical tests have been performed to compare classifier performance and to find the characteristics of data that determine classifier performance
determining a suitable classifier for a given problem is however still more an art than a science
the measures precision and recall are popular metrics used to evaluate the quality of a classification system
more recently receiver operating characteristic roc curves have been used to evaluate the tradeoff between true and falsepositive rates of classification algorithms
as a performance metric the uncertainty coefficient has the advantage over simple accuracy in that it is not affected by the relative sizes of the different classes
further it will not penalize an algorithm for simply rearranging the classes
classification has many applications
in some of these it is employed as a data mining procedure while in others more detailed statistical modeling is undertaken
evolutionary programming is one of the four major evolutionary algorithm paradigms
it is similar to genetic programming but the structure of the program to be optimized is fixed while its numerical parameters are allowed to evolve
it was first used by lawrence j
fogel in the us in  in order to use simulated evolution as a learning process aiming to generate artificial intelligence
fogel used finitestate machines as predictors and evolved them
currently evolutionary programming is a wide evolutionary computing dialect with no fixed structure or representation in contrast with some of the other dialects
it is becoming harder to distinguish from evolutionary strategies
its main variation operator is mutation members of the population are viewed as part of a specific species rather than members of the same species therefore each parent generates an offspring using a    survivor selection
in the field of statistical learning theory matrix regularization generalizes notions of vector regularization to cases where the object to be learned is a matrix
the purpose of regularization is to enforce conditions for example sparsity or smoothness that can produce stable predictive functions
for example in the more common vector framework tikhonov regularization optimizes overto find a vector formula that is a stable solution to the regression problem
when the system is described by a matrix rather than a vector this problem can be written aswhere the vector norm enforcing a regularization penalty on formula has been extended to a matrix norm on formula
matrix regularization has applications in matrix completion multivariate regression and multitask learning
ideas of feature and group selection can also be extended to matrices and these can be generalized to the nonparametric case of multiple kernel learning
consider a matrix formula to be learned from a set of examples formula where formula goes from formula to formula and formula goes from formula to formula
let each input matrix formula be formula and let formula be of size formula
a general model for the output formula can be posed aswhere the inner product is the frobenius inner product
for different applications the matrices formula will have different forms but for each of these the optimization problem to infer formula can be written aswhere formula defines the empirical error for a given formula and formula is a matrix regularization penalty
the function formula is typically chosen to be convex and is often selected to enforce sparsity using formulanorms andor smoothness using formulanorms
finally formula is in the space of matrices formula with forbenius inner product
in the problem of matrix completion the matrix formula takes the formwhere formula and formula are the canonical basis in formula and formula
in this case the role of the frobenius inner product is to select individual elements formula from the matrix formula
thus the output formula is a sampling of entries from the matrix formula
the problem of reconstructing formula from a small set of sampled entries is possible only under certain restrictions on the matrix and these restrictions can be enforced by a regularization function
for example it might be assumed that formula is lowrank in which case the regularization penalty can take the form of a nuclear norm
where formula with formula from formula to formula are the singular values of formula
models used in multivariate regression are parameterized by a matrix of coefficients
in the frobenius inner product above each matrix formula issuch that the output of the inner product is the dot product of one row of the input with one column of the coefficient matrix
the familiar form of such models ismany of the vector norms used in single variable regression can be extended to the multivariate case
one example is the squared frobenius norm which can be viewed as an formulanorm acting either entrywise or on the singular values of the matrixin the multivariate case the effect of regularizing with the frobenius norm is the same as the vector case very complex models will have larger norms and thus will be penalized more
the setup for multitask learning is almost the same as the setup for multivariate regression
the primary difference is that the input variables are also indexed by task columns of formula
the representation with the frobenius inner product is thenthe role of matrix regularization in this setting can be the same as in multivariate regression but matrix norms can also be used to couple learning problems across tasks
in particular note that for the optimization problemthe solutions corresponding to each column of formula are decoupled
that is the same solution can be found by solving the joint problem or by solving an isolated regression problem for each column
the problems can be coupled by adding an additional regulatization penalty on the covariance of solutionswhere formula models the relationship between tasks
this scheme can be used to both enforce similarity of solutions across tasks and to learn the specific structure of task similarity by alternating between optimizations of formula and formula
when the relationship between tasks is known to lie on a graph the laplacian matrix of the graph can be used to couple the learning problems
regularization by spectral filtering has been used to find stable solutions to problems such as those discussed above by addressing illposed matrix inversions see for example filter function for tikhonov regularization
in many cases the regularization function acts on the input or kernel to ensure a bounded inverse by eliminating small singular values but it can also be useful to have spectral norms that act on the matrix that is to be learned
there are a number of matrix norms that act on the singular values of the matrix
frequently used examples include the schatten pnorms with por
for example matrix regularization with a schatten norm also called the nuclear norm can be used to enforce sparsity in the spectrum of a matrix
this has been used in the context of matrix completion when the matrix in question is believed to have a restricted rank
in this case the optimization problem becomesspectral regularization is also used to enforce a reduced rank coefficient matrix in multivariate regression
in this setting a reduced rank coefficient matrix can be found by keeping just the top formula singular values but this can be extended to keep any reduced set of singular values and vectors
sparse optimization has become the focus of much research interest as a way to find solutions that depend on a small number of variables see e
the lasso method
in principle entrywise sparsity can be enforced by penalizing the entrywise formulanorm of the matrix but the formulanorm is not convex
in practice this can be implemented by convex relaxation to the formulanorm
while entrywise regularization with an formulanorm will find solutions with a small number of nonzero elements applying an formulanorm to different groups of variables can enforce structure in the sparsity of solutions
the most straightforward example of structured sparsity uses the formula norm with formula and formulafor example the formula norm is used in multitask learning to group features across tasks such that all the elements in a given row of the coefficient matrix can be forced to zero as a group
the grouping effect is achieved by taking the formulanorm of each row and then taking the total penalty to be the sum of these rowwise norms
this regularization results in rows that will tend to be all zeros or dense
the same type of regularization can be used to enforce sparsity columnwise by taking the formulanorms of each column
more generally the formula norm can be applied to arbitrary groups of variableswhere the index formula is across groups of variables and formula indicates the cardinality of group formula
algorithms for solving these group sparsity problems extend the more wellknown lasso and group lasso methods by allowing overlapping groups for example and have been implemented via matching pursuit and proximal gradient methods
by writing the proximal gradient with respect to a given coefficient formula it can be seen that this norm enforces a groupwise soft thresholdwhere formula is the indicator function for group norms formula
thus using formula norms it is straightforward to enforce structure in the sparsity of a matrix either rowwise columnwise or in arbitrary blocks
by enforcing group norms on blocks in multivariate or multitask regression for example it is possible to find groups of input and output variables such that defined subsets of output variables columns in the matrix formula will depend on the same sparse set of input variables
the ideas of structured sparsity and feature selection can be extended to the nonparametric case of multiple kernel learning
this can be useful when there are multiple types of input data color and texture for example with different appropriate kernels for each or when the appropriate kernel is unknown
if there are two kernels for example with feature maps formula and formula that lie in corresponding reproducing kernel hilbert spaces formula then a larger space formula can be created as the sum of two spacesassuming linear independence in formula and formula
in this case the formulanorm is again the sum of normsthus by choosing a matrix regularization function as this type of norm it is possible to find a solution that is sparse in terms of which kernels are used but dense in the coefficient of each used kernel
multiple kernel learning can also be used as a form of nonlinear variable selection or as a model aggregation technique e
by taking the sum of squared norms and relaxing sparsity constraints
for example each kernel can be taken to be the gaussian kernel with a different width
the computational intelligence and machine learning ciml community portal is an international multiuniversity initiative
its primary purpose is to help facilitate a virtual scientific community infrastructure for all those involved with or interested in computational intelligence and machine learning
this includes ciml research education and applicationoriented resources residing at the portal and others that are linked from the ciml site
the ciml community portal was created to facilitate an online virtual scientific community wherein anyone interested in ciml can share research obtain resources or simply learn more
the effort is currently led by jacek zurada principal investigator with rammohan ragade and janusz wojtusiak aided by a team of  volunteer researchers from  different countries
the ultimate goal of the ciml community portal is to accommodate and cater to a broad range of users including experts students the public and outside researchers interested in using ciml methods and software tools
each community member and user will be guided through the portal resources and tools based on their respective ciml experience e
expert student outside researcher and goals e
collaboration education
a preliminary version of the communitys portal with limited capabilities is now operational and available for users
all electronic resources on the portal are peerreviewed to ensure high quality and citeability for literature
most of the terms listed in wikipedia glossaries are already defined and explained within wikipedia itself
however glossaries like this one are useful for looking up comparing and reviewing large numbers of terms together
you can help enhance this page by adding new terms or writing definitions for existing ones
this glossary of artificial intelligence terms is about artificial intelligence its subdisciplines and related fields
data preprocessing is an important step in the data mining process
the phrase garbage in garbage out is particularly applicable to data mining and machine learning projects
datagathering methods are often loosely controlled resulting in outofrange values e
income  impossible data combinations e
sex male pregnant yes missing values etc
analyzing data that has not been carefully screened for such problems can produce misleading results
thus the representation and quality of data is first and foremost before running an analysis
often data preprocessing is the most important phase of a machine learning project especially in computational biology
if there is much irrelevant and redundant information present or noisy and unreliable data then knowledge discovery during the training phase is more difficult
data preparation and filtering steps can take considerable amount of processing time
data preprocessing includes cleaning instance selection normalization transformation feature extraction and selection etc
the product of data preprocessing is the final training set
kotsiantis et al
present a wellknown algorithm for each step of data preprocessing
in machine learning hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm
the same kind of machine learning model can require different constraints weights or learning rates to generalize different data patterns
these measures are called hyperparameters and have to be tuned so that the model can optimally solve the machine learning problem
hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data
the objective function takes a tuple of hyperparameters and returns the associated loss
crossvalidation is often used to estimate this generalization performance
the traditional way of performing hyperparameter optimization has been grid search or a parameter sweep which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm
a grid search algorithm must be guided by some performance metric typically measured by crossvalidation on the training setor evaluation on a heldout validation set
since the parameter space of a machine learner may include realvalued or unbounded value spaces for certain parameters manually set bounds and discretization may be necessary before applying grid search
for example a typical softmargin svm classifier equipped with an rbf kernel has at least two hyperparameters that need to be tuned for good performance on unseen data a regularization constant c and a kernel hyperparameter
both parameters are continuous so to perform grid search one selects a finite set of reasonable values for each saygrid search then trains an svm with each pair c  in the cartesian product of these two sets and evaluates their performance on a heldout validation set or by internal crossvalidation on the training set in which case multiple svms are trained per pair
finally the grid search algorithm outputs the settings that achieved the highest score in the validation procedure
grid search suffers from the curse of dimensionality but is often embarrassingly parallel because typically the hyperparameter settings it evaluates are independent of each other
random search replaces the exhaustive enumeration of all combinations by selecting them randomly
this can be simply applied to the discrete setting described above but also generalizes to continuous and mixed spaces
it can outperform grid search especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm
in this case the optimization problem is said to have a low intrinsic dimensionality
random search is also embarrassingly parallel and additionally allows to include prior knowledge by specifying the distribution from which to sample
bayesian optimization is a global optimization method for noisy blackbox functions
applied to hyperparameter optimization bayesian optimization builds a probabilistic model of the function mapping from hyperparameter values to the objective evaluated on a validation set
by iteratively evaluating a promising hyperparameter configuration based on the current model and then updating it bayesian optimization aims to gather observations revealing as much information as possible about this function and in particular the location of the optimum
it tries to balance exploration hyperparameters for which the outcome is most uncertain and exploitation hyperparameters expected close to the optimum
in practice bayesian optimization has been shown to obtain better results in fewer evaluations compared to grid search and random search due to the ability to reason about the quality of experiments before they are run
for specific learning algorithms it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent
the first usage of these techniques was focused on neural networks
since then these methods have been extended to other models such as support vector machines or logistic regression
a different approach in order to obtain a gradient with respect to hyperparameters consists in differentiating the steps of an iterative optimization algorithm using automatic differentiation
evolutionary optimization is a methodology for the global optimization of noisy blackbox functions
in hyperparameter optimization evolutionary optimization uses evolutionary algorithms to search the space of hyperparameters for a given algorithm
evolutionary hyperparameter optimization follows a process inspired by the biological concept of evolutionevolutionary optimization has been used in hyperparameter optimization for statistical machine learning algorithms automated machine learning deep neural network architecture search as well as training of the weights in deep neural networks
rbf and spectral approaches have also been developed
user behavior analytics uba as defined by gartner is a cybersecurity process about detection of insider threats targeted attacks and financial fraud
uba solutions look at patterns of human behavior and then apply algorithms and statistical analysis to detect meaningful anomalies from those patternsanomalies that indicate potential threats
instead of tracking devices or security events uba tracks a systems users
big data platforms like apache hadoop are increasing uba functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats
the problem uba responds to as described by nemertes research ceo johna till johnson is that security systems provide so much information that its tough to uncover information that truly indicates a potential for real attack
analytics tools help make sense of the vast amount of data that siem idsips system logs and other tools gather
uba tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them
uba technology first evolved in the field of marketing to help companies understand and predict consumerbuying patterns
but as it turns out uba can be extraordinarily useful in the security context too
developments in uba technology led gartner to evolve the category to user and entity behavior analytics ueba
in september  gartner published the market guide for user and entity analytics by vice president and distinguished analyst avivah litan that provided a thorough definition and explanation
ueba was referred to in earlier gartner reports but not in much depth
expanding the definition from uba includes devices applications servers data or anything with an ip address
it moves beyond the fraudoriented uba focus to a broader one encompassing malicious and abusive behavior that otherwise went unnoticed by existing security monitoring systems such as siem and dlp
the addition of entity reflects that devices may play a role in a network attack and may also be valuable in uncovering attack activity
when end users have been compromised malware can lay dormant and go undetected for months
rather than trying to find where the outsider entered uebas allow for quicker detection by using algorithms to detect insider threats
particularly in the computer security market there are many vendors for ueba applications
they can be differentiated by whether they are designed to monitor onpremises or cloudbased software as a service saas applications the methods in which they obtain the source data the type of analytics they use i
packaged analytics userdriven or vendorwritten and the service delivery method i
onpremises or a cloudbased
according to the  market guide released by gartner the ueba market grew substantially in  ueba vendors grew their customer base market consolidation began and gartner client interest in ueba and security analytics increased
the report further projected over the next three years leading ueba platforms will become preferred systems for security operations and investigations at some of the organizations they serve
it will beand in some cases already ismuch easier to discover some security events and analyze individual offenders in ueba than it is in many legacy security monitoring systems
in machine learning kernel methods arise from the assumption of an inner product space or similarity structure on inputs
for some such methods such as support vector machines svms the original formulation and its regularization were not bayesian in nature
it is helpful to understand them from a bayesian perspective
because the kernels are not necessarily positive semidefinite the underlying structure may not be inner product spaces but instead more general reproducing kernel hilbert spaces
in bayesian probability kernel methods are a key component of gaussian processes where the kernel function is known as the covariance function
kernel methods have traditionally been used in supervised learning problems where the input space is usually a space of vectors while the output space is a space of scalars
more recently these methods have been extended to problems that deal with multiple outputs such as in multitask learning
in this article we analyze the connections between the regularization and the bayesian point of view for kernel methods in the case of scalar outputs
a mathematical equivalence between the regularization and the bayesian point of view is easily proved in cases where the reproducing kernel hilbert space is finitedimensional
the infinitedimensional case raises subtle mathematical issues we will consider here the finitedimensional case
we start with a brief review of the main ideas underlying kernel methods for scalar learning and briefly introduce the concepts of regularization and gaussian processes
we then show how both points of view arrive at essentially equivalent estimators and show the connection that ties them together
the classical supervised learning problem requires estimating the output for some new input point formula by learning a scalarvalued estimator formula on the basis of a training set formula consisting of formula inputoutput pairs formula
given a symmetric and positive bivariate function formula called a kernel one of the most popular estimators in machine learning is given bywhere formula is the kernel matrix with entries formula formula and formula
we will see how this estimator can be derived both from a regularization and a bayesian perspective
the main assumption in the regularization perspective is that the set of functions formula is assumed to belong to a reproducing kernel hilbert space formula
a reproducing kernel hilbert space rkhs formula is a hilbert space of functions defined by a symmetric positivedefinite function formula called the reproducing kernel such that the function formula belongs to formula for all formula
there are three main properties make an rkhs appealing
the reproducing property which gives name to the spacewhere formula is the inner product in formula
functions in an rkhs are in the closure of the linear combination of the kernel at given pointsthis allows the construction in a unified framework of both linear and generalized linear models
the squared norm in an rkhs can be written asand could be viewed as measuring the complexity of the function
the estimator is derived as the minimizer of the regularized functionalwhere formula and formula is the norm in formula
the first term in this functional which measures the average of the squares of the errors between the formula and the formula is called the empirical risk and represents the cost we pay by predicting formula for the true value formula
the second term in the functional is the squared norm in a rkhs multiplied by a weight formula and serves the purpose of stabilizing the problem as well as of adding a tradeoff between fitting and complexity of the estimator
the weight formula called the regularizer determines the degree to which instability and complexity of the estimator should be penalized higher penalty for increasing value of formula
the explicit form of the estimator in equation  is derived in two steps
first the representer theorem states that the minimizer of the functional  can always be written as a linear combination of the kernels centered at the trainingset pointsfor some formula
the explicit form of the coefficients formula can be found by substituting for formula in the functional
for a function of the form in equation  we have thatwe can rewrite the functional  asthis functional is convex in formula and therefore we can find its minimum by setting the gradient with respect to formula to zerosubstituting this expression for the coefficients in equation  we obtain the estimator stated previously in equation the notion of a kernel plays a crucial role in bayesian probability as the covariance function of a stochastic process called the gaussian process
as part of the bayesian framework the gaussian process specifies the prior distribution that describes the prior beliefs about the properties of the function being modeled
these beliefs are updated after taking into account observational data by means of a likelihood function that relates the prior beliefs to the observations
taken together the prior and likelihood lead to an updated distribution called the posterior distribution that is customarily used for predicting test cases
a gaussian process gp is a stochastic process in which any finite number of random variables that are sampled follow a joint normal distribution
the mean vector and covariance matrix of the gaussian distribution completely specify the gp
gps are usually used as a priori distribution for functions and as such the mean vector and covariance matrix can be viewed as functions where the covariance function is also called the kernel of the gp
let a function formula follow a gaussian process with mean function formula and kernel function formulain terms of the underlying gaussian distribution we have that for any finite set formula if we let formula thenwhere formula is the mean vector and formula is the covariance matrix of the multivariate gaussian distribution
in a regression context the likelihood function is usually assumed to be a gaussian distribution and the observations to be independent and identically distributed iidthis assumption corresponds to the observations being corrupted with zeromean gaussian noise with variance formula
the iid assumption makes it possible to factorize the likelihood function over the data points given the set of inputs formula and the variance of the noise formula and thus the posterior distribution can be computed analytically
for a test input vector formula given the training data formula the posterior distribution is given bywhere formula denotes the set of parameters which include the variance of the noise formula and any parameters from the covariance function formula and wherea connection between regularization theory and bayesian theory can only be achieved in the case of finite dimensional rkhs
under this assumption regularization theory and bayesian theory are connected through gaussian process prediction
in the finite dimensional case every rkhs can be described in terms of a feature map formula such thatfunctions in the rkhs with kernel formula can be then be written asand we also have thatwe can now build a gaussian process by assuming formula to be distributed according to a multivariate gaussian distribution with zero mean and identity covariance matrixif we assume a gaussian likelihood we havewhere formula
the resulting posterior distribution is the given bywe can see that a maximum posterior map estimate is equivalent to the minimization problem defining tikhonov regularization where in the bayesian case the regularization parameter is related to the noise variance
from a philosophical perspective the loss function in a regularization setting plays a different role than the likelihood function in the bayesian setting
whereas the loss function measures the error that is incurred when predicting formula in place of formula the likelihood function measures how likely the observations are from the model that was assumed to be true in the generative process
from a mathematical perspective however the formulations of the regularization and bayesian frameworks make the loss function and the likelihood function to have the same mathematical role of promoting the inference of functions formula that approximate the labels formula as much as possible
labeled data is a group of samples that have been tagged with one or more labels
labeling typically takes a set of unlabeled data and augments each piece of that unlabeled data with meaningful tags that are informative
for example labels might be indicate whether a photo contains a horse or a cow which words were uttered in an audio recording what type of action is being performed in a video what the topic of a news article is what the overall sentiment of a tweet is whether the dot in an xray is a tumor etc
labels can be obtained by asking humans to make judgments about a given piece of unlabeled data e
does this photo contain a horse or a cow and are significantly more expensive to obtain than the raw unlabeled data
after obtaining a labeled dataset machine learning models can be applied to the data so that new unlabeled data can be presented to the model and a likely label can be guessed or predicted for that piece of unlabeled data
dataset could be of different types depending on use case look at some open human labeled datasets open human labeled datasets
active learning is a special case of semisupervised machine learning in which a learning algorithm is able to interactively query the user or some other information source to obtain the desired outputs at new data points
in statistics literature it is sometimes also called optimal experimental design
there are situations in which unlabeled data is abundant but manually labeling is expensive
in such a scenario learning algorithms can actively query the userteacher for labels
this type of iterative supervised learning is called active learning
since the learner chooses the examples the number of examples to learn a concept can often be much lower than the number required in normal supervised learning
with this approach there is a risk that the algorithm be overwhelmed by uninformative examples
recent developments are dedicated to multilabel active learning hybrid active learning and active learning in a singlepass online context combining concepts from the field of machine learning e
conflict and ignorance with adaptive incremental learning policies in the field of online machine learning
let be the total set of all data under consideration
for example in a protein engineering problem would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity
during each iteration  is broken up into three subsetsmost of the current research in active learning involves the best method to choose the data points for
algorithms for determining which data points should be labeled can be organized into a number of different categoriesa wide variety of algorithms have been studied that fall into these categories
some active learning algorithms are built upon support vector machines svms and exploit the structure of the svm to determine which data points to label
such methods usually calculate the margin  of each unlabeled datum in and treat as an dimensional distance from that datum to the separating hyperplane
minimum marginal hyperplane methods assume that the data with the smallest are those that the svm is most uncertain about and therefore should be placed in to be labeled
other similar methods such as maximum marginal hyperplane choose data with the largest
tradeoff methods choose a mix of the smallest and largest s
granular computing grc is an emerging computing paradigm of information processing
it concerns the processing of complex information entities called information granules which arise in the process of data abstraction and derivation of knowledge from information or data
generally speaking information granules are collections of entities that usually originate at the numeric level and are arranged together due to their similarity functional or physical adjacency indistinguishability coherency or the like
at present granular computing is more a theoretical perspective than a coherent set of methods or principles
as a theoretical perspective it encourages an approach to data that recognizes and exploits the knowledge present in data at various levels of resolution or scales
in this sense it encompasses all methods which provide flexibility and adaptability in the resolution at which knowledge or information is extracted and represented
as mentioned above granular computing is not an algorithm or process there is no particular method that is called granular computing
it is rather an approach to looking at data that recognizes how different and interesting regularities in the data can appear at different levels of granularity much as different features become salient in satellite images of greater or lesser resolution
on a lowresolution satellite image for example one might notice interesting cloud patterns representing cyclones or other largescale weather phenomena while in a higherresolution image one misses these largescale atmospheric phenomena but instead notices smallerscale phenomena such as the interesting pattern that is the streets of manhattan
the same is generally true of all data at different resolutions or granularities different features and relationships emerge
the aim of granular computing is to try to take advantage of this fact in designing more effective machinelearning and reasoning systems
there are several types of granularity that are often encountered in data mining and machine learning and we review them belowone type of granulation is the quantization of variables
it is very common that in data mining or machinelearning applications the resolution of variables needs to be decreased in order to extract meaningful regularities
an example of this would be a variable such as outside temperature formula which in a given application might be recorded to several decimal places of precision depending on the sensing apparatus
however for purposes of extracting relationships between outside temperature and say number of healthclub applications formula it will generally be advantageous to quantize outside temperature into a smaller number of intervals
there are several interrelated reasons for granulating variables in this fashionfor example a simple learner or pattern recognition system may seek to extract regularities satisfying a conditional probability threshold such as formula
in the special case where formula this recognition system is essentially detecting logical implication of the form formula or in words if formula then formula
the systems ability to recognize such implications or in general conditional probabilities exceeding threshold is partially contingent on the resolution with which the system analyzes the variables
as an example of this last point consider the feature space shown to the right
the variables may each be regarded at two different resolutions
variable formula may be regarded at a high quaternary resolution wherein it takes on the four values formula or at a lower binary resolution wherein it takes on the two values formula
similarly variable formula may be regarded at a high quaternary resolution or at a lower binary resolution where it takes on the values formula or formula respectively
it will be noted that at the high resolution there are no detectable implications of the form formula since every formula is associated with more than one formula and thus for all formula formula
however at the low binary variable resolution two bilateral implications become detectable formula and formula since every formula occurs iff formula and formula occurs iff formula
thus a pattern recognition system scanning for implications of this kind would find them at the binary variable resolution but would fail to find them at the higher quaternary variable resolution
it is not feasible to exhaustively test all possible discretization resolutions on all variables in order to see which combination of resolutions yields interesting or significant results
instead the feature space must be preprocessed often by an entropy analysis of some kind so that some guidance can be given as to how the discretization process should proceed
moreover one cannot generally achieve good results by naively analyzing and discretizing each variable independently since this may obliterate the very interactions that we had hoped to discover
a sample of papers that address the problem of variable discretization in general and multiplevariable discretization in particular is as follows                 variable granulation is a term that could describe a variety of techniques most of which are aimed at reducing dimensionality redundancy and storage requirements
we briefly describe some of the ideas here and present pointers to the literature
a number of classical methods such as principal component analysis multidimensional scaling factor analysis and structural equation modeling and their relatives fall under the genus of variable transformation
also in this category are more modern areas of study such as dimensionality reduction projection pursuit and independent component analysis
the common goal of these methods in general is to find a representation of the data in terms of new variables which are a linear or nonlinear transformation of the original variables and in which important statistical relationships emerge
the resulting variable sets are almost always smaller than the original variable set and hence these methods can be loosely said to impose a granulation on the feature space
these dimensionality reduction methods are all reviewed in the standard texts such as   and
a different class of variable granulation methods derive more from data clustering methodologies than from the linear systems theory informing the above methods
it was noted fairly early that one may consider clustering related variables in just the same way that one considers clustering related data
in data clustering one identifies a group of similar entities using a measure of similarity suitable to the domain and then in some sense replaces those entities with a prototype of some kind
the prototype may be the simple average of the data in the identified cluster or some other representative measure
but the key idea is that in subsequent operations we may be able to use the single prototype for the data cluster along with perhaps a statistical model describing how exemplars are derived from the prototype to stand in for the much larger set of exemplars
these prototypes are generally such as to capture most of the information of interest concerning the entities
similarly it is reasonable to ask whether a large set of variables might be aggregated into a smaller set of prototype variables that capture the most salient relationships between the variables
although variable clustering methods based on linear correlation have been proposed  more powerful methods of variable clustering are based on the mutual information between variables
watanabe has shown  that for any set of variables one can construct a polytomic i
nary tree representing a series of variable agglomerations in which the ultimate total correlation among the complete variable set is the sum of the partial correlations exhibited by each agglomerating subset see figure
watanabe suggests that an observer might seek to thus partition a system in such a way as to minimize the interdependence between the parts
as if they were looking for a natural division or a hidden crack
one practical approach to building such a tree is to successively choose for agglomeration the two variables either atomic variables or previously agglomerated variables which have the highest pairwise mutual information
the product of each agglomeration is a new constructed variable that reflects the local joint distribution of the two agglomerating variables and thus possesses an entropy equal to their joint entropy
from a procedural standpoint this agglomeration step involves replacing two columns in the attributevalue tablerepresenting the two agglomerating variableswith a single column that has a unique value for every unique combination of values in the replaced columns
no information is lost by such an operation however it should be noted that if one is exploring the data for intervariable relationships it would generally not be desirable to merge redundant variables in this way since in such a context it is likely to be precisely the redundancy or dependency between variables that is of interest and once redundant variables are merged their relationship to one another can no longer be studied
in database systems aggregations see e
olap aggregation and business intelligence systems result in transforming original data tables often called information systems into the tables with different semantics of rows and columns wherein the rows correspond to the groups granules of original tuples and the columns express aggregated information about original values within each of the groups
such aggregations are usually based on sql and its extensions
the resulting granules usually correspond to the groups of original tuples with the same values or ranges over some preselected original columns
there are also other approaches wherein the groups are defined basing on e
physical adjacency of rows
for example infobright implemented a database engine wherein data was partitioned onto rough rows each consisting of k of physically consecutive or almost consecutive rows
rough rows were automatically labeled with compact information about their values on data columns often involving multicolumn and multitable relationships
it resulted in a higher layer of granulated information where objects corresponded to rough rows and attributes  to various aspects of rough information
database operations could be efficiently supported within such a new framework with an access to the original data pieces still available
the origins of the granular computing ideology are to be found in the rough sets and fuzzy sets literatures
one of the key insights of rough set researchalthough by no means unique to itis that in general the selection of different sets of features or variables will yield different concept granulations
here as in elementary rough set theory by concept we mean a set of entities that are indistinguishable or indiscernible to the observer i
a simple concept or a set of entities that is composed from such simple concepts i
a complex concept
to put it in other words by projecting a data set valueattribute system onto different sets of variables we recognize alternative sets of equivalenceclass concepts in the data and these different sets of concepts will in general be conducive to the extraction of different relationships and regularities
we illustrate with an example
consider the attributevalue system belowwhen the full set of attributes formula is considered we see that we have the following seven equivalence classes or primitive simple conceptsthus the two objects within the first equivalence class formula cannot be distinguished from one another based on the available attributes and the three objects within the second equivalence class formula cannot be distinguished from one another based on the available attributes
the remaining five objects are each discernible from all other objects
now let us imagine a projection of the attribute value system onto attribute formula alone which would represent for example the view from an observer which is only capable of detecting this single attribute
then we obtain the following much coarser equivalence class structure
this is in a certain regard the same structure as before but at a lower degree of resolution larger grain size
just as in the case of value granulation discretizationquantization it is possible that relationships dependencies may emerge at one level of granularity that are not present at another
as an example of this we can consider the effect of concept granulation on the measure known as attribute dependency a simpler relative of the mutual information
to establish this notion of dependency see also rough sets let formula represent a particular concept granulation where each formula is an equivalence class from the concept structure induced by attribute set formula
for example if the attribute set formula consists of attribute formula alone as above then the concept structure formula will be composed of formula formula and formula
the dependency of attribute set formula on another attribute set formula formula is given bythat is for each equivalence class formula in formula we add up the size of its lower approximation see rough sets by the attributes in formula i
formula
more simply this approximation is the number of objects which on attribute set formula can be positively identified as belonging to target set formula
added across all equivalence classes in formula the numerator above represents the total number of objects whichbased on attribute set formulacan be positively categorized according to the classification induced by attributes formula
the dependency ratio therefore expresses the proportion within the entire universe of such classifiable objects in a sense capturing the synchronization of the two concept structures formula and formula
the dependency formula can be interpreted as a proportion of such objects in the information system for which it suffices to know the values of attributes in formula to determine the values of attributes in formula ziarko  shan
having gotten definitions now out of the way we can make the simple observation that the choice of concept granularity i
choice of attributes will influence the detected dependencies among attributes
consider again the attribute value table from abovelet us consider the dependency of attribute set formulaon attribute set formula
that is we wish to know what proportion of objects can be correctly classified into classes of formula based on knowledge of formula
the equivalence classes of formula and of formula are shown below
the objects that can be definitively categorized according to concept structure formula based on formula are those in the set formula and since there are six of these the dependency of formula on formula formula
this might be considered an interesting dependency in its own right but perhaps in a particular data mining application only stronger dependencies are desired
we might then consider the dependency of the smaller attribute set formulaon the attribute set formula
the move from formula to formula induces a coarsening of the class structure formula as will be seen shortly
we wish again to know what proportion of objects can be correctly classified into the now larger classes of formula based on knowledge of formula
the equivalence classes of the new formula and of formula are shown below
clearly formula has a coarser granularity than it did earlier
the objects that can now be definitively categorized according to the concept structure formula based on formula constitute the complete universe formula and thus the dependency of formula on formula formula
that is knowledge of membership according to category set formula is adequate to determine category membership in formula with complete certainty in this case we might say that formula
thus by coarsening the concept structure we were able to find a stronger deterministic dependency
however we also note that the classes induced in formula from the reduction in resolution necessary to obtain this deterministic dependency are now themselves large and few in number as a result the dependency we found while strong may be less valuable to us than the weaker dependency found earlier under the higher resolution view of formula
in general it is not possible to test all sets of attributes to see which induced concept structures yield the strongest dependencies and this search must be therefore be guided with some intelligence
papers which discuss this issue and others relating to intelligent use of granulation are those by y
yao and lotfi zadeh listed in the references below
another perspective on concept granulation may be obtained from work on parametric models of categories
in mixture model learning for example a set of data is explained as a mixture of distinct gaussian or other distributions
thus a large amount of data is replaced by a small number of distributions
the choice of the number of these distributions and their size can again be viewed as a problem of concept granulation
in general a better fit to the data is obtained by a larger number of distributions or parameters but in order to extract meaningful patterns it is necessary to constrain the number of distributions thus deliberately coarsening the concept resolution
finding the right concept resolution is a tricky problem for which many methods have been proposed e
aic bic mdl etc
and these are frequently considered under the rubric of model regularization
granular computing can be conceived as a framework of theories methodologies techniques and tools that make use of information granules in the process of problem solving
in this sense granular computing is used as an umbrella term to cover topics that have been studied in various fields in isolation
by examining all of these existing studies in light of the unified framework of granular computing and extracting their commonalities it may be possible to develop a general theory for problem solving
in a more philosophical sense granular computing can describe a way of thinking that relies on the human ability to perceive the real world under various levels of granularity i
abstraction in order to abstract and consider only those things that serve a specific interest and to switch among different granularities
by focusing on different levels of granularity one can obtain different levels of knowledge as well as a greater understanding of the inherent knowledge structure
granular computing is thus essential in human problem solving and hence has a very significant impact on the design and implementation of intelligent systems
proaftn is a fuzzy classification method that belongs to the class of supervised learning algorithms
the acronym proaftn stands for procdure daffectation floue pour la problmatique du tri nominal which means in english fuzzy assignment procedure for nominal sorting
the method enables to determine the fuzzy indifference relations by generalizing the indices concordance and discordance used in the electre iii method
to determine the fuzzy indifference relations proaftn uses the general scheme of the discretization technique described in that establishes a set of preclassified cases called a training set
to resolve the classification problems proaftn proceeds by the following stagesstage
modeling of classes in this stage the prototypes of the classes are conceived using the two following stepsdirect technique it consists in adjusting the parameters through the training set and with the expert intervention
indirect technique it consists in fitting the parameters without the expert intervention as used in machine learning approaches
in multicriteria classification problem the indirect technique is known as preference disaggregation analysis
this technique requires less cognitive effort than the former technique it uses an automatic method to determine the optimal parameters which minimize the classification errors
furthermore several heuristics and metaheuristics were used to learn the multicriteria classification method proaftn
stage
assignment after conceiving the prototypes proaftn proceeds to assign the new objects to specific classes
algorithmic bias occurs when a computer system behaves in ways that reflects the implicit values of humans involved in that data collection selection or use
algorithmic bias has been identified and critiqued for its impact on search engine results social media platforms privacy and racial profiling
in search results this bias can create results reflecting racist sexist or other social biases despite the presumed neutrality of the data
the study of algorithmic bias is most concerned with algorithms that reflect systematic and unfair discrimination
as algorithms expand their ability to organize society politics institutions and behavior sociologists have become concerned with the ways unanticipated output and manipulation can impact the physical world
because algorithms are often considered to be neutral and unbiased they can inaccurately project greater authority than human expertise and in some cases reliance on algorithms can displace human responsibility for their outcomes
nonetheless bias can enter into algorithmic systems as a result of preexisting cultural social or institutional expectations because of technical limitations of their design or through use in unanticipated contexts or by audiences not considered in their initial design
algorithmic bias has been discovered or theorized in cases ranging from election outcomes to the spread of online hate speech
problems in understanding researching and discovering algorithmic bias may come from the proprietary nature of algorithms which are typically treated as trade secrets
even with full transparency understanding algorithms can be difficult because of their complexity and because not every permutation of an algorithms input or output can be anticipated or reproduced
in many cases even within a single usecase such as a website or app there is no single algorithm to examine but a vast network of interrelated programs and data inputs even between users of the same service
algorithms are difficult to define but may be generally understood as sets of instructions within computer programs that determine how these programs read collect process and analyze data to generate some readable form of analysis or output
newer computers can process millions of these algorithmic instructions per second which has boosted the design and adoption of technologies such as machine learning and artificial intelligence
by analyzing and processing data algorithms drive search engines social media websites recommendation engines online retail online advertising and more
contemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications in order to understand their political effects and to question the underlying assumptions of their neutrality
the term algorithmic bias is used to describe systematic and repeatable errors that create unfair outcomes i
generating one result for certain users and another result for others
for example a credit score algorithm may deny a loan based on certain factors without being unfair if it is consistently weighing relevant financial criteria
if that algorithm allows loans to some but denies loans to another set of nearly identical users based on arbitrary criteria and if this behavior can be repeated across multiple occurrences an algorithm can be described as biased
this bias may be intentional or unintentional
bias can be introduced to an algorithm in several ways
during the assemblage of a database data must be collected digitized adapted and entered according to humanassisted cataloging criteria
next in the design of the algorithm programmers assign certain priorities or hierarchies in how programs assess and sort that data
this requires human decisions about how data is categorized and which data is discarded
some algorithms collect their own data based on humanselected criteria which can reflect the bias of human users
others may practice reinforcing stereotypes and preferences as they process and display relevant data for human users as in selecting information based on previous choices of a user or group of users
beyond assembling the data bias can emerge as a result of design
examples may arise in sorting processes that determine the allocation of resources or scrutiny as in determining school placements or classification and identification processes that may inadvertently discriminate against a category when assigning risk as in credit scores
in processing associations such as recommendation engines or inferred marketing traits algorithms may be flawed in ways that reveal personal information
inclusion and exclusion criteria may have unanticipated outcomes for search results such as in flight recommendation software omitting flights that dont follow the sponsoring airlines preferred flight paths
algorithms may also display an uncertainty bias offering more confident assessments when larger data sets are available
this can skew algorithmic processes toward results that more closely correspond with larger sample populations which may not align with data from underrepresented populations
the earliest computer programs reflected simple humanderived operations and were deemed to be functioning when they completed those operations
artificial intelligence pioneer joseph weizenbaum wrote that such programs are therefore understood to embody law
weizenbaum describes early simple computer programs changing perceptions of machines from transferring power to transferring information
however he noted that machines might transfer information with unintended consequences if there are errors in details provided to the machine and if users interpret data in intuitive ways that cannot be formally communicated to or from a machine
weizenbaum stated that all data fed to a machine must reflect human decisionmaking processes which have been translated into rules for the computer to follow
to do this weizenbaum asserted that programmers legislate the laws for a world one first has to create in imagination and as a result computer simulations can be built on models with incomplete or incorrect human data
weizenbaum compared the results of such decisions to a tourist in a country who can make correct decisions through a coin toss but has no basis of understanding how or why the decision was made
the complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design
decisions made by one designer or team of designers may be obscured among the many pieces of code created for a single program over time these decisions and their impact may be forgotten and taken as natural results of the programs output
these biases can create new patterns of behavior or scripts in relationship to specific technologies as the code interacts with other elements of society
biases may also impact how society shapes itself around the data points that algorithms require
the decisions of algorithmic programs can be weighed more heavily than the decisions of the human beings they are meant to assist a process described by author clay shirky as algorithmic authority
shirky uses the term to describe the decision to regard as authoritative an unmanaged process of extracting value from diverse untrustworthy sources such as search results
this neutrality can also be misrepresented through language frames used when results are presented to the public
for example a list of news items selected and presented as trending or popular may be weighed based on significantly wider criteria than their popularity
because of their convenience and authority algorithms are theorized as a means of delegating responsibility in decision making away from humans
this can have the effect of reducing alternative options compromises or flexibility
sociologist scott lash has critiqued algorithms as a new form of generative power in that they are a virtual means of generating actual ends
preexisting bias in an algorithm is a consequence of underlying social and institutional ideologies
such ideas may reflect personal biases of individual designers or programmers or can reflect social institutional or cultural assumptions
in both cases such prejudices can be explicit and conscious or implicit and unconscious
poorly selected input data will influence the outcomes created by machines
in a critical view encoding preexisting bias into software can preserve social and institutional bias and replicate it into all possible uses of the algorithm into the future
an example of this form of bias is the british nationality act program designed to automate the evaluation of new uk citizens after the  british nationality act
the program accurately reflected the tenets of the law which stated that a man is the father of only his legitimate children whereas a woman is the mother of all her children legitimate or not
by attempting to appropriately articulate this logic into an algorithmic process the bnap inscribed the logic of the british nationality act into its algorithm
technical bias emerges through limitations of a program computational power its design or other constraint on the system
such bias can also be a restraint of design for example a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three as in an airline price display
flaws in random number generation can also introduce bias into results
a decontextualized algorithm uses unrelated information to sort results for example a flightpricing algorithm that sorts results by alphabetical order would be biased in favor of american airlines over united airlines
the opposite may also apply in which results are evaluated in different contexts from which they are collected
for example data may be collected without crucial external context when facial recognition software is used by surveillance cameras but evaluated by remote staff in another country or region or evaluated by nonhuman algorithms with no awareness of what takes place beyond the cameras field of vision
lastly technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior will correlate
for example software that weighs data points to determine whether a defendant should accept a plea bargain while ignoring the impact of emotion on a jury is displaying a form of technical bias
emergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts
new forms of knowledge such as drug or medical breakthroughs new laws business models or shifting cultural norms may be discovered without algorithms being adjusted to consider them
this may exclude groups through technology without delineating clear outlines of authorship or personal responsibility
similarly problems may emerge when training data i
the samples fed to a machine by which it models certain conclusions do not align with uses that algorithm encounters in the real world
in  an example of emergent bias was identified in the software used to place us medical students into residencies the national residency match program nrmp
the algorithm was designed at a time when few married couples would seek residencies together
as more women entered medical schools more students were likely to request a residency alongside their partners
the process calls for each applicant to provide a list of preferences for placement across the us which is then sorted and assigned when a hospital and an applicant both agree to a match
in the case of married couples where both sought residencies the algorithm weighed a lead members location choices first
once it identified an optimum placement for that person it removed distant locations from their partners preferences reducing their list to the preferred locations within the same city as the partner
the result was a frequent assignment of highrated schools for the first partner and lowerpreference schools to the second partner rather than sorting for compromises in placement preference
additional emergent biases includeunpredictable correlations can emerge when large data sets are compared to each other in practice
for example data collected about webbrowsing patterns may align with signals marking sensitive data such as race or sexual orientation
by discrimination against certain behavior or browsing patterns the end effect would be almost identical to discrimination through the use of direct race or orientation data
in other cases correlations can be inferred for reasons beyond the algorithms ability to understand them as when a triage program gave lower priority to asthmatics who had pneumonia
because asthmatics with pneumonia were at the highest risk hospitals typically give them the best and most immediate care the algorithm simply compared survival rates
emergent bias can occur when an algorithm is used by unanticipated audiences such as machines that demand users can read write or understand numbers
certain metaphors may not carry across different populations or skill sets
for example the british national act program was created as a proofofconcept by computer scientists and immigration lawyers to evaluate suitability for british citizenship
the designers therefore have expertise beyond the user whose understanding of both software and immigration law would likely be unsophisticated
the agents administering the questions would not be aware of alternative pathways to citizenship outside of the software and shifting case law and legal interpretations would lead the algorithm to outdated results
an area of concern around emergent bias is that it may be compounded as biased technology is more deeply integrated into society
for example users with vision impairments may not be able to use an atm but can easily go to a bank branch
if bank branches begin to close because atms replace them they begin to exclude visionimpaired users from banking an unintended consequence of a technology
emergent bias may also create a feedback loop or recursion if data collected for an algorithm results in realworld responses which are fed back into the algorithm
for example simulations of the predictive policing software predpol deployed in oakland california suggested an increased police presence in black neighborhoods based on crime data reported by the public
the simulation showed that public reports of crime could rise based on the sight of increased police activity and could be interpreted by the software in modeling predictions of crime and to encourage a further increase in police presence within the same neighborhoods
the human rights data analysis group which conducted the study warned that in places where racial discrimination is a factor in arrests such feedback loops could reinforce and perpetuate racial discrimination in policing
an early example of algorithmic bias resulted in as many as  women and ethnic minorities denied entry to st
georges hospital medical school per year from  to  based on implementation of a new computerguidance assessment system that denied entry to women and men with foreignsounding names based on historical trends in admissions
other examples include the display of higherpaying jobs to male applicants on job search websites
the plagiarismdetection software turnitin compares studentwritten texts to information found online and returns a probability that the students work is copied
because the software compares strings of text it is more likely to identify nonnative speakers of english than native speakers who might be better able to adapt individual words and break up strings of plagiarized text or obscure them through synonyms
surveillance camera software may be considered inherently political as it requires algorithms to identify and flag normal from abnormal behaviors and to determine who belongs in certain locations at certain times
the ability for such algorithms to recognize faces across a racial spectrum has been shown to be limited by the racial diversity of images in its training database if the majority of photos belong to one race or gender the technology is better at recognizing other members of that race or gender
a  analysis of facial recognition software used to identify individuals in cctv images found several examples of bias when run against criminal databases
the software was assessed as identifying men more frequently than women older people more frequently than younger people and identified asians africanamericans and other races more often than whites
additional studies of facial recognition software have found the opposite to be true when built on noncriminal databases with software offering lowest accuracy rates to darkerskinned females than any other race or gender
in  a facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content according to internal facebook documents
the algorithm which is a blend of computer programs and human content reviewers was created to protect broad categories rather than specific subsets of categories
for example posts denouncing muslims would be blocked while posts denouncing radical muslims would be allowed
an unanticipated outcome of the algorithm is to allow hate speech against black children because they denounce the children subset of blacks rather than all blacks whereas all white men would trigger a block because whites and males are not considered subsets
facebook was also found to allow ad purchasers to target jew haters as a category of users which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data
the companys design also allowed ad buyers to block africanamericans from seeing housing ads
corporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies without the knowledge of a user who may mistake the algorithm as being impartial
for example american airlines created a flightfinding algorithm in the s
the software presented a range of flights from various airlines to customers but weighed factors that boosted its own flights regardless of price or convenience
in testimony to the united states congress the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment
in a  paper describing google the founders of the company adopted a policy of transparency in search results regarding paid placement arguing that advertisingfunded search engines will be inherently biased towards the advertisers and away from the needs of the consumers
this bias would be an invisible manipulation of the user
a series of studies of undecided voters in the us and in india found that search engine results were able to shift voting outcomes by about
the researchers concluded that candidates have no means of competing if an algorithm with or without intent boosted page listings for a rival candidate
facebook users who saw messages related to voting were more likely to vote themselves
a randomized trial of facebook users showing an increased effect of  votes among users and friends of users who saw provoting messages in
the legal scholar jonathan zittrain has warned that this could create a digital gerrymandering effect in elections the selective presentation of information by an intermediary to meet its agenda rather than to serve its users if intentionally manipulated
in  the professional networking site linkedin was discovered to recommend male variations of womens names in response to search queries for women
the site did not make similar recommendations in searches for male names
for example andrea would bring up a prompt asking if users meant andrew but queries for andrew did not ask if users meant to find andrea
the company said this was the result of an analysis of users interactions with the site
in  the department store franchise target was cited for gathering data points to infer when women customers were pregnant even if they hadnt announced it and then sharing that information with marketing partners
because the data had been predicted rather than directly observed or reported the company had no legal obligation to protect the privacy of those customers
web search algorithms have also been accused of bias
googles results may prioritize pornographic content in search terms related to sexuality for example lesbian
this bias extends to the search engine surfacing popular but sexualized content in neutral searches as in top  sexiest women athletes articles displayed as firstpage results in searches for women athletes
in  google announced plans to curb search results that surfaced hate groups racist views child abuse and pornography and other upsetting and offensive content
algorithms have been criticized as a method for obscuring racial prejudices in decisionmaking
lisa nakamura has noted that census machines were among the first to adopt the punchcard processes that lead to contemporary computing and that their use as categorization and sorting machines for race has been long established and socially tolerated
one example is the use of risk assessments in criminal sentencing and parole hearings an algorithmically generated score intended to reflect the risk that a suspect or prisoner will repeat a crime
from  until  the nationality of a suspects father was a consideration in such risk assessments
today these scores are shared with judges in arizona colorado delaware kentucky louisiana oklahoma virginia washington and wisconsin
an independent investigation by propublica found that the scores were inaccurate  of the time and disproportionately skewed to suggest blacks to be at risk of relapse  more often than whites
in  google apologized when black users complained that an imageidentification algorithm in its photos application identified them as gorillas
in  nikon cameras were criticized when imagerecognition algorithms consistently asked asian users if they were blinking
such examples are the product of bias in biometric data sets
biometric data is drawn from aspects of the body including racial features either observed or inferred which can then be transferred into data points
biometric data about race may also be inferred rather than observed
for example a  study showed that names commonly associated with blacks were more likely to yield search results implying arrest records regardless of any police record of that individuals name
in  users of the gay hookup app grindr reported that the app was linked to sexoffender lookup apps in the android app stores recommendation algorithm
writer mike ananny criticized this association in the atlantic arguing that such associations further stigmatized gay men and may discourage closeted men to maintain secrecy
a  incident with online retailer amazon saw  books delisted after a shift in the algorithm expanded its adult content blacklist for pornographic works to any books addressing sexuality or gay themes for example the critically acclaimed novel brokeback mountain
several challenges impede the study of largescale algorithmic bias hindering the application of academically rigorous studies and public understanding
commercial algorithms are proprietary and may be treated as trade secrets
this protects companies such as a search engine in cases where a transparent algorithm for ranking results would reveal techniques for manipulating the service
this makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function
it can also be used to obscure possible unethical methods used in producing or processing algorithmic output
the closed nature of the code is not the only concern however as a certain degree of obscurity is protected by the complexity of contemporary programs and the inability to know every permutations of a codes input or output
social scientist bruno latour has identified this process as blackboxing a process in which scientific and technical work is made invisible by its own success
when a machine runs efficiently when a matter of fact is settled one need focus only on its inputs and outputs and not on its internal complexity
thus paradoxically the more science and technology succeed the more opaque and obscure they become
others have critiqued the black box metaphor suggesting that current algorithms are not one black box but a network of interconnected ones
algorithmic processes are complex often exceeding the understanding of the people who use them
largescale operations may not be understood even by those involved in creating them
the social media site facebook factored in at least  data points to determine the layout of a users social media feed in
furthermore large teams of programmers may operate in relative isolation from one another and be unaware of the cumulative effects of small decisions with nested sections of sprawling algorithmic processes
not all code is original and may be borrowed from other libraries creating a complicated set of relationships between data processing and data input systems
a significant barrier to understanding tackling bias in practice is that categories such as demographics of individuals protected by antidiscrimination law are often not explicitly held by those collecting and processing data
in some cases there is little opportunity to collect this data explicitly such as in device fingerprinting ubiquitous computing and the internet of things
in other cases the data controller may not wish to collect such data for reputational reasons or because it represents a heightened liability and security risk
it may also be the case that at least in relation to the general data protection regulation such data falls under the special category provisions article  and therefore comes with more restrictions on potential collection and processing
algorithmic bias does not only relate to protected categories but can also concern something less easily observable or codifiable such as political viewpoints
in these cases there is rarely an easily accessible or noncontroversial ground truth and debiasing such a system becomes considerably more tricky
furthermore false and accidental correlations can emerge from a lack of understanding of protected categories for example insurance rates based on historical data of car accidents which may overlap with residential clusters of ethnic minorities
personalization of algorithms based on user interactions such as clicks time on site and other metrics can confuse attempts to understand them
one unidentified streaming radio service reported it had five unique musicselection algorithms it selected for its users based on behavior
this creates widely disparate experiences of the same streaming product between different users
companies also run frequent ab tests to finetune algorithms based on user response
for example the search engine bing can run up to ten million subtle variations of its service per day segmenting the experience of an algorithm between users or among the same users
computer programs and systems can quickly spread among users embedding biased algorithms into broader society before their impact can be recognized or remedied
there might be competing motives within an organization for example the availability of loans from a bank as opposed to the banks profit incentive and the banks reputation worry for being caught discriminating an already suppressed minority
a simple algorithm designed with a single purpose such as expanding profits would reduce loans to higherrisk applicants
in order to minimize discrimination between different groups of applicants for ex
a majority group and a minority group a banking algorithm would have to make choices between conflicting strategies such as these maximize its interest in shortterm profit apply the same technical criteria to all applicants irrespective of groups ensure an equal ratio of loan grants for each group of applicants irrespective of group members merits or grant loans to an equal ratio of qualified applicants from each group
the general data protection regulation gdpr the european unions revised data protection regime that enters force in  addresses automated individual decisionmaking including profiling in article
these rules prohibit solely automated decisions which have a significant or legal effect on an individual unless they are explicitly authorised by consent contract or member state law
where they are permitted there must be safeguards in place such as a right to a humanintheloop and allegedly although for political reasons only in a nonbinding recital a right to an explanation of decisions reached
while these are commonly considered to be new it is the case that nearly identical provisions have existed across europe since  in article  of the data protection directive with the original automated decision rules and safeguards originating in french law in the later s
they have rarely been used given the heavy carveouts that exist and are not discussed in any case law of the european court of justice
the gdpr does address algorithmic bias in profiling systems as well as the statistical approaches possible to clean it directly in recital  noting thatthe controller should use appropriate mathematical or statistical procedures for the profiling implement technical and organisational measures appropriate
that prevents inter alia discriminatory effects on natural persons on the basis of racial or ethnic origin political opinion religion or beliefs trade union membership genetic or health status or sexual orientation or that result in measures having such an effect
like the alleged right to an explanation in recital  this suffers from the nonbinding nature of recitals compared to the binding articles and while it has been treated as a requirement by the article  working party that advise on the implementation of data protection law its practical dimensions are unclear
it has been argued that the obligatory data protection impact assessments for high risk data profiling in tandem with other preemptive measures within data protection may be a better way to tackle issues of algorithmic discrimination than relying on individual transparency rights as information rights have traditionally fatigued individuals who are too overwhelmed and overburdened to use them effectively
the united states has no overall legislation regulating controls for algorithmic bias approaching the topic through various state and federal laws that might vary by industries sectors and uses
many policies are selfenforced or controlled by the federal trade commission
in  the obama administration released the national artificial intelligence research and development strategic plan which called for a critical assessment of algorithms and for researchers to design these systems so that their actions and decisionmaking are transparent and easily interpretable by humans and thus can be examined for any bias they may contain rather than just learning and repeating these biases
vgg image annotator via is an open source project developed at the visual geometry group and released under the bsd clause license
with this standalone application you can define regions in an image and create a textual description of those regions
such image regions and descriptions are useful for supervised training of machine learning algorithms
automated machine learning automl is the process of automating the endtoend process of applying machine learning to realworld problems
in a typical machine learning application practitioners must apply the appropriate data preprocessing feature engineering feature extraction and feature selection methods that make the dataset amenable for machine learning
following those preprocessing steps practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their final machine learning model
as many of these steps are often beyond the abilities of nonexperts automl was proposed as an artificial intelligencebased solution to the evergrowing challenge of applying machine learning
automating the endtoend process of applying machine learning offers the advantages of producing simpler solutions faster creation of those solutions and models that often outperform models that were designed by hand
automated machine learning can target various stages of the machine learning processsoftware tackling various stages of automl
similarity learning is an area of supervised machine learning in artificial intelligence
it is closely related to regression and classification but the goal is to learn from examples a similarity function that measures how similar or related two objects are
it has applications in ranking in recommendation systems visual identity tracking face verification and speaker verification
there are four common setups for similarity and metric distance learning
a common approach for learning similarity is to model the similarity function as a bilinear form
for example in the case of ranking similarity learning one aims to learn a matrix w that parametrizes the similarity function formula
similarity learning is closely related to distance metric learning
metric learning is the task of learning a distance function over objects
a metric or distance function has to obey four axioms nonnegativity identity of indiscernibles symmetry and subadditivity  triangle inequality
in practice metric learning algorithms ignore the condition of identity of indiscernibles and learn a pseudometric
when the objects formula are vectors in formula then any matrix formula in the symmetric positive semidefinite cone formula defines a distance pseudometric of the space of x through the form formula
when formula is a symmetric positive definite matrix formula is a metric
moreover as any symmetric positive semidefinite matrix formula can be decomposed as formula where formula and formula the distance function formula can be rewritten equivalently formula
the distance formula corresponds to the euclidean distance between the projected feature vectors formula and formula
some wellknown approaches for metric learning include large margin nearest neighbor information theoretic metric learning itml
in statistics the covariance matrix of the data is sometimes used to define a distance metric called mahalanobis distance
similarity learning is used in information retrieval for learning to rank in face verification or face identification and in recommendation systems
also many machine learning approaches rely on some metric
this includes unsupervised learning such as clustering which groups together close or similar objects
it also includes supervised approaches like knearest neighbor algorithm which rely on labels of nearby objects to decide on the label of a new object
metric learning has been proposed as a preprocessing step for many of these approaches metric and similarity learning naively scale quadratically with the dimension of the input space as can easily see when the learned metric has a bilinear form formula
scaling to higher dimensions can be achieved by enforcing a sparseness structure over the matrix model as done with hdsl and with comet
for further information on this topic see the surveys on metric and similarity learning by bellet et al
and kulis
a general framework for metric learning has been proposed by huang et al
in machine learning semantic analysis of a corpus is the task of building structures that approximate concepts from a large set of documents
it generally does not involve prior semantic understanding of the documents
latent semantic analysis sometimes latent semantic indexing is a class of techniques where documents are represented as vectors in term space
a prominent example is plsi
latent dirichlet allocation involves attributing document terms to topics
ngrams and hidden markov models work by representing the term stream as a markov chain where each term is derived from the few terms before it
semantic folding theory describes a procedure for encoding the semantics of natural language text in a semantically grounded binary representation
this approach provides a framework for modelling how language data is processed by the neocortex
semantic folding theory draws inspiration from douglas r
hofstadters analogy as the core of cognition which suggests that the brain makes sense of the world by identifying and applying analogies
the theory hypothesises that semantic data must therefore be introduced to the neocortex in such a form as to allow the application of a similarity measure and offers as a solution the sparse binary vector employing a twodimensional topographic semantic space as a distributional reference frame
the theory builds on the computational theory of the human cortex known as hierarchical temporal memory htm and positions itself as a complementary theory for the representation of language semantics
a particular strength claimed by this approach is that the resulting binary representation enables complex semantic operations to be performed simply and efficiently at the most basic computational level
analogous to the structure of the neocortex semantic folding theory posits the implementation of a semantic space as a twodimensional grid
this grid is populated by contextvectors in such a way as to place similar contextvectors closer to each other for instance by using competitive learning principles
this vector space model is presented in the theory as an equivalence to the well known word space model described in the information retrieval literature
given a semantic space implemented as described above a wordvector can be obtained for any given word y by employing the following algorithmfor each position x in the semantic map where x represents cartesian coordinatesthe result of this process will be a wordvector containing all the contexts in which the word y appears and will therefore be representative of the semantics of that word in the semantic space
it can be seen that the resulting wordvector is also in a sparse distributed representation sdr format schtze   sahlgreen
some properties of wordsdrs that are of particular interest with respect to computational semantics aresemantic spaces in the natural language domain aim to create representations of natural language that are capable of capturing meaning
the original motivation for semantic spaces stems from two core challenges of natural language vocabulary mismatch the fact that the same meaning can be expressed in many ways and ambiguity of natural language the fact that the same term can have several meanings
the application of semantic spaces in natural language processing nlp aims at overcoming limitations of rulebased or modelbased approaches operating on the keyword level
the main drawback with these approaches is their brittleness and the large manual effort required to create either rulebased nlp systems or training corpora for model learning
rulebased and machine learning based models are fixed on the keyword level and break down if the vocabulary differs from that defined in the rules or from the training material used for the statistical models
research in semantic spaces dates back more than  years
in  two papers were published that raised a lot of attention around the general idea of creating semantic spaces latent semantic analysis from microsoft and hyperspace analogue to language from the university of california
however their adoption was limited by the large computational effort required to construct and use those semantic spaces
a breakthrough with regard to the accuracy of modelling associative relations between words e
spiderweb lightercigarette as opposed to synonymous relations such as whaledolphin astronautdriver was achieved by explicit semantic analysis esa in
esa was a novel nonmachine learning based approach that represented words in the form of vectors with  dimensions where each dimension represents an article in wikipedia
however practical applications of the approach are limited due to the large number of required dimensions in the vectors
more recently advances in neural networking techniques in combination with other new approaches tensors led to a host of new recent developments wordvec from google and glove from stanford university
semantic folding represents a novel biologically inspired approach to semantic spaces where each word is represented as a sparse binary vector with  dimensions a semantic fingerprint in a d semantic map the semantic universe
sparse binary representation are advantageous in terms of computational efficiency and allow for the storage of very large numbers of possible patterns
the topological distribution over a twodimensional grid outlined above lends itself to a bitmap type visualization of the semantics of any word or text where each active semantic feature can be displayed as e
a pixel
as can be seen in the images shown here this representation allows for a direct visual comparison of the semantics of two or more linguistic items
image  clearly demonstrates that the two disparate terms dog and car have as expected very obviously different semantics
image  shows that only one of the meaning contexts of jaguar that of jaguar the car overlaps with the meaning of porsche indicating partial similarity
other meaning contexts of jaguar e
jaguar the animal clearly have different nonoverlapping contexts
note also that the visualization of semantic similarity using semantic folding bears a strong resemblance to the fmri images produced in a research study conducted by a
huth et al
where it is claimed that words are grouped in the brain by meaning
stability also known as algorithmic stability is a notion in computational learning theory of how a machine learning algorithm is perturbed by small changes to its inputs
a stable learning algorithm is one for which the prediction does not change much when the training data is modified slightly
for instance consider a machine learning algorithm that is being trained to recognize handwritten letters of the alphabet using  examples of handwritten letters and their labels a to z as a training set
one way to modify this training set is to leave out an example so that only  examples of handwritten letters and their labels are available
a stable learning algorithm would produce a similar classifier with both the element and element training sets
stability can be studied for many types of learning problems from language learning to inverse problems in physics and engineering as it is a property of the learning process rather than the type of information being learned
the study of stability gained importance in computational learning theory in the s when it was shown to have a connection with generalization
it was shown that for large classes of learning algorithms notably empirical risk minimization algorithms certain types of stability ensure good generalization
a central goal in designing a machine learning system is to guarantee that the learning algorithm will generalize or perform accurately on new examples after being trained on a finite number of them
in the s milestones were reached in obtaining generalization bounds for supervised learning algorithms
the technique historically used to prove generalization was to show that an algorithm was consistent using the uniform convergence properties of empirical quantities to their means
this technique was used to obtain generalization bounds for the large class of empirical risk minimization erm algorithms
an erm algorithm is one that selects a solution from a hypothesis space formula in such a way to minimize the empirical error on a training set formula
a general result proved by vladimir vapnik for an erm binary classification algorithms is that for any target function and input distribution any hypothesis space formula with vcdimension formula and formula training examples the algorithm is consistent and will produce a training error that is at most formula plus logarithmic factors from the true training error
the result was later extended to almosterm algorithms with function classes that do not have unique minimizers
vapniks work using what became known as vc theory established a relationship between generalization of a learning algorithm and properties of the hypothesis space formula of functions being learned
however these results could not be applied to algorithms with hypothesis spaces of unbounded vcdimension
put another way these results could not be applied when the information being learned had a complexity that was too large to measure
some of the simplest machine learning algorithmsfor instance for regressionhave hypothesis spaces with unbounded vcdimension
another example is language learning algorithms that can produce sentences of arbitrary length
stability analysis was developed in the s for computational learning theory and is an alternative method for obtaining generalization bounds
the stability of an algorithm is a property of the learning process rather than a direct property of the hypothesis space formula and it can be assessed in algorithms that have hypothesis spaces with unbounded or undefined vcdimension such as nearest neighbor
a stable learning algorithm is one for which the learned function does not change much when the training set is slightly modified for instance by leaving out an example
a measure of leave one out error is used in a cross validation leave one out cvloo algorithm to evaluate a learning algorithms stability with respect to the loss function
as such stability analysis is the application of sensitivity analysis to machine learning
we define several terms related to learning algorithms training sets so that we can then define stability in multiple ways and present theorems from the field
a machine learning algorithm also known as a learning map formula maps a training data set which is a set of labeled examples formula onto a function formula from formula to formula where formula and formula are in the same space of the training examples
the functions formula are selected from a hypothesis space of functions called formula
the training set from which an algorithm learns is defined asformulaand is of size formula in formuladrawn i
from an unknown distribution d
thus the learning map formula is defined as a mapping from formula into formula mapping a training set formula onto a function formula from formula to formula
here we consider only deterministic algorithms where formula is symmetric with respect to formula i
it does not depend on the order of the elements in the training set
furthermore we assume that all functions are measurable and all sets are countable
the loss formula of a hypothesis formula with respect to an example formula is then defined as formula
the empirical error of formula is formula
the true error of formula is formulagiven a training set s of size m we will build for all i
m modified training sets as followsformulaformulaan algorithm formula has hypothesis stability  with respect to the loss function v if the following holdsformulaan algorithm formula has pointwise hypothesis stability  with respect to the loss function v if the following holdsformulaan algorithm formula has error stability  with respect to the loss function v if the following holdsformulaan algorithm formula has uniform stability  with respect to the loss function v if the following holdsformulaa probabilistic version of uniform stability  isformulaan algorithm is said to be stable when the value of formula decreases as formula
an algorithm formula has cvloo stability  with respect to the loss function v if the following holdsformulathe definition of cvloo stability is equivalent to pointwisehypothesis stability seen earlier
an algorithm formula has formula stability if for each n there exists a formula and a formula such thatformula with formula and formula going to zero for formulafrom bousquet and elisseeff for symmetric learning algorithms with bounded loss if the algorithm has uniform stability with the probabilistic definition above then the algorithm generalizes
uniform stability is a strong condition which is not met by all algorithms but is surprisingly met by the large and important class of regularization algorithms
the generalization bound is given in the article
from mukherjee et al
this is an important result for the foundations of learning theory because it shows that two previously unrelated properties of an algorithm stability and consistency are equivalent for erm and certain loss functions
the generalization bound is given in the article
this is a list of algorithms that have been shown to be stable and the article where the associated generalization bounds are provided
predictive learning is a technique of machine learning in which an agent tries to build a model of its environment by trying out different actions in various circumstances
it uses knowledge of the effects its actions appear to have turning them into planning operators
these allow the agent to act purposefully in its world
predictive learning is one attempt to learn with a minimum of preexisting mental structure
it may have been inspired by piagets account of how children construct knowledge of the world by interacting with it
gary dreschers book madeup minds was seminal for the area
another more recent predictive learning theory is jeff hawkins memoryprediction framework which is laid out in his on intelligence
in computer science and operations research a genetic algorithm ga is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms ea
genetic algorithms are commonly used to generate highquality solutions to optimization and search problems by relying on bioinspired operators such as mutation crossover and selection
in a genetic algorithm a population of candidate solutions called individuals creatures or phenotypes to an optimization problem is evolved toward better solutions
each candidate solution has a set of properties its chromosomes or genotype which can be mutated and altered traditionally solutions are represented in binary as strings of s and s but other encodings are also possible
the evolution usually starts from a population of randomly generated individuals and is an iterative process with the population in each iteration called a generation
in each generation the fitness of every individual in the population is evaluated the fitness is usually the value of the objective function in the optimization problem being solved
the more fit individuals are stochastically selected from the current population and each individuals genome is modified recombined and possibly randomly mutated to form a new generation
the new generation of candidate solutions is then used in the next iteration of the algorithm
commonly the algorithm terminates when either a maximum number of generations has been produced or a satisfactory fitness level has been reached for the population
a typical genetic algorithm requiresa standard representation of each candidate solution is as an array of bits
arrays of other types and structures can be used in essentially the same way
the main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size which facilitates simple crossover operations
variable length representations may also be used but crossover implementation is more complex in this case
treelike representations are explored in genetic programming and graphform representations are explored in evolutionary programming a mix of both linear chromosomes and trees is explored in gene expression programming
once the genetic representation and the fitness function are defined a ga proceeds to initialize a population of solutions and then to improve it through repetitive application of the mutation crossover inversion and selection operators
the population size depends on the nature of the problem but typically contains several hundreds or thousands of possible solutions
often the initial population is generated randomly allowing the entire range of possible solutions the search space
occasionally the solutions may be seeded in areas where optimal solutions are likely to be found
during each successive generation a portion of the existing population is selected to breed a new generation
individual solutions are selected through a fitnessbased process where fitter solutions as measured by a fitness function are typically more likely to be selected
certain selection methods rate the fitness of each solution and preferentially select the best solutions
other methods rate only a random sample of the population as the former process may be very timeconsuming
the fitness function is defined over the genetic representation and measures the quality of the represented solution
the fitness function is always problem dependent
for instance in the knapsack problem one wants to maximize the total value of objects that can be put in a knapsack of some fixed capacity
a representation of a solution might be an array of bits where each bit represents a different object and the value of the bit  or  represents whether or not the object is in the knapsack
not every such representation is valid as the size of objects may exceed the capacity of the knapsack
the fitness of the solution is the sum of values of all objects in the knapsack if the representation is valid or  otherwise
in some problems it is hard or even impossible to define the fitness expression in these cases a simulation may be used to determine the fitness function value of a phenotype e
computational fluid dynamics is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype or even interactive genetic algorithms are used
the next step is to generate a second generation population of solutions from those selected through a combination of genetic operators crossover also called recombination and mutation
for each new solution to be produced a pair of parent solutions is selected for breeding from the pool selected previously
by producing a child solution using the above methods of crossover and mutation a new solution is created which typically shares many of the characteristics of its parents
new parents are selected for each new child and the process continues until a new population of solutions of appropriate size is generated
although reproduction methods that are based on the use of two parents are more biology inspired some research suggests that more than two parents generate higher quality chromosomes
these processes ultimately result in the next generation population of chromosomes that is different from the initial generation
generally the average fitness will have increased by this procedure for the population since only the best organisms from the first generation are selected for breeding along with a small proportion of less fit solutions
these less fit solutions ensure genetic diversity within the genetic pool of the parents and therefore ensure the genetic diversity of the subsequent generation of children
opinion is divided over the importance of crossover versus mutation
there are many references in fogel  that support the importance of mutationbased search
although crossover and mutation are known as the main genetic operators it is possible to use other operators such as regrouping colonizationextinction or migration in genetic algorithms
it is worth tuning parameters such as the mutation probability crossover probability and population size to find reasonable settings for the problem class being worked on
a very small mutation rate may lead to genetic drift which is nonergodic in nature
a recombination rate that is too high may lead to premature convergence of the genetic algorithm
a mutation rate that is too high may lead to loss of good solutions unless elitist selection is employed
in addition to the main operators above other heuristics may be employed to make the calculation faster or more robust
the speciation heuristic penalizes crossover between candidate solutions that are too similar this encourages population diversity and helps prevent premature convergence to a less optimal solution
this generational process is repeated until a termination condition has been reached
common terminating conditions aregenetic algorithms are simple to implement but their behavior is difficult to understand
in particular it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems
the building block hypothesis bbh consists ofgoldberg describes the heuristic as followsdespite the lack of consensus regarding the validity of the buildingblock hypothesis it has been consistently evaluated and used as reference throughout the years
many estimation of distribution algorithms for example have been proposed in an attempt to provide an environment in which the hypothesis would hold
although good results have been reported for some classes of problems skepticism concerning the generality andor practicality of the buildingblock hypothesis as an explanation for gas efficiency still remains
indeed there is a reasonable amount of work that attempts to understand its limitations from the perspective of estimation of distribution algorithms
there are limitations of the use of a genetic algorithm compared to alternative optimization algorithmsthe simplest algorithm represents each chromosome as a bit string
typically numeric parameters can be represented by integers though it is possible to use floating point representations
the floating point representation is natural to evolution strategies and evolutionary programming
the notion of realvalued genetic algorithms has been offered but is really a misnomer because it does not really represent the building block theory that was proposed by john henry holland in the s
this theory is not without support though based on theoretical and experimental results see below
the basic algorithm performs crossover and mutation at the bit level
other variants treat the chromosome as a list of numbers which are indexes into an instruction table nodes in a linked list hashes objects or any other imaginable data structure
crossover and mutation are performed so as to respect data element boundaries
for most data types specific variation operators can be designed
different chromosomal data types seem to work better or worse for different specific problem domains
when bitstring representations of integers are used gray coding is often employed
in this way small changes in the integer can be readily affected through mutations or crossovers
this has been found to help prevent premature convergence at so called hamming walls in which too many simultaneous mutations or crossover events must occur in order to change the chromosome to a better solution
other approaches involve using arrays of realvalued numbers instead of bit strings to represent chromosomes
results from the theory of schemata suggest that in general the smaller the alphabet the better the performance but it was initially surprising to researchers that good results were obtained from using realvalued chromosomes
this was explained as the set of real values in a finite population of chromosomes as forming a virtual alphabet when selection and recombination are dominant with a much lower cardinality than would be expected from a floating point representation
an expansion of the genetic algorithm accessible problem domain can be obtained through more complex encoding of the solution pools by concatenating several types of heterogenously encoded genes into one chromosome
this particular approach allows for solving optimization problems that require vastly disparate definition domains for the problem parameters
for instance in problems of cascaded controller tuning the internal loop controller structure can belong to a conventional regulator of three parameters whereas the external loop could implement a linguistic controller such as a fuzzy system which has an inherently different description
this particular form of encoding requires a specialized crossover mechanism that recombines the chromosome by section and it is a useful tool for the modelling and simulation of complex adaptive systems especially evolution processes
a practical variant of the general process of constructing a new population is to allow the best organisms from the current generation to carry over to the next unaltered
this strategy is known as elitist selection and guarantees that the solution quality obtained by the ga will not decrease from one generation to the next
parallel implementations of genetic algorithms come in two flavors
coarsegrained parallel genetic algorithms assume a population on each of the computer nodes and migration of individuals among the nodes
finegrained parallel genetic algorithms assume an individual on each processor node which acts with neighboring individuals for selection and reproduction
other variants like genetic algorithms for online optimization problems introduce timedependence or noise in the fitness function
genetic algorithms with adaptive parameters adaptive genetic algorithms agas is another significant and promising variant of genetic algorithms
the probabilities of crossover pc and mutation pm greatly determine the degree of solution accuracy and the convergence speed that genetic algorithms can obtain
instead of using fixed values of pc and pm agas utilize the population information in each generation and adaptively adjust the pc and pm in order to maintain the population diversity as well as to sustain the convergence capacity
in aga adaptive genetic algorithm the adjustment of pc and pm depends on the fitness values of the solutions
in caga clusteringbased adaptive genetic algorithm through the use of clustering analysis to judge the optimization states of the population the adjustment of pc and pm depends on these optimization states
it can be quite effective to combine ga with other optimization methods
ga tends to be quite good at finding generally good global solutions but quite inefficient at finding the last few mutations to find the absolute optimum
other techniques such as simple hill climbing are quite efficient at finding absolute optimum in a limited region
alternating ga and hill climbing can improve the efficiency of ga while overcoming the lack of robustness of hill climbing
this means that the rules of genetic variation may have a different meaning in the natural case
for instance  provided that steps are stored in consecutive order  crossing over may sum a number of steps from maternal dna adding a number of steps from paternal dna and so on
this is like adding vectors that more probably may follow a ridge in the phenotypic landscape
thus the efficiency of the process may be increased by many orders of magnitude
moreover the inversion operator has the opportunity to place steps in consecutive order or any other suitable order in favour of survival or efficiency
a variation where the population as a whole is evolved rather than its individual members is known as gene pool recombination
a number of variations have been developed to attempt to improve performance of gas on problems with a high degree of fitness epistasis i
where the fitness of a solution consists of interacting subsets of its variables
such algorithms aim to learn before exploiting these beneficial phenotypic interactions
as such they are aligned with the building block hypothesis in adaptively reducing disruptive recombination
prominent examples of this approach include the mga gemga and llga
problems which appear to be particularly appropriate for solution by genetic algorithms include timetabling and scheduling problems and many scheduling software packages are based on gas
gas have also been applied to engineering
genetic algorithms are often applied as an approach to solve global optimization problems
as a general rule of thumb genetic algorithms might be useful in problem domains that have a complex fitness landscape as mixing i
mutation in combination with crossover is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in
observe that commonly used crossover operators cannot change any uniform population
mutation alone can provide ergodicity of the overall genetic algorithm process seen as a markov chain
examples of problems solved by genetic algorithms include mirrors designed to funnel sunlight to a solar collector antennae designed to pick up radio signals in space and walking methods for computer figures
in his algorithm design manual skiena advises against genetic algorithms for any taskin  alan turing proposed a learning machine which would parallel the principles of evolution
computer simulation of evolution started as early as in  with the work of nils aall barricelli who was using the computer at the institute for advanced study in princeton new jersey
his  publication was not widely noticed
starting in  the australian quantitative geneticist alex fraser published a series of papers on simulation of artificial selection of organisms with multiple loci controlling a measurable trait
from these beginnings computer simulation of evolution by biologists became more common in the early s and the methods were described in books by fraser and burnell  and crosby
frasers simulations included all of the essential elements of modern genetic algorithms
in addition hansjoachim bremermann published a series of papers in the s that also adopted a population of solution to optimization problems undergoing recombination mutation and selection
bremermanns research also included the elements of modern genetic algorithms
other noteworthy early pioneers include richard friedberg george friedman and michael conrad
many early papers are reprinted by fogel
although barricelli in work he reported in  had simulated the evolution of ability to play a simple game artificial evolution became a widely recognized optimization method as a result of the work of ingo rechenberg and hanspaul schwefel in the s and early s  rechenbergs group was able to solve complex engineering problems through evolution strategies
another approach was the evolutionary programming technique of lawrence j
fogel which was proposed for generating artificial intelligence
evolutionary programming originally used finite state machines for predicting environments and used variation and selection to optimize the predictive logics
genetic algorithms in particular became popular through the work of john holland in the early s and particularly his book adaptation in natural and artificial systems
his work originated with studies of cellular automata conducted by holland and his students at the university of michigan
holland introduced a formalized framework for predicting the quality of the next generation known as hollands schema theorem
research in gas remained largely theoretical until the mids when the first international conference on genetic algorithms was held in pittsburgh pennsylvania
in the late s general electric started selling the worlds first genetic algorithm product a mainframebased toolkit designed for industrial processes
in  axcelis inc
released evolver the worlds first commercial ga product for desktop computers
the new york times technology writer john markoff wrote about evolver in  and it remained the only interactive commercial genetic algorithm until
evolver was sold to palisade in  translated into several languages and is currently in its th version
genetic algorithms are a subfield ofevolutionary algorithms is a subfield of evolutionary computing
swarm intelligence is a subfield of evolutionary computing
evolutionary computation is a subfield of the metaheuristic methods
metaheuristic methods broadly fall within stochastic optimisation methods
proactive learning is a generalization of active learning designed to relax unrealistic assumptions and thereby reach practical applications
active learning seeks to select the most informative unlabeled instances and ask an omniscient oracle for their labels so as to retrain a learning algorithm maximizing accuracy
however the oracle is assumed to be infallible never wrong indefatigable always answers individual only one oracle and insensitive to costs always free or always charges the same
in real life it is possible and more general to have multiple sources of information with differing reliabilities or areas of expertise
active learning also assumes that the single oracle is perfect always providing a correct answer when requested
in reality though an oracle if we generalize the term to mean any source of expert information may be incorrect fallible with a probability that should be a function of the difficulty of the question
moreover an oracle may be reluctant  it may refuse to answer if it is too uncertain or too busy
finally active learning presumes the oracle is either free or charges uniform cost in label elicitation
such an assumption is naive since cost is likely to be regulated by difficulty amount of work required to formulate an answer or other factors
proactive learning relaxes all four of these assumptions relying on a decisiontheoretic approach to jointly select the optimal oracle and instance by casting the problem as a utility optimization problem subject to a budget constraint
algorithmic inference gathers new developments in the statistical inference methods made feasible by the powerful computing devices widely available to any data analyst
cornerstones in this field are computational learning theory granular computing bioinformatics and long ago structural probability
the main focus is on the algorithms which compute statistics rooting the study of a random phenomenon along with the amount of data they must feed on to produce reliable results
this shifts the interest of mathematicians from the study of the distribution laws to the functional properties of the statistics and the interest of computer scientists from the algorithms for processing data to the information they process
concerning the identification of the parameters of a distribution law the mature reader may recall lengthy disputes in the mid th century about the interpretation of their variability in terms of fiducial distribution  structural probabilities  priorsposteriors  and so on
from an epistemology viewpoint this entailed a companion dispute as to the nature of probability is it a physical feature of phenomena to be described through random variables or a way of synthesizing data about a phenomenon opting for the latter fisher defines a fiducial distribution law of parameters of a given random variable that he deduces from a sample of its specifications
with this law he computes for instance the probability that  mean of a gaussian variable  our note is less than any assigned value or the probability that it lies between any assigned values or in short its probability distribution in the light of the sample observed
fisher fought hard to defend the difference and superiority of his notion of parameter distribution in comparison to analogous notions such as bayes posterior distribution frasers constructive probability and neymans confidence intervals
for half a century neymans confidence intervals won out for all practical purposes crediting the phenomenological nature of probability
with this perspective when you deal with a gaussian variable its mean  is fixed by the physical features of the phenomenon you are observing where the observations are random operators hence the observed values are specifications of a random sample
because of their randomness you may compute from the sample specific intervals containing the fixed  with a given probability that you denote confidence
let x be a gaussian variable with parameters formula and formula and formula a sample drawn from it
working with statisticsandis the sample mean we recognize thatfollows a students t distribution with parameter degrees of freedom m so thatgauging t between two quantiles and inverting its expression as a function of formula you obtain confidence intervals for formula
with the sample specificationhaving size m   you compute the statistics formula and formula and obtain a
confidence interval for formula with extremes
from a modeling perspective the entire dispute looks like a chickenegg dilemma either fixed data by first and probability distribution of their properties as a consequence or fixed properties by first and probability distribution of the observed data as a corollary
the classic solution has one benefit and one drawback
the former was appreciated particularly back when people still did computations with sheet and pencil
per se the task of computing a neyman confidence interval for the fixed parameter  is hard you dont know  but you look for disposing around it an interval with a possibly very low probability of failing
the analytical solution is allowed for a very limited number of theoretical cases
vice versa a large variety of instances may be quickly solved in an approximate way via the central limit theorem in terms of confidence interval around a gaussian distribution  thats the benefit
the drawback is that the central limit theorem is applicable when the sample size is sufficiently large
therefore it is less and less applicable with the sample involved in modern inference instances
the fault is not in the sample size on its own part
rather this size is not sufficiently large because of the complexity of the inference problem
with the availability of large computing facilities scientists refocused from isolated parameters inference to complex functions inference i
re sets of highly nested parameters identifying functions
in these cases we speak about learning of functions in terms for instance of regression neurofuzzy system or computational learning on the basis of highly informative samples
a first effect of having a complex structure linking data is the reduction of the number of sample degrees of freedom i
the burning of a part of sample points so that the effective sample size to be considered in the central limit theorem is too small
focusing on the sample size ensuring a limited learning error with a given confidence level the consequence is that the lower bound on this size grows with complexity indices such as vc dimension or detail of a class to which the function we want to learn belongs
a sample of  independent bits is enough to ensure an absolute error of at most
on the estimation of the parameter p of the underlying bernoulli variable with a confidence of at least
the same size cannot guarantee a threshold less than
with the same confidence
when the error is identified with the probability that a yearold man living in new york does not fit the ranges of height weight and waistline observed on  big apple inhabitants
the accuracy shortage occurs because both the vc dimension and the detail of the class of parallelepipeds among which the one observed from the  inhabitants ranges falls are equal to
with insufficiently large samples the approach fixed sample  random properties suggests inference procedures in three stepsfor a random variable and a sample drawn from it a compatible distribution is a distribution having the same sampling mechanism formula of x with a value formula of the random parameter formula derived from a master equation rooted on a wellbehaved statistic s
you may find the distribution law of the pareto parameters aand kas an implementation example of the population bootstrapmethod as in the figure on the left
implementing the twisting argumentmethod you get the distribution law formulaof the mean mof a gaussian variable xon the basis of the statistic formulawhen formulais known to be equal to formula
its expression isshown in the figure on the right where formula is the cumulative distribution function of a standard normal distribution
the achilles heel of fishers approach lies in the joint distribution of more than one parameter say mean and variance of a gaussian distribution
on the contrary with the last approach and abovementioned methods population bootstrap and twisting argument we may learn the joint distribution of many parameters
for instance focusing on the distribution of two or many more parameters in the figures below we report two confidence regions where the function to be learnt falls with a confidence of
the former concerns the probability with which an extended support vector machine attributes a binary label  to the points of the formula plane
the two surfaces are drawn on the basis of a set of sample points in turn labelled according to a specific distribution law
the latter concerns the confidence region of the hazard rate of breast cancer recurrence computed from a censored sample
in machine learning local casecontrol sampling is an algorithm used to reduce the complexity of training a logistic regression classifier
the algorithm reduces the training complexity by selecting a small subsample of the original dataset for training
it assumes the availability of a unreliable pilot estimation of the parameters
it then performs a single pass over the entire dataset using the pilot estimation to identify the most surprising samples
in practice the pilot may come from prior knowledge or training using a subsample of the dataset
the algorithm is most effective when the underlying dataset is imbalanced
it exploits the structures of conditional imbalanced datasets more efficiently than alternative methods such as case control sampling and weighted case control sampling
in classification a dataset is a set of n data points formula where formula is a feature vector formula is a label
intuitively a dataset is imbalanced when certain important statistical patterns are rare
the lack of observations of certain patterns does not always imply their irrelevance
for example in medical studies of rare diseases the small number of infected patients cases conveys the most valuable information for diagnosis and treatments
formally an imbalanced dataset exhibits one or more of the following propertiesin logistic regression given the model formula the prediction is made according to formula
the localcase control sampling algorithm assumes the availability of a pilot model formula
given the pilot model the algorithm performs a single pass over the entire dataset to select the subset of samples to include in training the logistic regression model
for a sample formula define the acceptance probability as formula
the algorithm proceeds as followsthe algorithm can be understood as selecting samples that surprises the pilot model
intuitively these samples are closer to the decision boundary of the classifier and is thus more informative
in practice for cases where a pilot model is naturally available the algorithm can be applied directly to reduce the complexity of training
in cases where a natural pilot is nonexistent an estimate using a subsample selected through another sampling technique can be used instead
in the original paper describing the algorithm the authors propose to use weighted casecontrol sampling with half the assigned sampling budget
for example if the objective is to use a subsample with size formula first estimate a model formula using formula samples from weighted case control sampling then collect another formula samples using local casecontrol sampling
it is possible to control the sample size by multiplying the acceptance probability with a constant formula
for a larger sample size pick formula and adjust the acceptance probability to formula
for a smaller sample size the same strategy applies
in cases where the number of samples desired is precise a convenient alternative method is to uniformly downsample from a larger subsample selected by local casecontrol sampling
the algorithm has the following properties
when the pilot is consistent the estimates using the samples from local casecontrol sampling is consistent even under model misspecification
if the model is correct then the algorithm has exactly twice the asymptotic variance of logistic regression on the full data set
for a larger sample size with formula the factor  is improved to formula
in machine learning feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data
this replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task
feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process
however realworld data such as images video and sensor data has not yielded to attempts to algorithmically define specific features
an alternative is to discover such features or representations through examination without relying on explicit algorithms
feature learning can be either supervised or unsupervised
supervised feature learning is learning features from labeled data
the data label allows the system to compute an error term the degree to which the system fails to produce the label which can then be used as feedback to correct the learning process reduceminimize the error
approaches includedictionary learning develops a set dictionary of representative elements from the input data such that each data point can be represented as a weighted sum of the representative elements
the dictionary elements and the weights may be found by minimizing the average representation error over the input data together with l regularization on the weights to enable sparsity i
the representation of each data point has only a few nonzero weights
supervised dictionary learning exploits both the structure underlying the input data and the labels for optimizing the dictionary elements
for example a supervised dictionary learning technique applied dictionary learning on classification problems by jointly optimizing the dictionary elements weights for representing data points and parameters of the classifier based on the input data
in particular a minimization problem is formulated where the objective function consists of the classification error the representation error an l regularization on the representing weights for each data point to enable sparse representation of data and an l regularization on the parameters of the classifier
neural networks are a family of learning algorithms that use a network consisting of multiple layers of interconnected nodes
it is inspired by the animal nervous system where the nodes are viewed as neurons and edges are viewed as synapses
each edge has an associated weight and the network defines computational rules for passing input data from the networks input layer to the output layer
a network function associated with a neural network characterizes the relationship between input and output layers which is parameterized by the weights
with appropriately defined network functions various learning tasks can be performed by minimizing a cost function over the network function weights
multilayer neural networks can be used to perform feature learning since they learn a representation of their input at the hidden layers which is subsequently used for classification or regression at the output layer
unsupervised feature learning is learning features from unlabeled data
the goal of unsupervised feature learning is often to discover lowdimensional features that captures some structure underlying the highdimensional input data
when the feature learning is performed in an unsupervised way it enables a form of semisupervised learning where features learned from an unlabeled dataset are then employed to improve performance in a supervised setting with labeled data
several approaches are introduced in the following
kmeans clustering is an approach for vector quantization
in particular given a set of n vectors kmeans clustering groups them into k clusters i
subsets in such a way that each vector belongs to the cluster with the closest mean
the problem is computationally nphard although suboptimal greedy algorithms have been developed
kmeans clustering can be used to group an unlabeled set of inputs into k clusters and then use the centroids of these clusters to produce features
these features can be produced in several ways
the simplest is to add k binary features to each sample where each feature j has value one iff the jth centroid learned by kmeans is the closest to the sample under consideration
it is also possible to use the distances to the clusters as features perhaps after transforming them through a radial basis function a technique that has been used to train rbf networks
coates and ng note that certain variants of kmeans behave similarly to sparse coding algorithms
in a comparative evaluation of unsupervised feature learning methods coates lee and ng found that kmeans clustering with an appropriate transformation outperforms the more recently invented autoencoders and rbms on an image classification task
kmeans also improves performance in the domain of nlp specifically for namedentity recognition there it competes with brown clustering as well as with distributed word representations also known as neural word embeddings
principal component analysis pca is often used for dimension reduction
given an unlabeled set of n input data vectors pca generates p which is much smaller than the dimension of the input data right singular vectors corresponding to the p largest singular values of the data matrix where the kth row of the data matrix is the kth input data vector shifted by the sample mean of the input i
subtracting the sample mean from the data vector
equivalently these singular vectors are the eigenvectors corresponding to the p largest eigenvalues of the sample covariance matrix of the input vectors
these p singular vectors are the feature vectors learned from the input data and they represent directions along which the data has the largest variations
pca is a linear feature learning approach since the p singular vectors are linear functions of the data matrix
the singular vectors can be generated via a simple algorithm with p iterations
in the ith iteration the projection of the data matrix on the ith eigenvector is subtracted and the ith singular vector is found as the right singular vector corresponding to the largest singular of the residual data matrix
pca has several limitations
first it assumes that the directions with large variance are of most interest which may not be the case
pca only relies on orthogonal transformations of the original data and it exploits only the first and secondorder moments of the data which may not well characterize the data distribution
furthermore pca can effectively reduce dimension only when the input data vectors are correlated which results in a few dominant eigenvalues
local linear embedding lle is a nonlinear learning approach for generating lowdimensional neighborpreserving representations from unlabeled highdimension input
the approach was proposed by roweis and saul
the general idea of lle is to reconstruct the original highdimensional data using lowerdimensional points while maintaining some geometric properties of the neighborhoods in the original data set
lle consists of two major steps
the first step is for neighborpreserving where each input data point xi is reconstructed as a weighted sum of k nearest neighbor data points and the optimal weights are found by minimizing the average squared reconstruction error i
difference between an input point and its reconstruction under the constraint that the weights associated with each point sum up to one
the second step is for dimension reduction by looking for vectors in a lowerdimensional space that minimizes the representation error using the optimized weights in the first step
note that in the first step the weights are optimized with fixed data which can be solved as a least squares problem
in the second step lowerdimensional points are optimized with fixed weights which can be solved via sparse eigenvalue decomposition
the reconstruction weights obtained in the first step capture the intrinsic geometric properties of a neighborhood in the input data
it is assumed that original data lie on a smooth lowerdimensional manifold and the intrinsic geometric properties captured by the weights of the original data are also expected to be on the manifold
this is why the same weights are used in the second step of lle
compared with pca lle is more powerful in exploiting the underlying data structure
independent component analysis ica is a technique for forming a data representation using a weighted sum of independent nongaussian components
the assumption of nongaussian is imposed since the weights cannot be uniquely determined when all the components follow gaussian distribution
unsupervised dictionary learning does not utilize data labels and exploits the structure underlying the data for optimizing dictionary elements
an example of unsupervised dictionary learning is sparse coding which aims to learn basis functions dictionary elements for data representation from unlabeled input data
sparse coding can be applied to learn overcomplete dictionaries where the number of dictionary elements is larger than the dimension of the input data
aharon et al
proposed algorithm ksvd for learning a dictionary of elements that enables sparse representation
the hierarchical architecture of the biological neural system inspires deep learning architectures for feature learning by stacking multiple layers of learning nodes
these architectures are often designed based on the assumption of distributed representation observed data is generated by the interactions of many different factors on multiple levels
in a deep learning architecture the output of each intermediate layer can be viewed as a representation of the original input data
each level uses the representation produced by previous level as input and produces new representations as output which is then fed to higher levels
the input at the bottom layer is raw data and the output of the final layer is the final lowdimensional feature or representation
restricted boltzmann machines rbms are often used as a building block for multilayer learning architectures
an rbm can be represented by an undirected bipartite graph consisting of a group of binary hidden variables a group of visible variables and edges connecting the hidden and visible nodes
it is a special case of the more general boltzmann machines with the constraint of no intranode connections
each edge in an rbm is associated with a weight
the weights together with the connections define an energy function based on which a joint distribution of visible and hidden nodes can be devised
based on the topology of the rbm the hidden visible variables are independent conditioned on the visible hidden variables
such conditional independence facilitates computations
an rbm can be viewed as a single layer architecture for unsupervised feature learning
in particular the visible variables correspond to input data and the hidden variables correspond to feature detectors
the weights can be trained by maximizing the probability of visible variables using hintons contrastive divergence cd algorithm
in general training rbm by solving the maximization problem tends to result in nonsparse representations
sparse rbm was proposed to enable sparse representations
the idea is to add a regularization term in the objective function of data likelihood which penalizes the deviation of the expected hidden variables from a small constant formula
an autoencoder consisting of an encoder and a decoder is a paradigm for deep learning architectures
an example is provided by hinton and salakhutdinov where the encoder uses raw data e
image as input and produces feature or representation as output and the decoder uses the extracted feature from the encoder as input and reconstructs the original input raw data as output
the encoder and decoder are constructed by stacking multiple layers of rbms
the parameters involved in the architecture were originally trained in a greedy layerbylayer manner after one layer of feature detectors is learned they are fed up as visible variables for training the corresponding rbm
current approaches typically apply endtoend training with stochastic gradient descent methods
training can be repeated until some stopping criteria are satisfied
empirical risk minimization erm is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance
consider the following situation which is a general setting of many supervised learning problems
we have two spaces of objects formula and formula and would like to learn a function formula often called hypothesis which outputs an object formula given formula
to do so we have at our disposal a training set of m examples formula where formula is an input and formula is the corresponding response that we wish to get from formula
to put it more formally we assume that there is a joint probability distribution formula over formula and formula and that the training set consists of formula instances formula drawn i
from formula
note that the assumption of a joint probability distribution allows us to model uncertainty in predictions e
from noise in data because formula is not a deterministic function of formula but rather a random variable with conditional distribution formula for a fixed formula
we also assume that we are given a nonnegative realvalued loss function formula which measures how different the prediction formula of a hypothesis is from the true outcome formula the risk associated with hypothesis formula is then defined as the expectation of the loss functiona loss function commonly used in theory is the  loss function formula where formula is the indicator notation
the ultimate goal of a learning algorithm is to find a hypothesis formula among a fixed class of functions formula for which the risk formula is minimalin general the risk formula cannot be computed because the distribution formula is unknown to the learning algorithm this situation is referred to as agnostic learning
however we can compute an approximation called empirical risk by averaging the loss function on the training setthe empirical risk minimization principle states that the learning algorithm should choose a hypothesis formula which minimizes the empirical riskthus the learning algorithm defined by the erm principle consists in solving the above optimization problem
empirical risk minimization for a classification problem with a  loss function is known to be an nphard problem even for such a relatively simple class of functions as linear classifiers
though it can be solved efficiently when the minimal empirical risk is zero i
data is linearly separable
in practice machine learning algorithms cope with that either by employing a convex approximation to the  loss function like hinge loss for svm which is easier to optimize or by posing assumptions on the distribution formula and thus stop being agnostic learning algorithms to which the above result applies
in machine learning the vanishing gradient problem is a difficulty found in training artificial neural networks with gradientbased learning methods and backpropagation
in such methods each of the neural networks weights receives an update proportional to the gradient of the error function with respect to the current weight in each iteration of training
the problem is that in some cases the gradient will be vanishingly small effectively preventing the weight from changing its value
in the worst case this may completely stop the neural network from further training
as one example of the problem cause traditional activation functions such as the hyperbolic tangent function have gradients in the range  and backpropagation computes gradients by the chain rule
this has the effect of multiplying of these small numbers to compute gradients of the front layers in an layer network meaning that the gradient error signal decreases exponentially with while the front layers train very slowly
backpropagation allowed researchers to train supervised deep artificial neural networks from scratch initially with little success
hochreiters diploma thesis of  formally identified the reason for this failure in the vanishing gradient problem which not only affects manylayered feedforward networks but also recurrent networks
the latter are trained by unfolding them into very deep feedforward networks where a new layer is created for each time step of an input sequence processed by the network
when activation functions are used whose derivatives can take on larger values one risks encountering the related exploding gradient problem
to overcome this problem several methods were proposed
one is jrgen schmidhubers multilevel hierarchy of networks  pretrained one level at a time through unsupervised learning finetuned through backpropagation
here each level learns a compressed representation of the observations that is fed to the next level
similar ideas have been used in feedforward neural network for unsupervised pretraining to structure a neural network making it first learn generally useful feature detectors
then the network is trained further by supervised backpropagation to classify labeled data
the deep belief network model by hinton et al
involves learning the distribution of a high level representation using successive layers of binary or realvalued latent variables
it uses a restricted boltzmann machine to model each new layer of higher level features
each new layer guarantees an increase on the lowerbound of the log likelihood of the data thus improving the model if trained properly
once sufficiently many layers have been learned the deep architecture may be used as a generative model by reproducing the data when sampling down the model an ancestral pass from the top level feature activations
hinton reports that his models are effective feature extractors over highdimensional structured data
this work plays a key role in reintroducing the interests in deep neural network research and consequently leads to the developments of deep learning although deep belief network is no longer the main deep learning technique
another technique particularly used for recurrent neural networks is the long shortterm memory lstm network of  by hochreiter  schmidhuber
in  deep multidimensional lstm networks demonstrated the power of deep learning with many nonlinear layers by winning three icdar  competitions in connected handwriting recognition without any prior knowledge about the three different languages to be learned
hardware advances have meant that from  to  computer power especially as delivered by gpus has increased around a millionfold making standard backpropagation feasible for networks several layers deeper than when the vanishing gradient problem was recognized
schmidhuber notes that this is basically what is winning many of the image recognition competitions now but that it does not reallyovercome the problem in a fundamental way since the original models tackling the vanishing gradient problem by hinton et al
were trained in a xeon processor not gpus
one of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks resnets not to be confused with recurrent neural networks
it was noted prior to resnets that a deeper network would actually have higher training error than the shallow network
this intuitively can be understood as data disappearing through too many layers of the network meaning output from a shallow layer was diminished through the greater number of layers in the deeper network yielding a worse result
going with this intuitive hypothesis microsoft research found that splitting a deep network into three layer chunks and passing the input into each chunk straight through to the next chunk along with the residualoutput of the chunk minus the input to the chunk that is reintroduced helped eliminate much of this disappearing signal problem
no extra parameters or changes to the learning algorithm were needed
resnets yielded lower training error and test error than their shallower counterparts simply by reintroducing outputs from shallower layers in the network to compensate for the vanishing data
note that resnets are an ensemble of relatively shallow nets and do not resolve the vanishing gradient problem by preserving gradient flow throughout the entire depth of the network  rather they avoid the problem simply by constructing ensembles of many short networks together
ensemble by constructionrectifiers such as relu suffer less from the vanishing gradient problem because they only saturate in one direction
behnke relied only on the sign of the gradient rprop when training his neural abstraction pyramid to solve problems like image reconstruction and face localization
neural networks can also be optimized by using a universal search algorithm on the space of neural networks weights e
random guess or more systematically genetic algorithm
this approach is not based on gradient and avoids the vanishing gradient problem
depending on the type and variation in training data machine learning can be roughly categorized into three frameworks supervised learning unsupervised learning and reinforcement learning
multiple instance learning mil falls under the supervised learning framework where every training instance has a label either discrete or real valued
mil deals with problems with incomplete knowledge of labels in training sets
more precisely in multipleinstance learning the training set consists of labeled bags each of which is a collection of unlabeled instances
a bag is positively labeled if at least one instance in it is positive and is negatively labeled if all instances in it are negative
the goal of the mil is to predict the labels of new unseen bags
convenient and simple example for mil was given in
imagine several people and each of them has a key chain that contains few keys
some of these people are able to enter a certain room and some arent
the task is then to predict whether a certain key or a certain key chain can get you into that room
to solve this problem we need to find the exact key that is common for all the positive key chains
if we can correctly identify this key we can also correctly classify an entire key chain  positive if it contains the required key or negative if it doesnt
keeler et al
in his work in early s was the first one to explore the area of mil
the actual term multiinstance learning was introduced in the middle of the s by dietterich et al
while they were investigating the problem of drug activity prediction
they tried to create a learning systems that could predict whether new molecule was qualified to make some drug or not through analyzing a collection of known molecules
molecules can have many alternative lowenergy states but only one or some of them are qualified to make a drug
the problem arose because scientists could only determine if molecule is qualified or not but they couldnt say exactly which of its lowenergy shapes are responsible for that
one of the proposed ways to solve this problem was to use supervised learning and regard all the lowenergy shapes of the qualified molecule as positive training instances while all of the lowenergy shapes of unqualified molecules as negative instances
dietterich et al
showed that such method would have a high false positive noise from all lowenergy shapes that are mislabeled as positive and thus wasnt really useful
their approach was to regard each molecule as a labeled bag and all the alternative lowenergy shapes of that molecule as instances in the bag without individual labels
thus formulating multipleinstance learning
solution to the multiple instance learning problem that dietterich et al
proposed is three axisparallel rectangle apr algorithm
it attempts to search for appropriate axisparallel rectangles constructed by the conjunction of the features
they tested the algorithm on musk dataset which is a concrete test data of drug activity prediction and the most popularly used benchmark in multipleinstance learning
apr algorithm achieved the best result but it should be noted that apr was designed with musk data in mind
problem of multiinstance learning is not unique to drug finding
in  maron and ratan found another application of multiple instance learning to scene classification in machine vision and devised diverse density framework
given an image an instance is taken to be one or more fixedsize subimages and the bag of instances is taken to be the entire image
an image is labeled positive if it contains the target scene  a waterfall for example  and negative otherwise
multiple instance learning can be used to learn the properties of the subimages which characterize the target scene
from there on these frameworks have been applied to a wide spectrum of applications ranging from image concept learning and text categorization to stock market prediction
if the space of instances is formula then the set of bags is the set of functions formula which is isomorphic to the set of multisubsets of formula
for each bag formula and each instance formula formula is viewed as the number of times formula occurs in formula
let formula be the space of labels then a multiple instance concept is a map formula
the goal of mil is to learn such a concept
the remainder of the article will focus on binary classification where formula
most of the work on multiple instance learning including dietterich et al
and maron  lozanoperez  early papers make the assumption regarding the relationship between the instances within a bag and the class label of the bag
because of its importance that assumption is often called standard mi assumption
the standard assumption takes each instance formula to have an associated label formula which is hidden to the learner
the pair formula is called an instancelevel concept
a bag is now viewed as a multiset of instancelevel concepts and is labeled positive if at least one of its instances has a positive label and negative if all of its instances have negative labels
formally let formula be a bag
the label of formula is then formula
standard mi assumption is asymmetric which means that if the positive and negative labels are reversed the assumption has a different meaning
because of that when we use this assumption we need to be clear which label should be the positive one
standard assumption might be viewed as too strict and therefore in the recent years researchers tried to relax that position which gave rise to other more loose assumptions
reason for this is the belief that standard mi assumption is appropriate for the musk dataset but since mli can be applied to numerous other problems some different assumptions could probably be more appropriate
guided by that idea weidmann formulated a hierarchy of generalized instancebased assumptions for mil
it consists of the standard mi assumption and three types of generalized mi assumptions each more general than the last formula with the countbased assumption being the most general and the standard assumption being the least general
one would expect an algorithm which performs well under one of these assumptions to perform at least as well under the less general assumptions
the presencebased assumption is a generalization of the standard assumption wherein a bag must contain one or more instances that belong to a set of required instancelevel concepts in order to be labeled positive
formally let formula be the set of required instancelevel concepts and let formula denote the number of times the instancelevel concept formula occurs in the bag formula
then formula for all formula
note that by taking formula to contain only one instancelevel concept the presencebased assumption reduces to the standard assumption
a further generalization comes with the thresholdbased assumption where each required instancelevel concept must occur not only once in a bag but some minimum threshold number of times in order for the bag to be labeled positive
with the notation above to each required instancelevel concept formula is associated a threshold formula
for a bag formula formula for all formula
the countbased assumption is a final generalization which enforces both lower and upper bounds for the number of times a required concept can occur in a positively labeled bag
each required instancelevel concept formula has a lower threshold formula and upper threshold formula with formula
a bag formula is labeled according to formula for all formula
scott zhang and brown  describe another generalization of the standard model which they call generalized multiple instance learning gmil
the gmil assumption specifies a set of required instances formula
a bag formula is labeled positive if it contains instances which are sufficiently close to at least formula of the required instances formula
under only this condition the gmil assumption is equivalent to the presencebased assumption
however scott et
al
describe a further generalization in which there is a set of attraction points formula and a set of repulsion points formula
a bag is labeled positive if and only if it contains instances which are sufficiently close to at least formula of the attraction points and are sufficiently close to at most formula of the repulsion points
this condition is strictly more general than the presencebased though it does not fall within the above hierarchy
in contrast to the previous assumptions where the bags were viewed as fixed the collective assumption views a bag formula as a distribution formula over instances formula and similarly view labels as a distribution formula over instances
the goal of an algorithm operating under the collective assumption is then to model the distribution formula
since formula is typically considered fixed but unknown algorithms instead focus on computing the empirical version formula where formula is the number of instances in bag formula
since formula is also typically taken to be fixed but unknown most collectiveassumption based methods focus on learning this distribution as in the singleinstance version
while the collective assumption weights every instance with equal importance foulds extended the collective assumption to incorporate instance weights
the weighted collective assumption is then that formula where formula is a weight function over instances and formula
there are two major flavors of algorithms for multiple instance learning instancebased and metadatabased or embeddingbased algorithms
the term instancebased denotes that the algorithm attempts to find a set of representative instances based on an mi assumption and classify future bags from these representatives
by contrast metadatabased algorithms make no assumptions about the relationship between instances and bag labels and instead try to extract instanceindependent information or metadata about the bags in order to learn the concept
for a survey of some of the modern mi algorithms see foulds and frank the earliest proposed mi algorithms were a set of iterateddiscrimination algorithms developed by dietterich et
al and diverse density developed by maron and lozanoprez
both of these algorithms operated under the standard assumption
broadly all of the iterateddiscrimination algorithms consist of two phases
the first phase is to grow an axis parallel rectangle apr which contains at least one instance from each positive bag and no instances from any negative bags
this is done iteratively starting from a random instance formula in a positive bag the apr is expanded to the smallest apr covering any instance formula in a new positive bag formula
this process is repeated until the apr covers at least one instance from each positive bag
then each instance formula contained in the apr is given a relevance corresponding to how many negative points it excludes from the apr if removed
the algorithm then selects candidate representative instances in order of decreasing relevance until no instance contained in a negative bag is also contained in the apr
the algorithm repeats these growth and representative selection steps until convergence where apr size at each iteration is taken to be only along candidate representatives
after the first phase the apr is thought to tightly contain only the representative attributes
the second phase expands this tight apr as follows a gaussian distribution is centered at each attribute and a looser apr is drawn such that positive instances will fall outside the tight apr with fixed probability
though iterated discrimination techniques work well with the standard assumption they do not generalize well to other mi assumptions
in its simplest form diverse density dd assumes a single representative instance formula as the concept
this representative instance must be dense in that it is much closer to instances from positive bags than from negative bags as well as diverse in that it is close to at least one instance from each positive bag
let formula be the set of positively labeled bags and let formula be the set of negatively labeled bags then the best candidate for the representative instance is given by formula where the diverse density formula under the assumption that bags are independently distributed given the concept formula
letting formula denote the jth instance of bag i the noisyor model givesformula is taken to be the scaled distance formula where formula is the scaling vector
this way if every positive bag has an instance close to formula then formula will be high for each formula but if any negative bag formula has an instance close to formula formula will be low
hence formula is high only if every positive bag has an instance close to formula and no negative bags have an instance close to formula
the candidate concept formula can be obtained through gradient methods
classification of new bags can then be done by evaluating proximity to formula
though diverse density was originally proposed by maron et
al
in  more recent mil algorithms use the dd framework such as emdd in  and ddsvm in  and miles in  a number of singleinstance algorithms have also been adapted to a multipleinstance context under the standard assumption includingpost  there was a movement away from the standard assumption and the development of algorithms designed to tackle the more general assumptions listed above
because of the high dimensionality of the new feature space and the cost of explicitly enumerating all aprs of the original instance space gmil is inefficient both in terms of computation and memory
gmil was developed as a refinement of gmil in an effort to improve efficiency
gmil preprocesses the instances to find a set of candidate representative instances
gmil then maps each bag to a boolean vector as in gmil but only considers aprs corresponding to unique subsets of the candidate representative instances
this significantly reduces the memory and computational requirements
by mapping each bag to a feature vector of metadata metadatabased algorithms allow the flexibility of using an arbitrary singleinstance algorithm to perform the actual classification task
future bags are simply mapped embedded into the feature space of metadata and labeled by the chosen classifier
therefore much of the focus for metadatabased algorithms is on what features or what type of embedding leads to effective classification
note that some of the previously mentioned algorithms such as tlc and gmil could be considered metadatabased
they define two variations of knn bayesianknn and citationknn as adaptations of the traditional nearestneighbor problem to the multipleinstance setting
so far this article has considered multiple instance learning exclusively in the context of binary classifiers
however the generalizations of singleinstance binary classifiers can carry over to the multipleinstance case
the master algorithm how the quest for the ultimate learning machine will remake our world is a book by pedro domingos released in
domingos wrote the book in order to generate interest from people outside the field
the book outlines five tribes of machine learning inductive reasoning connectionism evolutionary computation bayes theorem and analogical modelling
the author explains these tribes to the reader by referring to more understandable processes of logic connections made in the brain natural selection probability and similarity judgements
throughout the book it is suggested that each different tribe has the potential to contribute to a unifying master algorithm
towards the end of the book the author pictures a master algorithm in the near future where machine learning algorithms asymptotically grow to a perfect understanding of how the world and people in it work
although the algorithm doesnt yet exist he briefly reviews his own invention of the markov logic network
in  bill gates recommended the book alongside nick bostroms superintelligence as one of two books everyone should read to understand ai
in  the book was noted to be on chinese president xi jinpings bookshelf
a computer science educator stated in times higher education that the examples are clear and accessible
in contrast the economist agreed domingo does a good job but complained that he constantly invents metaphors that grate or confuse
kirkus reviews praised the book stating readers unfamiliar with logic and computer theory will have a difficult time but those who persist will discover fascinating insights
a new scientist review called it compelling but rather unquestioning
a committee machine is a type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks experts are combined into a single response
the combined response of the committee machine is supposed to be superior to those of its constituent experts
compare with ensembles of classifiers
in this class of committee machines the responses of several predictors experts are combined by means of a mechanism that does not involve the input signal hence the designation static
this category includes the following methodsin ensemble averaging outputs of different predictors are linearly combined to produce an overall output
in boosting a weak algorithm is converted into one that achieves arbitrarily high accuracy
in this second class of committee machines the input signal is directly involved in actuating the mechanism that integrates the outputs of the individual experts into an overall output hence the designation dynamic
there are two kinds of dynamic structuresin mixture of experts the individual responses of the experts are nonlinearly combined by means of a single gating network
in hierarchical mixture of experts the individual responses of the individual experts are nonlinearly combined by means of several gating networks arranged in a hierarchical fashion
robot learning is a research field at the intersection of machine learning and robotics
it studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms
the embodiment of the robot situated in a physical embedding provides at the same time specific difficulties e
highdimensionality real time constraints for collecting data and learning and opportunities for guiding the learning process e
sensorimotor synergies motor primitives
example of skills that are targeted by learning algorithms include sensorimotor skills such as locomotion grasping active object categorization as well as interactive skills such as joint manipulation of an object with a human peer and linguistic skills such as the grounded and situated meaning of human language
learning can happen either through autonomous selfexploration or through guidance from a human teacher like for example in robot learning by imitation
robot learning can be closely related to adaptive control reinforcement learning as well as developmental robotics which considers the problem of autonomous lifelong acquisition of repertoires of skills
while machine learning is frequently used by computer vision algorithms employed in the context of robotics these applications are usually not referred to as robot learning
maya cakmak assistant professor of computer science and engineering at the university of washington is trying to create a robot that learns by imitating  a technique called programming by demonstration
a researcher shows it a cleaning technique for the robots vision system and it generalizes the cleaning motion from the human demonstration as well as identifying the state of dirt before and after cleaning
similarly the baxter industrial robot can be taught how to do something by grabbing its arm and showing it the desired movements
it can also use deep learning to teach itself to grasp an unknown object
in telexs million object challenge the goal is robots that learn how to spot and handle simple items and upload their data to the cloud to allow other robots to analyze and use the information
robobrain is a knowledge engine for robots which can be freely accessed by any device wishing to carry out a task
the database gathers new information about tasks as robots perform them by searching the internet interpreting natural language text images and videos object recognition as well as interaction
the project is led by ashutosh saxena at stanford university
roboearth is a project that has been described as a world wide web for robots  it is a network and database repository where robots can share information and learn from each other and a cloud for outsourcing heavy computation tasks
the project brings together researchers from five major universities in germany the netherlands and spain and is backed by the european union
google research deepmind and google x have decided to allow their robots share their experiences
in statistics multivariate adaptive regression splines mars is a form of regression analysis introduced by jerome h
friedman in
it is a nonparametric regression techniqueand can be seen as an extension of linear models that automatically models nonlinearities and interactions between variables
the term mars is trademarked and licensed to salford systems
in order to avoid trademark infringements many open source implementations of mars are called earth
this section introduces mars using a few examples
we start with a set of data a matrix of input variables x and a vector of the observed responses y with a response for each row in x
for example the data could behere there is only one independent variable so the x matrix is just a single column
given these measurements we would like to build a model which predicts the expected y for a given x
a linear model for the above data isthe hat on the formula indicates that formula is estimated from the data
the figure on the right shows a plot of this function a line giving the predicted formula versus x with the original values of y shown as red dots
the data at the extremes of x indicates that the relationship between y and x may be nonlinear look at the red dots relative to the regression line at low and high values of x
we thus turn to mars to automatically build a model taking into account nonlinearities
mars software constructs a model from the given x and y as followsthe figure on the right shows a plot of this function the predicted formula versus x with the original values of y once again shown as red dots
the predicted response is now a better fit to the original y values
mars has automatically produced a kinkin the predicted y to take into account nonlinearity
the kink is produced by hinge functions
the hinge functions are the expressions starting with formulawhere formula is formula if formula else formula
hinge functions are described in more detail below
in this simple example we can easily see from the plot thaty has a nonlinear relationship with xand might perhaps guess that y varies with the square of x
however in general there will be multiple independent variablesand the relationship between y and these variables will be unclearand not easily visible by plotting
we can use mars to discover that nonlinear relationship
an example mars expression with multiple variables isthis expression models air pollution the ozone levelas a function of the temperature and a few other variables
note that the last term in the formula on the last lineincorporates an interaction between formulaand formula
the figure on the right plots the predicted formula as formula and formula varywith the other variables fixed at their median values
the figure shows that wind does not affect the ozonelevel unless visibility is low
we see that mars can build quite flexible regression surfacesby combining hinge functions
to obtain the above expression the mars model building procedureautomatically selects which variables to use some variables areimportant others not the positions of the kinks in the hingefunctions and how the hinge functions are combined
mars builds models of the formthe model is a weighted sum of basis functionsformula
each formula is a constant coefficient
for example each line in the formula for ozone above is one basis functionmultiplied by its coefficient
each basis functionformulatakes one of the following three forms a constant
there is just one such term the intercept
in the ozone formula above the intercept term is
a hinge function
a hinge function has the form formulaor formula
mars automatically selects variablesand values of those variables for knots of the hinge functions
examples of such basis functions can be seenin the middle three lines of the ozone formula
a product of two or more hinge functions
these basis functions can model interaction between two or more variables
an example is the last line of the ozone formula
hinge functions are a key part of mars models
a hinge function takes the formor where formula is a constant called the knot
the figure on the right shows a mirrored pair of hinge functions with a knot at
a hinge function is zero for part of its range so can be used to partition the data into disjoint regions each of which can be treated independently
thus for example a mirrored pair of hinge functions in the expressioncreates the piecewise linear graph shown for the simple mars model in the previous section
one might assume that only piecewise linear functions can be formed from hinge functions but hinge functions can be multiplied together to form nonlinear functions
hinge functions are also called ramp hockey stick or rectifier functions
instead of the formula notation used in this article hinge functions are often represented by formula where formula means take the positive part
mars builds a model in two phasesthe forward and the backward pass
this twostage approach is the same as that used by recursive partitioning trees
mars starts with a model which consists of just the intercept termwhich is the mean of the response values
mars then repeatedly adds basis function in pairs to the model
at each step it finds the pair of basis functions that gives the maximum reduction in sumofsquaresresidual errorit is a greedy algorithm
the two basis functions in the pairare identical except that a differentside of a mirrored hinge function is used for each function
each new basis function consists of a term already in the model multiplied by a new hinge function
a hinge function is defined by a variable and a knotso to add a new basis function mars must search overall combinations of the following existing terms called parent terms in this context all variables to select one for the new basis function all values of each variable for the knot of the new hinge function
to calculate the coefficient of each termmars applies a linear regression over the terms
this process of adding terms continues untilthe change in residual error is too small to continueor until the maximum number of terms is reached
the maximum number of termsis specified by the user before model building starts
the search at each step is done in a brute force fashionbut a key aspect of mars is thatbecause of the nature of hinge functionsthe search can be done relativelyquickly using a fast leastsquares update technique
actually the search is not quite brute force
the search can be sped up with a heuristic that reduces the numberof parent terms to consider at each stepfast marsthe forward pass usually builds an overfit model
an overfit model has a good fit to the data used to buildthe model but will not generalize well to new data
to build a model with better generalization abilitythe backward pass prunes the model
it removes terms one by one deleting the least effective term at each stepuntil it finds the best submodel
model subsets are compared using the gcv criterion described below
the backward pass has an advantage over the forward passat any step it can choose any term to deletewhereas the forward pass at each step can only see the next pair of terms
the forward pass adds terms in pairsbut the backward pass typically discards one side of the pairand so terms are often not seen in pairs in the final model
a paired hinge can be seen in the equation for formula in thefirst mars example abovethere are no complete pairs retained in the ozone example
the backward pass uses generalized cross validation gcv to compare the performance of model subsets in order to choose the best subset lower values of gcv are better
the gcv is a form ofregularizationit trades off goodnessoffit against model complexity
the formula for the gcv iswhere rss is the residual sumofsquaresmeasured on the training data and n is thenumber of observations the number of rows in the x matrix
the effectivenumberofparameters is defined inthe mars context aswhere penalty is about  or  themars software allows the user to preset penalty
note that is the number of hingefunction knots so the formula penalizes the addition of knots
thus the gcv formula adjusts i
increases the training rss to take intoaccount the flexibility of the model
we penalize flexibility because models that are too flexible will model the specific realization of noise in the data instead of just the systematic structure of the data
generalized cross validation is so named becauseit uses a formula to approximate the errorthat would be determined by leaveoneout validation
it is just an approximation but works well in practice
gcvs were introduced by craven andwahba and extended by friedman for mars
one constraint has already been mentioned the usercan specify the maximum number of terms in the forward pass
a further constraint can be placed on the forward passby specifying a maximum allowable degree of interaction
typically only one or two degrees of interaction are allowedbut higher degrees can be used when the data warrants it
the maximum degree of interaction in the first mars exampleabove is one i
no interactions or an additive model in the ozone example it is two
other constraints on the forward pass are possible
for example the user can specify that interactions are allowed only for certain input variables
such constraints could make sense because of knowledgeof the process that generated the data
no regression modeling technique is best for all situations
the guidelines below are intended to give an idea of the pros and cons of mars but there will be exceptions to the guidelines
it is useful to compare mars to recursive partitioning and this is done below
recursive partitioning is also commonly called regression treesdecision trees or cartsee the recursive partitioning article for details
several free and commercial software packages are available for fitting marstype models
the universal portfolio algorithm is a portfolio selection algorithm from the field of machine learning and information theory
the algorithm learns adaptively from historical data and maximizes the logoptimal growth rate in the long run
it was introduced by the late stanford university information theorist thomas m
cover
the algorithm rebalances the portfolio at the beginning of each trading period
at the beginning of the first trading period it starts with a naive diversification
in the following trading periods the portfolio composition depends on the historical total return of all possible constantrebalanced portfolios
in machine learning and pattern recognition a feature is an individual measurable property or characteristic of a phenomenon being observed
choosing informative discriminating and independent features is a crucial step for effective algorithms in pattern recognition classification and regression
features are usually numeric but structural features such as strings and graphs are used in syntactic pattern recognition
the concept of feature is related to that of explanatory variable used in statistical techniques such as linear regression
a set of numeric features can be conveniently described by a feature vector
an example of reaching a twoway classification from a feature vector related to the perceptron consists ofcalculating the scalar product between the feature vector and a vector of weightscomparing the result with a threshold and deciding the class based on the comparison
algorithms for classification from a feature vector include nearest neighbor classification neural networks and statistical techniques such as bayesian approaches
in character recognition features may include histograms counting the number of black pixels along horizontal and vertical directions number of internal holes stroke detection and many others
in speech recognition features for recognizing phonemes can include noise ratios length of sounds relative power filter matches and many others
in spam detection algorithms features may include the presence or absence of certain email headers the email structure the language the frequency of specific terms the grammatical correctness of the text
in computer vision there are a large number of possible features such as edges and objects
in pattern recognition and machine learning a feature vector is an ndimensional vector of numerical features that represent some object
many algorithms in machine learning require a numerical representation of objects since such representations facilitate processing and statistical analysis
when representing images the feature values might correspond to the pixels of an image while when representing texts the features might be the frequencies of occurrence of textual terms
feature vectors are equivalent to the vectors of explanatory variables used in statistical procedures such as linear regression
feature vectors are often combined with weights using a dot product in order to construct a linear predictor function that is used to determine a score for making a prediction
the vector space associated with these vectors is often called the feature space
in order to reduce the dimensionality of the feature space a number of dimensionality reduction techniques can be employed
higherlevel features can be obtained from already available features and added to the feature vector for example for the study of diseases the feature age is useful and is defined as age  year of death minus year of birth
this process is referred to as feature construction
feature construction is the application of a set of constructive operators to a set of existing features resulting in construction of new features
examples of such constructive operators include checking for the equality conditions   the arithmetic operators   the array operators maxs mins averages as well as other more sophisticated operators for example countsc that counts the number of features in the feature vector s satisfying some condition c or for example distances to other recognition classes generalized by some accepting device
feature construction has long been considered a powerful tool for increasing both accuracy and understanding of structure particularly in highdimensional problems
applications include studies of disease and emotion recognition from speech
the initial set of raw features can be redundant and too large to be managed
therefore a preliminary step in many applications of machine learning and pattern recognition consists of selecting a subset of features or constructing a new and reduced set of features to facilitate learning and to improve generalization and interpretability
extracting or selecting features is a combination of art and science developing systems to do so is known as feature engineering
it requires the experimentation of multiple possibilities and the combination of automated techniques with the intuition and knowledge of the domain expert
automating this process is feature learning where a machine not only uses features for learning but learns the features itself
trax is a technology company headquartered in singapore with offices throughout apac europe middle east north america and south america
its computer vision technology is used by fmcg companies such as cocacola and retailers to collect measure and analyse what is happening on physical shelves
founded in  trax has over  customers in the retail and fmcg industries including beverage giant cocacola and brewer anheuserbusch inbev
its service is available in  markets and the companys development centre is located in tel aviv
trax closed its first round of funding for us
million in june
they opened their telaviv office in july  and closed their second round of funding for us
million in december
their third round of funding for us
million closed in february
in december  trax announced its fourth round of investment of us million
in  trax opened their first two regional offices london in january and brazil in april
in march  trax established their latam headquarters in atlanta georgia
trax announced a th round of funding for us million on june
two new regional offices were opened in shanghai and mexico city in june and september  respectively
on february   trax closed their sixth round of funding for us
million
on june   trax announced its most recent funding round of us million lead by global private equity giant warburg pincus
on july   trax announced that they had acquired nielsen store observation nso assets in the usa from nielsen corporation
on january   trax announced that they had acquired usbased quri the leader in crowdsourced instore conditions data for the consumer packaged goods cpg industry
trax reduces the time an employee needs to spend on audits to check inventory shelf display and product promotions
it is also gathers more extensive data such as product assortment shelf space pricing promotions shelf location and arrangement of products on display
this market intelligence is valuable to retail and fmcg manufacturers because they pay large sums for space in supermarkets and stores
for example in the us companies pay approximately  billion for shelf space
the computer vision technology uses artificial intelligence finegrained image recognition and machine learning engines to convert store images into shelf insights
trax is able to recognise products that are similar or identical such as branded drinks or shampoo bottles whilst also being able to differentiate between them based on variety and size
it piloted its machine learning algorithms with initial customers allowing its algorithm to learn about different products
as the company processes more images the better it gets at recognising the same products in different shapes and sizes
to date trax has recognized more than  billion images and recognizes approximately  million new products per month
learning to rank or machinelearned ranking mlr is the application of machine learning typically supervised semisupervised or reinforcement learning in the construction of ranking models for information retrieval systems
training data consists of lists of items with some partial order specified between items in each list
this order is typically induced by giving a numerical or ordinal score or a binary judgment e
relevant or not relevant for each item
the ranking models purpose is to rank i
produce a permutation of items in new unseen lists in a way which is similar to rankings in the training data in some sense
ranking is a central part of many information retrieval problems such as document retrieval collaborative filtering sentiment analysis and online advertising
a possible architecture of a machinelearned search engine is shown in the figure to the right
training data consists of queries and documents matching them together with relevance degree of each match
it may be prepared manually by human assessors or raters as google calls themwho check results for some queries and determine relevance of each result
it is not feasible to check the relevance of all documents and so typically a technique called pooling is used  only the top few documents retrieved by some existing ranking models are checked
alternatively training data may be derived automatically by analyzing clickthrough logs i
search results which got clicks from users query chains or such search engines features as googles searchwiki
training data is used by a learning algorithm to produce a ranking model which computes the relevance of documents for actual queries
typically users expect a search query to complete in a short time such as a few hundred milliseconds for web search which makes it impossible to evaluate a complex ranking model on each document in the corpus and so a twophase scheme is used
first a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation such as the vector space model boolean model weighted and or bm
this phase is called topformula document retrieval and many heuristics were proposed in the literature to accelerate it such as using a documents static quality score and tiered indexes
in the second phase a more accurate but computationally expensive machinelearned model is used to rerank these documents
learning to rank algorithms have been applied in areas other than information retrievalfor the convenience of mlr algorithms querydocument pairs are usually represented by numerical vectors which are called feature vectors
such an approach is sometimes called bag of features and is analogous to the bag of words model and vector space model used in information retrieval for representation of documents
components of such vectors are called features factors or ranking signals
they may be divided into three groups features from document retrieval are shown as examplessome examples of features which were used in the wellknown letor datasetselecting and designing good features is an important area in machine learning which is called feature engineering
there are several measures metrics which are commonly used to judge how well an algorithm is doing on training data and to compare the performance of different mlr algorithms
often a learningtorank problem is reformulated as an optimization problem with respect to one of these metrics
examples of ranking quality measuresdcg and its normalized variant ndcg are usually preferred in academic research when multiple levels of relevance are used
other metrics such as map mrr and precision are defined only for binary judgments
recently there have been proposed several new evaluation metrics which claim to model users satisfaction with search results better than the dcg metricboth of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document than after a less relevant document
tieyan liu of microsoft research asia has analyzed existing algorithms for learning to rank problems in his paper learning to rank for information retrieval
he categorized them into three groups by their input representation and loss function the pointwise pairwise and listwise approach
in practice listwise approaches often outperform pairwise approaches and pointwise approaches
this statement was further supported by a large scale experiment on the performance of different learningtorank methods on a large collection of benchmark data sets
in this case it is assumed that each querydocument pair in the training data has a numerical or ordinal score
then the learningtorank problem can be approximated by a regression problem  given a single querydocument pair predict its score
a number of existing supervised machine learning algorithms can be readily used for this purpose
ordinal regression and classification algorithms can also be used in pointwise approach when they are used to predict the score of a single querydocument pair and it takes a small finite number of values
in this case the learningtorank problem is approximated by a classification problem  learning a binary classifier that can tell which document is better in a given pair of documents
the goal is to minimize the average number of inversions in ranking
these algorithms try to directly optimize the value of one of the above evaluation measures averaged over all queries in the training data
this is difficult because most evaluation measures are not continuous functions with respect to ranking models parameters and so continuous approximations or bounds on evaluation measures have to be used
a partial list of published learningtorank algorithms is shown below with years of first publication of each methodnote as most supervised learning algorithms can be applied to pointwise case only those methods which are specifically designed with ranking in mind are shown above
norbert fuhr introduced the general idea of mlr in  describing learning approaches in information retrieval as a generalization of parameter estimation a specific variant of this approach using polynomial regression had been published by him three years earlier
bill cooper proposed logistic regression for the same purpose in  and used it with his berkeley research group to train a successful ranking function for trec
manning et al
suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques
several conferences such as nips sigir and icml had workshops devoted to the learningtorank problem since mids decade
commercial web search engines began using machine learned ranking systems since the s decade
one of the first search engines to start using it was altavista later its technology was acquired by overture and then yahoo which launched a gradient boostingtrained ranking function in april
bings search is said to be powered by ranknet algorithm which was invented at microsoft research in
in november  a russian search engine yandex announced that it had significantly increased its search quality due to deployment of a new proprietary matrixnet algorithm a variant of gradient boosting method which uses oblivious decision trees
recently they have also sponsored a machinelearned ranking competition internet mathematics  based on their own search engines production data
yahoo has announced a similar competition in
as of  googles peter norvig denied that their search engine exclusively relies on machinelearned ranking
cuils ceo tom costello suggests that they prefer handbuilt models because they can outperform machinelearned models when measured against metrics like clickthrough rate or time on landing page which is because machinelearned models learn what people say they like not what people actually like
in january  the technology was included in the open source search engine apache solr thus making machine learned search rank widely accessible also for enterprise search
data exploration is an approach similar to initial data analysis whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data rather than through traditional data management systems
these characteristics can include size or amount of data completeness of the data correctness of the data possible relationships amongst data elements or filestables in the data
data exploration is typically conducted using a combination of automated and manual activities
automated activities can include data profiling or data visualization or tabular reports to give the analyst an initial view into the data and an understanding of key characteristics
this is often followed by manual drilldown or filtering of the data to identify anomalies or patterns identified through the automated actions
data exploration can also require manual scripting and queries into the data e
using languages such as sql or r or using excel or similar tools to view the raw data
all of these activities are aimed at creating a clear mental model and understanding of the data in the mind of the analyst and defining basic metadata statistics structure relationships for the data set that can be used in further analysis
once this initial understanding of the data is had the data can be pruned or refined by removing unusable parts of the data correcting poorly formatted elements and defining relevant relationships across datasets
this process is also known as determining data quality
data exploration can also refer to the ad hoc querying and visualization of data to identify potential relationships or insights that may be hidden in the data
traditionally this had been a key area of focus for statisticians with john tukey being a key evangelist in the field
today data exploration is more widespread and is the focus of data analysts and data scientists the latter being a relatively new role within enterprises and larger organizations
this area of data exploration has become an area of interest in the field of machine learning
this is a relatively new field and is still evolving
as its most basic level a machinelearning algorithm can be fed a data set and can be used to identify whether a hypothesis is true based on the dataset
common machine learning algorithms can focus on identifying specific patterns in the data
many common patterns include regression and classification or clustering but there are many possible patterns and algorithms that can be applied to data via machine learning
by employing machine learning it is possible to find patterns or relationships in the data that would be difficult or impossible to find via manual inspection trial and error or traditional exploration techniques
the bradleyterry model is a probability model that can predict the outcome of a comparison
given a pair of individuals and drawn from some population it estimates the probability that the pairwise comparison turns out true aswhere is a positive realvalued score assigned to individual
the comparison can be read as  is preferred to   ranks higher than  or  beats  depending on the application
for example may represent the skill of a team in a sports tournament estimated from the number of times has won a match
formula then represents the probability that will win a match against
another example used to explain the models purpose is that of scoring products in a certain category by quality
while its hard for a person to draft a direct ranking of many brands of wine it may be feasible to compare a sample of pairs of wines and say for each pair which one is better
the bradleyterry model can then be used to derive a full ranking
the model is named after r
bradley and m
terry who presented it in  although it had already been studied by zermelo in the s
realworld applications of the model include estimation of the influence of statistical journals or ranking documents by relevance in machinelearned search engines
in the latter application formula may reflect that document is more relevant to the users query than document  so it should be displayed earlier in the results list
the individual then express the relevance of the document and can be estimated from the frequency with which users click particular hits when presented with a result list
the bradleyterry model can be parametrized in various ways
one way to do so is to pick a single parameter per observation leading to a model of parameters
another variant in fact the version considered by bradley and terry uses exponential score functions formula so thator using the logit and disallowing tiesreducing the model to logistic regression on pairs of individuals
the following algorithm computes the parameters of the basic version of the model from a sample of observations
formally it computes a maximum likelihood estimate i
it maximizes the likelihood of the observed data
the algorithm dates back to the work of zermelo
the observations required are the outcomes of previous comparisons for example pairs where beats
summarizing these outcomes as  the number of times has beaten  we obtain the loglikelihood of the parameter vector asdenote the number of comparisons won by as  and the number of comparisons made between and as
starting from an arbitrary vector  the algorithm iteratively performs the updatefor all
after computing all of the new parameters they should be renormalizedthis estimation procedure improves the loglikelihood in every iteration and eventually converges to a unique maximum
feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work
feature engineering is fundamental to the application of machine learning and is both difficult and expensive
the need for manual feature engineering can be obviated by automated feature learning
feature engineering is an informal topic but it is considered essential in applied machine learning
a feature is an attribute or property shared by all of the independent units on which analysis or prediction is to be done
any attribute could be a feature as long as it is useful to the model
the purpose of a feature other than being an attribute would be much easier to understand in the context of a problem
a feature is a characteristic that might help when solving the problem
the features in your data are important to the predictive models you use and will influence the results you are going to achieve
the quality and quantity of the features will have great influence on whether the model is good or not
you could say the better the features are the better the result is
this isnt entirely true because the results achieved also depend on the model and the data not just the chosen features
that said choosing the right features is still very important
better features can produce simpler and more flexible models and they often yield better results
depending on a feature it could be strongly relevant has information that doesnt exist in any other feature relevant weakly relevant some information that other features include or irrelevant
it is important to create a lot of features
even if some of them are irrelevant you cant afford missing the rest
afterwards feature selection can be used in order to prevent overfitting
feature explosion can be caused by feature combination or feature templates both leading to a quick growth in the total number of features
there are a few solutions to help stop feature explosion such as regularization kernel method feature selection
automation of feature engineering has become an emerging topic of research in academia
in  researchers at mit presented the deep feature synthesis algorithm and demonstrated its effectiveness in online data science competitions where it beat  of  human teams
deep feature synthesis is available as an open source library called featuretools
that work was followed by other researchers including ibms onebm and berkeleys explorekit
the researchers at ibm state that feature engineering automation helps data scientists reduce data exploration time allowing them to try and error many ideas in short time
on the other hand it enables nonexperts who are not familiar with data science to quickly extract value from their data with a little effort time and cost
commercial tools have emerged from new machine learning focused startups including h
ai and feature labs
the inductive bias also known as learning bias of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered
in machine learning one aims to construct algorithms that are able to learn to predict a certain target output
to achieve this the learning algorithm is presented some training examples that demonstrate the intended relation of input and output values
then the learner is supposed to approximate the correct output even for examples that have not been shown during training
without any additional assumptions this problem cannot be solved exactly since unseen situations might have an arbitrary output value
the kind of necessary assumptions about the nature of the target function are subsumed in the phrase inductive bias
a classical example of an inductive bias is occams razor assuming that the simplest consistent hypothesis about the target function is actually the best
here consistent means that the hypothesis of the learner yields correct outputs for all of the examples that have been given to the algorithm
approaches to a more formal definition of inductive bias are based on mathematical logic
here the inductive bias is a logical formula that together with the training data logically entails the hypothesis generated by the learner
unfortunately this strict formalism fails in many practical cases where the inductive bias can only be given as a rough description e
in the case of artificial neural networks or not at all
the following is a list of common inductive biases in machine learning algorithms
although most learning algorithms have a static bias some algorithms are designed to shift their bias as they acquire more data
this does not avoid bias since the bias shifting process itself must have a bias
feature scaling is a method used to standardize the range of independent variables or features of data
in data processing it is also known as data normalization and is generally performed during the data preprocessing step
since the range of values of raw data varies widely in some machine learning algorithms objective functions will not work properly without normalization
for example the majority of classifiers calculate the distance between two points by the euclidean distance
if one of the features has a broad range of values the distance will be governed by this particular feature
therefore the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance
another reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it
the simplest method is rescaling the range of features to scale the range in   or
selecting the target range depends on the nature of the data
the general formula is given asformulawhere formula is an original value formula is the normalized value
for example suppose that we have the students weight data and the students weights span  pounds  pounds
to rescale this data we first subtract  from each students weight and divide the result by  the difference between the maximum and minimum weights
formulawhere formula is an original value formula is the normalized value
in machine learning we can handle various types of data e
audio signals and pixel values for image data and this data can include multiple dimensions
feature standardization makes the values of each feature in the data have zeromean when subtracting the mean in the numerator and unitvariance
this method is widely used for normalization in many machine learning algorithms e
support vector machines logistic regression and artificial neural networks
the general method of calculation is to determine the distribution mean and standard deviation for each feature
next we subtract the mean from each feature
then we divide the values mean is already subtracted of each feature by its standard deviation
formulawhere formula is the original feature vector formula is the mean of that feature vector and formula is its standard deviation
another option that is widely used in machinelearning is to scale the components of a feature vector such that the complete vector has length one
this usually means dividing each component by the euclidean length of the vector in some applications e
histogram features it can be more practical to use the l norm i
manhattan distance cityblock length or taxicab geometry of the feature vector
this is especially important if in the following learning steps the scalar metric is used as a distance measure
in stochastic gradient descent feature scaling can sometimes improve the convergence speed of the algorithm
in support vector machines it can reduce the time to find support vectors
note that feature scaling changes the svm result
instance selection or dataset reduction or dataset condensation is an important data preprocessing step that can be applied in many machine learning or data mining tasks
approaches for instance selection can be applied for reducing the original dataset to a manageable volume leading to a reduction of the computational resources that are necessary for performing the learning process
algorithms of instance selection can also be applied for removing noisy instances before applying learning algorithms
this step can improve the accuracy in classification problems
algorithm for instance selection should identify a subset of the total available data to achieve the original purpose of the data mining or machine learning application as if the whole data had been used
considering this the optimal outcome of is would be the minimum data subset that can accomplish the same task with no performance loss in comparison with the performance achieved when the task is performed using the whole available data
therefore every instance selection strategy should deal with a tradeoff between the reduction rate of the dataset and the classification quality
the literature provides several different algorithms for instance selection
they can be distinguished from each other according to several different criteria
considering this instance selection algorithms can be grouped in two main classes according to what instances they select algorithms that preserve the instances at the boundaries of classes and algorithms that preserve the internal instances of the classes
within the category of algorithms that select instances at the boundaries it is possible to cite drop icf and lsbo
on the other hand within the category of algorithms that select internal instances it is possible to mention enn and lssm
in general algorithm such as enn and lssm are used for removing harmful noisy instances from the dataset
they do not reduce the data as the algorithms that select border instances but they remove instances at the boundaries that have a negative impact on the data mining task
they can be used by other instance selection algorithms as a filtering step
for example the enn algorithm is used by drop as the first step and the lssm algorithm is used by lsbo
there is also another group of algorithms that adopt different selection criteria
for example the algorithms ldis and cdis select the densest instances in a given arbitrary neighborhood
the selected instances can include both border and internal instances
the ldis and cdis algorithms are very simple and select subsets that are very representative of the original dataset
besides that since they search by the representative instances in each class separately they are faster in terms of time complexity and effective running time than other algorithms such as drop and icf
mountain car a standard testing domain in reinforcement learning is a problem in which an underpowered car must drive up a steep hill
since gravity is stronger than the cars engine even at full throttle the car cannot simply accelerate up the steep slope
the car is situated in a valley and must learn to leverage potential energy by driving up the opposite hill before the car is able to make it to the goal at the top of the rightmost hill
the domain has been used as a test bed in various reinforcement learning papers
the mountain car problem although fairly simple is commonly applied because it requires a reinforcement learning agent to learn on two continuous variables position and velocity
for any given state position and velocity of the car the agent is given the possibility of driving left driving right or not using the engine at all
in the standard version of the problem the agent receives a negative reward at every time step when the goal is not reached the agent has no information about the goal until an initial success
the mountain car problem appeared first in andrew moores phd thesis
it was later more strictly defined in singh and suttons reinforcement leaning paper with eligibility traces
the problem became more widely studied when sutton and barto added it to their book reinforcement learning an introduction
throughout the years many versions of the problem have been used such as those which modify the reward function termination condition andor the start state
qlearning and similar techniques for mapping discrete states to discrete actions need to be extended to be able to deal with the continuous state space of the problem
approaches often fall into one of two categories state space discretization or function approximation
in this approach two continuous state variables are pushed into discrete states by bucketing each continuous variable into multiple discrete states
this approach works with properly tuned parameters but a disadvantage is information gathered from one state is not used to evaluate another state
tile coding can be used to improve discretization and involves continuous variables mapping into sets of buckets offset from one another
each step of training has a wider impact on the value function approximation because when the offset grids are summed the information is diffused
function approximation is another way to solve the mountain car
by choosing a set of basis functions beforehand or by generating them as the car drives the agent can approximate the value function at each state
unlike the stepwise version of the value function created with discretization function approximation can more cleanly estimate the true smooth function of the mountain car domain
an interesting aspect of the problem involves the delay of actual reward
the agent isnt able to learn about the goal until a successful completion
given a naive approach without traces for each trial the car can only backup the reward of the goal slightly
this is a problem for naive discretization because each discrete state will only be backup once taking a larger number of episodes to learn the problem
to alleviate this problem traces will automatically backup the reward given to states before dramatically increasing the speed of learning
the mountain car problem has undergone many iterations
this section will focus on the standard well defined version from sutton
twodimensional continuous state space
formulaformulaonedimensional discrete action space
formulafor every time stepformulafor every time stepformulaformulaformulaoptionally many implementations include randomness in both parameters to show better generalized learning
formulaformulaend the simulation whenformulathere are many versions of the mountain car which deviate in different ways from the standard model
variables that vary include but are not limited to changing the constants gravity and steepness of the problem so specific tuning for specific policies become irrelevant and altering the reward function to affect the agents ability to learn in a different manner
an example is changing the reward to be equal to the distance from the goal or changing the reward to zero everywhere and one at the goal
additionally we can use a d mountain car with a d continuous state space
version space learning is a logical approach to machine learning specifically binary classification
version space learning algorithms search a predefined space of hypotheses viewed as a set of logical sentences
formally the hypothesis space is a disjunctioni
either hypothesis  is true or hypothesis  or any subset of the hypotheses  through
a version space learning algorithm is presented with examples which it will use to restrict its hypothesis space for each example  the hypotheses that are inconsistent with are removed from the space
this iterative refining of the hypothesis space is called the candidate elimination algorithm the hypothesis space maintained inside the algorithm its version space
in settings where there is a generalityordering on hypotheses it is possible to represent the version space by two sets of hypotheses  the most specific consistent hypotheses and  the most general consistent hypotheses where consistent indicates agreement with observed data
the most specific hypotheses i
the specific boundary sb cover the observed positive training examples and as little of the remaining feature space as possible
these hypotheses if reduced any further exclude a positive training example and hence become inconsistent
these minimal hypotheses essentially constitute a pessimistic claim that the true concept is defined just by the positive data already observed thus if a novel neverbeforeseen data point is observed it should be assumed to be negative
if data has not previously been ruled in then its ruled out
the most general hypotheses i
the general boundary gb cover the observed positive training examples but also cover as much of the remaining feature space without including any negative training examples
these if enlarged any further include a negative training example and hence become inconsistent
these maximal hypotheses essentially constitute a optimistic claim that the true concept is defined just by the negative data already observed thus if a novel neverbeforeseen data point is observed it should be assumed to be positive
if data has not previously been ruled out then its ruled in
thus during learning the version space which itself is a set  possibly infinite  containing all consistent hypotheses can be represented by just its lower and upper bounds maximally general and maximally specific hypothesis sets and learning operations can be performed just on these representative sets
after learning classification can be performed on unseen examples by testing the hypothesis learned by the algorithm
if the example is consistent with multiple hypotheses a majority vote rule can be applied
the notion of version spaces was introduced by mitchell in the early s as a framework for understanding the basic problem of supervised learning within the context of solution search
although the basic candidate elimination search method that accompanies the version space framework is not a popular learning algorithm there are some practical implementations that have been developed e
sverdlik  reynolds  hong  tsang  dubois  quafafou
a major drawback of version space learning is its inability to deal with noise any pair of inconsistent examples can cause the version space to collapse i
become empty so that classification becomes impossible
statistical learning theory is a framework for machine learningdrawing from the fields of statistics and functional analysis
statistical learning theory deals with the problem of finding a predictive function based on data
statistical learning theory has led to successful applications in fields such as computer vision speech recognition bioinformatics and baseball
the goals of learning are understanding and prediction
learning falls into many categories including supervised learning unsupervised learning online learning and reinforcement learning
from the perspective of statistical learning theory supervised learning is best understood
supervised learning involves learning from a training set of data
every point in the training is an inputoutput pair where the input maps to an output
the learning problem consists of inferring the function that maps between the input and the output such that the learned function can be used to predict output from future input
depending on the type of output supervised learning problems are either problems of regression or problems of classification
if the output takes a continuous range of values it is a regression problem
using ohms law as an example a regression could be performed with voltage as input and current as output
the regression would find the functional relationship between voltage and current to be  such thatclassification problems are those for which the output will be an element from a discrete set of labels
classification is very common for machine learning applications
in facial recognition for instance a picture of a persons face would be the input and the output label would be that persons name
the input would be represented by a large multidimensional vector whose elements represent pixels in the picture
after learning a function based on the training set data that function is validated on a test set of data data that did not appear in the training set
take formula to be the vector space of all possible inputs and formula to bethe vector space of all possible outputs
statistical learning theory takes the perspective that there is some unknown probability distribution over the product space formula i
there exists some unknown formula
the training set is made up of formula samples from this probability distribution and is notated every formula is an input vector from the training data and formulais the output that corresponds to it
in this formalism the inference problem consists of finding a function formula such that formula
let formula be a space of functions formula called the hypothesis space
the hypothesis space is the space of functions the algorithm will search through
let formula be the loss function a metric for the difference between the predicted value formula and the actual value formula
the expected risk is defined to bethe target function the best possible function formula that can bechosen is given by the formula that satisfiesbecause the probability distribution formula is unknown aproxy measure for the expected risk must be used
this measure is based on the training set a sample from this unknown probability distribution
it is called the empirical riska learning algorithm that chooses the function formula that minimizesthe empirical risk is called empirical risk minimization
the choice of loss function is a determining factor on the function formula that will be chosen by the learning algorithm
the loss functionalso affects the convergence rate for an algorithm
it is important for the loss function to be convex
different loss functions are used depending on whether the problem isone of regression or one of classification
the most common loss function for regression is the square loss function also known as the lnorm
this familiar loss function is used in ordinary least squares regression
the form isthe absolute value loss also known as the lnorm is also sometimes usedin some sense the  indicator function is the most natural loss function for classification
it takes the value  if the predicted output is the same as the actual output and it takes the value  if the predicted output is different from the actual output
for binary classification with formula this iswhere formula is the heaviside step function
in machine learning problems a major problem that arises is that of overfitting
because learning is a prediction problem the goal is not to find a function that most closely fits the previously observed data but to find one that will most accurately predict output from future input
empirical risk minimization runs this risk of overfitting finding a function that matches the data exactly but does not predict future output well
overfitting is symptomatic of unstable solutions a small perturbation in the training set data would cause a large variation in the learned function
it can be shown that if the stability for the solution can be guaranteed generalization and consistency are guaranteed as well
regularization can solve the overfitting problem and givethe problem stability
regularization can be accomplished by restricting the hypothesis space formula
a common example would be restricting formula to linear functions this can be seen as a reduction to the standard problem of linear regression
formula could also be restricted to polynomial of degree formula exponentials or bounded functions on l
restriction of the hypothesis space avoids overfitting because the form of the potential functions are limited and so does not allow for the choice of a function that gives empirical risk arbitrarily close to zero
one example of regularization is tikhonov regularization
this consists of minimizingwhere formula is a fixed and positive parameter the regularization parameter
tikhonov regularization ensures existence uniqueness and stability of the solution
in mathematics and statistics random projection is a technique used to reduce the dimensionality of a set of points which lie in euclidean space
random projection methods are powerful methods known for their simplicity and less erroneous output compared with other methods
according to experimental results random projection preserve distances well but empirical results are sparse
they have been applied to many natural language tasks under the name of random indexing
dimensionality reduction as the name suggests is reducing the number of random variables using various mathematical methods from statistics and machine learning
dimensionality reduction is often used to reduce the problem of managing and manipulating large data sets
dimensionality reduction techniques generally use linear transformations in determining the intrinsic dimensionality of the manifold as well as extracting its principal directions
for this purpose there are various related techniques including principal component analysis linear discriminant analysis canonical correlation analysis discrete cosine transform random projection etc
random projection is a simple and computationally efficient way to reduce the dimensionality of data by trading a controlled amount of error for faster processing times and smaller model sizes
the dimensions and distribution of random projection matrices are controlled so as to approximately preserve the pairwise distances between any two samples of the dataset
the core idea behind random projection is given in the johnsonlindenstrauss lemma which states that if points in a vector space are of sufficiently high dimension then they may be projected into a suitable lowerdimensional space in a way which approximately preserves the distances between the points
in random projection the original ddimensional data is projected to a kdimensional k  d subspace using a random formula  dimensional matrix r whose rows have unit lengths
using matrix notation if formula is the original set of n ddimensional observations then formula is the projection of the data onto a lower kdimensional subspace
random projection is computationally simple form the random matrix r and project the formula data matrix x onto k dimensions of order formula
if the data matrix x is sparse with about c nonzero entries per column then the complexity of this operation is of order formula
the random matrix r can be generated using a gaussian distribution
the first row is a random unit vector uniformly chosen from formula
the second row is a random unit vector from the space orthogonal to the first row the third row is a random unit vector from the space orthogonal to the first two rows and so on
in this way of choosing r r is an orthogonal matrix the inverse of its transpose and the following properties are satisfiedachlioptas has shown that the gaussian distribution can be replaced by a much simpler distribution such asthis is efficient for database applications because the computations can be performed using integer arithmetic
it was later shown how to use integer arithmetic while making the distribution even sparser having very few nonzeroes per column in work on the sparse jl transform
this is advantageous since a sparse embedding matrix means being able to project the data to lower dimension even faster
the johnsonlindenstrauss lemma states that large sets of vectors in a highdimensional space can be linearly mapped in a space of much lower but still high dimension n with approximate preservation of distances
one of the explanations of this effect is the exponentially high quasiorthogonal dimension of ndimensional euclidean space
there are exponentially large in dimension n sets of almost orthogonal vectors with small value of inner products in ndimensional euclidean space
this observation is useful in indexing of highdimensional data
quasiorthogonality of large random sets is important for methods of random approximation in machine learning
in high dimensions exponentially large numbers of randomly and independently chosen vectors from equidistribution on a sphere and from many other distributions are almost orthogonal with probability close to one
this implies that in order to represent an element of such a highdimensional space by linear combinations of randomly and independently chosen vectors it may often be necessary to generate samples of exponentially large length if we use bounded coefficients in linear combinations
on the other hand if coefficients with arbitrarily large values are allowed the number of randomly generated elements that are sufficient for approximation is even less than dimension of the data space
random forests or random decision forests are an ensemble learning method for classification regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes classification or mean prediction regression of the individual trees
random decision forests correct for decision trees habit of overfitting to their training set
the first algorithm for random decision forests was created by tin kam ho using the random subspace method which in hos formulation is a way to implement the stochastic discrimination approach to classification proposed by eugene kleinberg
an extension of the algorithm was developed by leo breiman and adele cutler and random forests is their trademark
the extension combines breimans bagging idea and random selection of features introduced first by ho and later independently by amit and geman in order to construct a collection of decision trees with controlled variance
the general method of random decision forests was first proposed by ho in  who established that forests of trees splitting with oblique hyperplanes if randomly restricted to be sensitive to only selected feature dimensions can gain accuracy as they grow without suffering from overtraining
a subsequent work along the same lines concluded that other splitting methods as long as they are randomly forced to be insensitive to some feature dimensions behave similarly
note that this observation of a more complex classifier a larger forest getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting
the explanation of the forest methods resistance to overtraining can be found in kleinbergs theory of stochastic discrimination
the early development of breimans notion of random forests was influenced by the work of amit andgeman who introduced the idea of searching over a random subset of theavailable decisions when splitting a node in the context of growing a singletree
the idea of random subspace selection from ho was also influential in the design of random forests
in this method a forest of trees is grownand variation among the trees is introduced by projecting the training datainto a randomly chosen subspace before fitting each tree or each node
finally the idea ofrandomized node optimization where the decision at each node is selected by arandomized procedure rather than a deterministic optimization was firstintroduced by dietterich
the introduction of random forests proper was first made in a paperby leo breiman
this paper describes a method of building a forest ofuncorrelated trees using a cart like procedure combined with randomized nodeoptimization and bagging
in addition this paper combines severalingredients some previously known and some novel which form the basis of themodern practice of random forests in particularthe report also offers the first theoretical result for random forests in theform of a bound on the generalization error which depends on the strength of thetrees in the forest and their correlation
decision trees are a popular method for various machine learning tasks
tree learning comes closest to meeting the requirements for serving as an offtheshelf procedure for data mining say hastie et al
because it is invariant under scaling and various other transformations of feature values is robust to inclusion of irrelevant features and produces inspectable models
however they are seldom accurate
in particular trees that are grown very deep tend to learn highly irregular patterns they overfit their training sets i
have low bias but very high variance
random forests are a way of averaging multiple deep decision trees trained on different parts of the same training set with the goal of reducing the variance
this comes at the expense of a small increase in the bias and some loss of interpretability but generally greatly boosts the performance in the final model
the training algorithm for random forests applies the general technique of bootstrap aggregating or bagging to tree learners
given a training set
with responses
bagging repeatedly b times selects a random sample with replacement of the training set and fits trees to these samplesafter training predictions for unseen samples can be made by averaging the predictions from all the individual regression trees on or by taking the majority vote in the case of classification trees
this bootstrapping procedure leads to better model performance because it decreases the variance of the model without increasing the bias
this means that while the predictions of a single tree are highly sensitive to noise in its training set the average of many trees is not as long as the trees are not correlated
simply training many trees on a single training set would give strongly correlated trees or even the same tree many times if the training algorithm is deterministic bootstrap sampling is a way of decorrelating the trees by showing them different training sets
additionally an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on the number of samplestrees  is a free parameter
typically a few hundred to several thousand trees are used depending on the size and nature of the training set
an optimal number of trees can be found using crossvalidation or by observing the outofbag error the mean prediction error on each training sample  using only the trees that did not have in their bootstrap sample
the training and test error tend to level off after some number of trees have been fit
the above procedure describes the original bagging algorithm for trees
random forests differ in only one way from this general scheme they use a modified tree learning algorithm that selects at each candidate split in the learning process a random subset of the features
this process is sometimes called feature bagging
the reason for doing this is the correlation of the trees in an ordinary bootstrap sample if one or a few features are very strong predictors for the response variable target output these features will be selected in many of the trees causing them to become correlated
an analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by ho
typically for a classification problem with features rounded down features are used in each split
for regression problems the inventors recommend rounded down with a minimum node size of  as the default
adding one further step of randomization yields extremely randomized trees or extratrees
these are trained using bagging and the random subspace method like in an ordinary random forest but additionally the topdown splitting in the tree learner is randomized
instead of computing the locally optimal featuresplit combination based on e
information gain or the gini impurity for each feature under consideration a random value is selected for the split
this value is selected from the features empirical range in the trees training set i
the bootstrap sample
random forests can be used to rank the importance of variables in a regression or classification problem in a natural way
the following technique was described in breimans original paper and is implemented in the r package randomforest
the first step in measuring the variable importance in a data set formula is to fit a random forest to the data
during the fitting process the outofbag error for each data point is recorded and averaged over the forest errors on an independent test set can be substituted if bagging is not used during training
to measure the importance of the formulath feature after training the values of the formulath feature are permuted among the training data and the outofbag error is again computed on this perturbed data set
the importance score for the formulath feature is computed by averaging the difference in outofbag error before and after the permutation over all trees
the score is normalized by the standard deviation of these differences
features which produce large values for this score are ranked as more important than features which produce small values
the statistical definition of the variable importance measure was given and analyzed by zhu et al
this method of determining variable importance has some drawbacks
for data including categorical variables with different number of levels random forests are biased in favor of those attributes with more levels
methods such as partial permutationsand growing unbiased treescan be used to solve the problem
if the data contain groups of correlated features of similar relevance for the output then smaller groups are favored over larger groups
a relationship between random forests and the nearest neighbor algorithm nn was pointed out by lin and jeon in
it turns out that both can be viewed as socalled weighted neighborhoods schemes
these are models built from a training set formula that make predictions formula for new points by looking at the neighborhood of the point formalized by a weight function here formula is the nonnegative weight of the th training point relative to the new point in the same tree
for any particular  the weights for points formula must sum to one
weight functions are given as followssince a forest averages the predictions of a set of trees with individual weight functions formula its predictions arethis shows that the whole forest is again a weighted neighborhood scheme with weights that average those of the individual trees
the neighbors of in this interpretation are the points formula sharing the same leaf in any tree formula
in this way the neighborhood of depends in a complex way on the structure of the trees and thus on the structure of the training set
lin and jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature
as part of their construction random forest predictors naturally lead to a dissimilarity measure among the observations
one can also define a random forest dissimilarity measure between unlabeled data the idea is to construct a random forest predictor that distinguishes the observed data from suitably generated synthetic data
the observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution
a random forest dissimilarity can be attractive because it handles mixed variable types very well is invariant to monotonic transformations of the input variables and is robust to outlying observations
the random forest dissimilarity easily deals with a large number of semicontinuous variables due to its intrinsic variable selection for example the addcl  random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables
the random forest dissimilarity has been used in a variety of applications e
to find clusters of patients based on tissue marker data
instead of decision trees linear models have been proposed and evaluated as base estimators in random forests in particular multinomial logistic regression and naive bayes classifiers
in machine learning kernel random forests establish the connection between random forests and kernel methods
by slightly modifying their definition random forests can be rewritten as kernel methods which are more interpretable and easier to analyze
leo breiman was the first person to notice the link between random forest and kernel methods
he pointed out that random forests which are grown using i
random vectors in the tree construction are equivalent to a kernel acting on the true margin
lin and jeon established the connection between random forests and adaptive nearest neighbor implying that random forests can be seen as adaptive kernel estimates
davies and ghahramani proposed random forest kernel and show that it can empirically outperform stateofart kernel methods
scornet first defined kerf estimates and gave the explicit link between kerf estimates and random forest
he also gave explicit expressions for kernels based on centered random forest and uniform random forest two simplified models of random forest
he named these two kerfs centered kerf and uniform kerf and proved upper bounds on their rates of consistency
centered forest is a simplified model for breimans original random forest which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the prechosen attribute
the algorithm stops when a fully binary tree of level formula is built where formula is a parameter of the algorithm
uniform forest is another simplified model for breimans original random forest which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell along the preselected feature
given a training sample formula of formulavalued independent random variables distributed as the independent prototype pair formula where formula
we aim at predicting the response formula associated with the random variable formula by estimating the regression function formula
a random regression forest is an ensemble of formula randomized regression trees
denote formula the predicted value at point formula by the formulath tree where formula are independent random variables distributed as a generic random variable formula independent of the sample formula
this random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction
the trees are combined to form the finite forest estimate formula
for regression trees we have formula where formula is the cell containing formula designed with randomness formula and dataset formula and formula
thus random forest estimates satisfy for all formula formula
random regression forest has two level of averaging first over the samples in the target cell of a tree then over all trees
thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells
in order to improve the random forest methods and compensate the misestimation scornet defined kerf bywhich is equal to the mean of the formulas falling in the cells containing formula in the forest
if we define the connection function of the formula finite forest as formula i
the proportion of cells shared between formula and formula then almost surely we have formula which defines the kerf
the construction of centered kerf of level formula is the same as for centered forest except that predictions are made by formula the corresponding kernel function or connection function isuniform kerf is built in the same way as uniform forest except that predictions are made by formula the corresponding kernel function or connection function ispredictions given by kerf and random forests are close if the number of points in each cell is controlledassume that there exist sequences formula such that almost surelythen almost surelywhen the number of trees formula goes to infinity then we have infinite random forest and infinite kerf
their estimates are close if the number of observations in each cell is boundedassume that there exist sequences formula such that almost surelythen almost surelyassume that formula where formula is a centered gaussian noise independent of formula with finite variance formula
moreover formula is uniformly distributed on formula and formula is lipschitz
scornet proved upper bounds on the rates of consistency for centered kerf and uniform kerf
providing formula and formula there exists a constant formula such that for all formulaformula
providing formula and formula there exists a constant formula such thatformula
the algorithm is often used in scientific works because of its advantages
for example it can be used for quality assessment of wikipedia articles
opennn open neural networks library is a software library written in the c programming language which implements neural networks a main area of deep learning research
the library is open source licensed under the gnu lesser general public license
the software implements any number of layers of nonlinear processing units for supervised learning
this deep architecture allows the design of neural networks with universal approximation properties
additionally it allows multiprocessing programming by means of openmp in order to increase computer performance
opennn contains data mining algorithms as a bundle of functions
these can be embedded in other software tools using an application programming interface for the integration of the predictive analytics tasks
in this regard a graphical user interface is missing but some functions can be supported by specific visualization tools
the development started in  at the international center for numerical methods in engineering cimne within the research project funded by the european union called ramflood risk assessment and management of floods
then it continued as part of similar projects
at present opennn is being developed by the startup company artelnics
opennn is a general purpose artificial intelligence software package
it uses machine learning techniques for solving data mining and predictive analytics tasks in different fields
for instance the library has been applied in the engineering energy or chemistry sectors
in logic statistical inference and supervised learningtransduction or transductive inference is reasoning fromobserved specific training cases to specific test cases
in contrastinduction is reasoning from observed training casesto general rules which are then applied to the test cases
the distinction ismost interesting in cases where the predictions of the transductive model arenot achievable by any inductive model
note that this is caused by transductiveinference on different test sets producing mutually inconsistent predictions
transduction was introduced by vladimir vapnik in the s motivated byhis view that transduction is preferable to induction since according to him induction requiressolving a more general problem inferring a function before solving a morespecific problem computing outputs for new cases when solving a problem ofinterest do not solve a more general problem as an intermediate step
try toget the answer that you really need but not a more general one
a similarobservation had been made earlier by bertrand russellwe shall reach the conclusion that socrates is mortal with a greater approach to certainty if we make our argument purely inductive than if we go by way of all men are mortal and then use deduction russell  chap vii
an example of learning which is not inductive would be in the case of binaryclassification where the inputs tend to cluster in two groups
a large set oftest inputs may help in finding the clusters thus providing useful informationabout the classification labels
the same predictions would not be obtainablefrom a model which induces a function based only on the training cases
somepeople may call this an example of the closely related semisupervised learning since vapniks motivation is quite different
an example of an algorithm in this category is the transductive support vector machine tsvm
a third possible motivation which leads to transduction arises through the needto approximate
if exact inference is computationally prohibitive one may atleast try to make sure that the approximations are good at the test inputs
inthis case the test inputs could come from an arbitrary distribution notnecessarily related to the distribution of the training inputs which wouldntbe allowed in semisupervised learning
an example of an algorithm falling inthis category is the bayesian committee machine bcm
the following example problem contrasts some of the unique properties of transduction against induction
a collection of points is given such that some of the points are labeled a b or c but most of the points are unlabeled
the goal is to predict appropriate labels for all of the unlabeled points
the inductive approach to solving this problem is to use the labeled points to train a supervised learning algorithm and then have it predict labels for all of the unlabeled points
with this problem however the supervised learning algorithm will only have five labeled points to use as a basis for building a predictive model
it will certainly struggle to build a model that captures the structure of this data
for example if a nearestneighbor algorithm is used then the points near the middle will be labeled a or c even though it is apparent that they belong to the same cluster as the point labeled b
transduction has the advantage of being able to consider all of the points not just the labeled points while performing the labeling task
in this case transductive algorithms would label the unlabeled points according to the clusters to which they naturally belong
the points in the middle therefore would most likely be labeled b because they are packed very close to that cluster
an advantage of transduction is that it may be able to make better predictions with fewer labeled points because it uses the natural breaks found in the unlabeled points
one disadvantage of transduction is that it builds no predictive model
if a previously unknown point is added to the set the entire transductive algorithm would need to be repeated with all of the points in order to predict a label
this can be computationally expensive if the data is made available incrementally in a stream
further this might cause the predictions of some of the old points to change which may be good or bad depending on the application
a supervised learning algorithm on the other hand can label new points instantly with very little computational cost
transduction algorithms can be broadly divided into two categories those that seek to assign discrete labels to unlabeled points and those that seek to regress continuous labels for unlabeled points
algorithms that seek to predict discrete labels tend to be derived by adding partial supervision to a clustering algorithm
these can be further subdivided into two categories those that cluster by partitioning and those that cluster by agglomerating
algorithms that seek to predict continuous labels tend to be derived by adding partial supervision to a manifold learning algorithm
partitioning transduction can be thought of as topdown transduction
it is a semisupervised extension of partitionbased clustering
it is typically performed as followsof course any reasonable partitioning technique could be used with this algorithm
max flow min cut partitioning schemes are very popular for this purpose
agglomerative transduction can be thought of as bottomup transduction
it is a semisupervised extension of agglomerative clustering
it is typically performed as followsmanifoldlearningbased transduction is still a very young field of research
expectation propagation ep is a technique in bayesian machine learning
ep finds approximations to a probability distribution
it uses an iterative approach that leverages the factorization structure of the target distribution
it differs from other bayesian approximation approaches such as variational bayesian methods
the journal of machine learning research is a peerreviewed open access scientific journal covering machine learning
it was established in  and the first editorinchief was leslie kaelbling
the current editorsinchief are kevin murphy google and bernhard schlkopf max planck institute for intelligent systems
the journal was established as an openaccess alternative to the journal machine learning
in  forty editorial board members of machine learning resigned saying that in the era of the internet it was detrimental for researchers to continue publishing their papers in expensive journals with payaccess archives
the open access model employed by the journal of machine learning research allows authors to publish articles for free and retain copyright while archives are freely available online
print editions of the journal were published by mit press until  and by microtome publishing thereafter
from its inception the journal received no revenue from the print edition and paid no subvention to mit press or microtome publishing
in response to the prohibitive costs of arranging workshop and conference proceedings publication with traditional academic publishing companies the journal launched a proceedings publication arm in  and now publishes proceedings for several leading machine learning conferences including the international conference on machine learning colt aistats and workshops held at the conference on neural information processing systems
meta learning is a subfield of machine learning where automatic learning algorithms are applied on metadata about machine learning experiments
as of  the term had not found a standard interpretation however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems hence to improve the performance of existing learning algorithms or to learn induce the learning algorithm itself hence the alternative term learning to learn
flexibility is important because each learning algorithm is based on a set of assumptions about the data its inductive bias
this means that it will only learn well if the bias matches the learning problem
a learning algorithm may perform very well in one domain but not on the next
this poses strong restrictions on the use of machine learning or data mining techniques since the relationship between the learning problem often some kind of database and the effectiveness of different learning algorithms is not yet understood
by using different kinds of metadata like properties of the learning problem algorithm properties like performance measures or patterns previously derived from the data it is possible to learn select alter or combine different learning algorithms to effectively solve a given learning problem
critiques of meta learning approaches bear a strong resemblance to the critique of metaheuristic a possibly related problem
a good analogy to metalearning and the inspiration for bengio et al
s early work  considers that genetic evolution learns the learning procedure encoded in genes and executed in each individuals brain
a proposed definition for a meta learning system combines three requirementsbias refers to the assumptions that influence the choice of explanatory hypotheses and not the notion of bias represented in the biasvariance dilemma
meta learning is concerned with two aspects of learning bias
some approaches which have been viewed as instances of meta learning
neural modeling field nmf is a mathematical framework for machine learning which combines ideas from neural networks fuzzy logic and model based recognition
it has also been referred to as modeling fields modeling fields theory mft maximum likelihood artificial neural networks mlans
this framework has been developed by leonid perlovsky at the afrl
nmf is interpreted as a mathematical description of minds mechanisms including concepts emotions instincts imagination thinking and understanding
nmf is a multilevel heterohierarchical system
at each level in nmf there are conceptmodels encapsulating the knowledge they generate socalled topdown signals interacting with input bottomup signals
these interactions are governed by dynamic equations which drive conceptmodel learning adaptation and formation of new conceptmodels for better correspondence to the input bottomup signals
in the general case nmf system consists of multiple processing levels
at each level output signals are the concepts recognized in or formed from input bottomup signals
input signals are associated with or recognized or grouped into concepts according to the models and at this level
in the process of learning the conceptmodels are adapted for better representation of the input signals so that similarity between the conceptmodels and signals increases
this increase in similarity can be interpreted as satisfaction of an instinct for knowledge and is felt as aesthetic emotions
each hierarchical level consists of n neurons enumerated by index n
these neurons receive input bottomup signals xn from lower levels in the processing hierarchy
xn is a field of bottomup neuronal synaptic activations coming from neurons at a lower level
each neuron has a number of synapses for generality each neuron activation is described as a set of numbers  where d is the number or dimensions necessary to describe individual neurons activation
topdown or priming signals to these neurons are sent by conceptmodels msn  where m is the number of models
each model is characterized by its parameters s in the neuron structure of the brain they are encoded by strength of synaptic connections mathematically they are given by a set of numbers  where a is the number of dimensions necessary to describe invividual model
models represent signals in the following way
suppose that signal xn is coming from sensory neurons n activated by object m which is characterized by parameters s
these parameters may include position orientation or lighting of an object m
model msn predicts a value xn of a signal at neuron n
for example during visual perception a neuron n in the visual cortex receives a signal xn from retina and a priming signal msn from an objectconceptmodel m
neuron n is activated if both the bottomup signal from lowerlevelinput and the topdown priming signal are strong
various models compete for evidence in the bottomup signals while adapting their parameters for better match as described below
this is a simplified description of perception
the most benign everyday visual perception uses many levels from retina to object perception
the nmf premise is that the same laws describe the basic interaction dynamics at each level
perception of minute features or everyday objects or cognition of complex abstract concepts is due to the same mechanism described below
perception and cognition involve conceptmodels and learning
in perception conceptmodels correspond to objects in cognition models correspond to relationships and situations
learning is an essential part of perception and cognition and in nmf theory it is driven by the dynamics that increase a similarity measure between the sets of models and signals lxm
the similarity measure is a function of model parameters and associations between the input bottomup signals and topdown conceptmodel signals
in constructing a mathematical description of the similarity measure it is important to acknowledge two principlestherefore the similarity measure is constructed so that it accounts for all bottomup signals xnthis expression contains a product of partial similarities lxn over all bottomup signals therefore it forces the nmf system to account for every signal even if one term in the product is zero the product is zero the similarity is low and the knowledge instinct is not satisfied this is a reflection of the first principle
second before perception occurs the mind does not know which object gave rise to a signal from a particular retinal neuron
therefore a partial similarity measure is constructed so that it treats each model as an alternative a sum over conceptmodels for each input neuron signal
its constituent elements are conditional partial similarities between signal xn and model m lxnm
this measure is conditional on object m being present therefore when combining these quantities into the overall similarity measure l they are multiplied by rm which represent a probabilistic measure of object m actually being present
combining these elements with the two principles noted above a similarity measure is constructed as follows the structure of the expression above follows standard principles of the probability theory a summation is taken over alternatives m and various pieces of evidence n are multiplied
this expression is not necessarily a probability but it has a probabilistic structure
if learning is successful it approximates probabilistic description and leads to nearoptimal bayesian decisions
the name conditional partial similarity for lxnm or simply lnm follows the probabilistic terminology
if learning is successful lnm becomes a conditional probability density function a probabilistic measure that signal in neuron n originated from object m
then l is a total likelihood of observing signals xn coming from objects described by conceptmodel m
coefficients rm called priors in probability theory contain preliminary biases or expectations expected objects m have relatively high rm values their true values are usually unknown and should be learned like other parameters s
note that in probability theory a product of probabilities usually assumes that evidence is independent
expression for l contains a product over n but it does not assume independence among various signals xn
there is a dependence among signals due to conceptmodels each model msn predicts expected signal values in many neurons n
during the learning process conceptmodels are constantly modified
usually the functional forms of models msn are all fixed and learningadaptation involves only model parameters s
from time to time a system forms a new concept while retaining an old one as well alternatively old concepts are sometimes merged or eliminated
this requires a modification of the similarity measure l the reason is that more models always result in a better fit between the models and data
this is a well known problem it is addressed by reducing similarity l using a skeptic penalty function penalty method pnm that grows with the number of models m and this growth is steeper for a smaller amount of data n
for example an asymptotically unbiased maximum likelihood estimation leads to multiplicative pnm  expn where n is a total number of adaptive parameters in all models this penalty function is known as akaike information criterion see perlovsky  for further discussion and references
the learning process consists of estimating model parameters s and associating signals with concepts by maximizing the similarity l
note that all possible combinations of signals and models are accounted for in expression  for l
this can be seen by expanding a sum and multiplying all the terms resulting in m items a huge number
this is the number of combinations between all signals n and all models m
this is the source of combinatorial complexity which is solved in nmf by utilizing the idea of dynamic logic
an important aspect of dynamic logic is matching vagueness or fuzziness of similarity measures to the uncertainty of models
initially parameter values are not known and uncertainty of models is high so is the fuzziness of the similarity measures
in the process of learning models become more accurate and the similarity measure more crisp the value of the similarity increases
the maximization of similarity l is done as follows
first the unknown parameters s are randomly initialized
then the association variables fmn are computedequation for fmn looks like the bayes formula for a posteriori probabilities if lnm in the result of learning become conditional likelihoods fmn become bayesian probabilities for signal n originating from object m
the dynamic logic of the nmf is defined as followsthe following theorem has been proved perlovsky theorem
equations   and  define a convergent dynamic nmf system with stationary states defined by maxsl
it follows that the stationary states of an mf system are the maximum similarity states
when partial similarities are specified as probability density functions pdf or likelihoods the stationary values of parameters s are asymptotically unbiased and efficient estimates of these parameters
the computational complexity of dynamic logic is linear in n
practically when solving the equations through successive iterations fmn can be recomputed at every iteration using  as opposed to incremental formula
the proof of the above theorem contains a proof that similarity l increases at each iteration
this has a psychological interpretation that the instinct for increasing knowledge is satisfied at each step resulting in the positive emotions nmfdynamic logic system emotionally enjoys learning
finding patterns below noise can be an exceedingly complex problem
if an exact pattern shape is not known and depends on unknown parameters these parameters should be found by fitting the pattern model to the data
however when the locations and orientations of patterns are not known it is not clear which subset of the data points should be selected for fitting
a standard approach for solving this kind of problem is multiple hypothesis testing singer et al
since all combinations of subsets and models are exhaustively searched this method faces the problem of combinatorial complexity
in the current example noisy smile and frown patterns are sought
they are shown in fig
a without noise and in fig
b with the noise as actually measured
the true number of patterns is  which is not known
therefore at least  patterns should be fit to the data to decide that  patterns fit best
the image size in this example is x   points
if one attempts to fit  models to all subsets of  data points computation of complexity m
an alternative computation by searching through the parameter space yields lower complexity each pattern is characterized by a parameter parabolic shape
fitting x parameters to x grid by a bruteforce testing would take about  to  operations still a prohibitive computational complexity
to apply nmf and dynamic logic to this problem one needs to develop parametric adaptive models of expected patterns
the models and conditional partial similarities for this case are described in details in a uniform model for noise gaussian blobs for highlyfuzzy poorly resolved patterns and parabolic models for smiles and frowns
the number of computer operations in this example was about
thus a problem that was not solvable due to combinatorial complexity becomes solvable using dynamic logic
during an adaptation process initially fuzzy and uncertain models are associated with structures in the input signals and fuzzy models become more definite and crisp with successive iterations
the type shape and number of models are selected so that the internal representation within the system is similar to input signals the nmf conceptmodels represent structureobjects in the signals
the figure below illustrates operations of dynamic logic
in fig
a true smile and frown patterns are shown without noise b actual image available for recognition signal is below noise signaltonoise ratio is between db and
db c an initial fuzzy model a large fuzziness corresponds to uncertainty of knowledge d through m show improved models at various iteration stages total of  iterations
every five iterations the algorithm tried to increase or decrease the number of models
between iterations d and e the algorithm decided that it needs three gaussian models for the best fit
there are several types of models one uniform model describing noise it is not shown and a variable number of blob models and parabolic models their number location and curvature are estimated from the data
until about stage g the algorithm used simple blob models at g and beyond the algorithm decided that it needs more complex parabolic models to describe the data
iterations stopped at h when similarity stopped increasing
above a single processing level in a hierarchical nmf system was described
at each level of hierarchy there are input signals from lower levels models similarity measures l emotions which are defined as changes in similarity and actions actions include adaptation behavior satisfying the knowledge instinct  maximization of similarity
an input to each level is a set of signals xn or in neural terminology an input field of neuronal activations
the result of signal processing at a given level are activated models or concepts m recognized in the input signals n these models along with the corresponding instinctual signals and emotions may activate behavioral models and generate behavior at this level
the activated models initiate other actions
they serve as input signals to the next processing level where more general conceptmodels are recognized or created
output signals from a given level serving as input to the next level are the model activation signals a defined asa   fmn
the hierarchical nmf system is illustrated in fig
within the hierarchy of the mind each conceptmodel finds its mental meaning and purpose at a higher level in addition to other purposes
for example consider a conceptmodel chair
it has a behavioral purpose of initiating sitting behavior if sitting is required by the body this is the bodily purpose at the same hierarchical level
in addition it has a purely mental purpose at a higher level in the hierarchy a purpose of helping to recognize a more general concept say of a concert hall a model of which contains rows of chairs
from time to time a system forms a new concept or eliminates an old one
at every level the nmf system always keeps a reserve of vague fuzzy inactive conceptmodels
they are inactive in that their parameters are not adapted to the data therefore their similarities to signals are low
yet because of a large vagueness covariance the similarities are not exactly zero
when a new signal does not fit well into any of the active models its similarities to inactive models automatically increase because first every piece of data is accounted for and second inactive models are vaguefuzzy and potentially can grab every signal that does not fit into more specific less fuzzy active models
when the activation signal a for an inactive model m exceeds a certain threshold the model is activated
similarly when an activation signal for a particular model falls below a threshold the model is deactivated
thresholds for activation and deactivation are set usually based on information existing at a higher hierarchical level prior information system resources numbers of activated models of various types etc
activation signals for active models at a particular level  a  form a neuronal field which serve as input signals to the next level where more abstract and more general concepts are formed
random indexing is a dimensionality reduction method and computational framework for distributional semantics based on the insight that veryhighdimensional vector space model implementations are impractical that models need not grow in dimensionality when new items e
new terminology is encountered and that a highdimensional model can be projected into a space of lower dimensionality without compromising l distance metrics if the resulting dimensions are chosen appropriately
this is the original point of the random projection approach to dimension reduction first formulated as the johnsonlindenstrauss lemma and localitysensitive hashing has some of the same starting points
random indexing as used in representation of language originates from the work of pentti kanerva on sparse distributed memory and can be described as an incremental formulation of a random projection
it can be also verified that random indexing is a random projection technique for the construction of euclidean spacesi
l normed vector spaces
in euclidean spaces random projections are elucidated using the johnsonlindenstrauss lemma
the topsig technique extends the random indexing model to produce bit vectors for comparison with the hamming distance similarity function
it is used for improving the performance of information retrieval and document clustering
in a similar line of research random manhattan integer indexing rmii is proposed for improving the performance of the methods that employ the manhattan distance between text units
many random indexing methods primarily generate similarity from cooccurrence of items in a corpus
reflexive random indexing rri generates similarity from cooccurrence and from shared occurrence with other items
preference learning is a subfield in machine learning in which the goal is to learn a predictive preference model from observed preference information
in the view of supervised learning preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items
while the concept of preference learning has been emerged for some time in many fields such as economics its a relatively new topic in artificial intelligence research
several workshops have been discussing preference learning and related topics in the past decade
the main task in preference learning concerns problems in learning to rank
according to different types of preference information observed the tasks are categorized as three main problems in the book preference learningin label ranking the model has an instance space formula and a finite set of labels formula
the preference information is given in the form formula indicating instance formula shows preference in formula rather than formula
a set of preference information is used as training data in the model
the task of this model is to find a preference ranking among the labels for any instance
it was observed some conventional classification problems can be generalized in the framework of label ranking problem if a training instance formula is labeled as class formula it implies that formula
in the multilabel case formula is associated with a set of labels formula and thus the model can extract a set of preference information formula
training a preference model on this preference information and the classification result of an instance is just the corresponding top ranking label
instance ranking also has the instance space formula and label set formula
in this task labels are defined to have a fixed order formula and each instance formula is associated with a label formula
giving a set of instances as training data the goal of this task is to find the ranking order for a new set of instances
object ranking is similar to instance ranking except that no labels are associated with instances
given a set of pairwise preference information in the form formula and the model should find out a ranking order among instances
there are two practical representations of the preference information formula
one is assigning formula and formula with two real numbers formula and formula respectively such that formula
another one is assigning a binary value formula for all pairs formula denoting whether formula or formula
corresponding to these two different representations there are two different techniques applied to the learning process
if we can find a mapping from data to real numbers ranking the data can be solved by ranking the real numbers
this mapping is called utility function
for label ranking the mapping is a function formula such that formula
for instance ranking and object ranking the mapping is a function formula
finding the utility function is a regression learning problem which is well developed in machine learning
the binary representation of preference information is called preference relation
for each pair of alternatives instances or labels a binary predicate can be learned by conventional supervising learning approach
frnkranz and hllermeier proposed this approach in label ranking problem
for object ranking there is an early approach by cohen et al
using preference relations to predict the ranking will not be so intuitive
since preference relation is not transitive it implies that the solution of ranking satisfying those relations would sometimes be unreachable or there could be more than one solution
a more common approach is to find a ranking solution which is maximally consistent with the preference relations
this approach is a natural extension of pairwise classification
preference learning can be used in ranking search results according to feedback of user preference
given a query and a set of documents a learning model is used to find the ranking of documents corresponding to the relevance with this query
more discussions on research in this field can be found in tieyan lius survey paper
another application of preference learning is recommender systems
online store may analyze customers purchase record to learn a preference model and then recommend similar products to customers
internet content providers can make use of users ratings to provide more user preferred contents
elastic matching is one of the pattern recognition techniques in computer science
elastic matching em is also known as deformable template flexible matching or nonlinear template matching
elastic matching can be defined as an optimization problem of twodimensional warping specifying corresponding pixels between subjected images
connectionist temporal classification ctc is a type of neural network output and associated scoring function for training recurrent neural networks rnns such as lstm networks to tackle sequence problems where the timing is variable
it can be used for tasks like online handwriting recognition or recognizing phonemes in speech audio
ctc refers to the outputs and scoring and is independent of the underlying neural network structure
it was introduced in
the input is a sequence of observations and the outputs are a sequence of labels which can include blank outputs
the difficulty of training comes from there being many more observations than there are labels
for example in speech audio there can be multiple time slices which correspond to a single phoneme
since we dont know the alignment of the observed sequence with the target labels we predict a probability distribution at each time step
a ctc network has a continuous output e
softmax which is fitted through training to model the probability of a label
ctc does not attempt to learn boundaries and timings label sequences are considered equivalent if they differ only in alignment ignoring blanks
equivalent label sequences can occur in many ways  which makes scoring a nontrivial task
fortunately there is an efficient forwardbackward algorithm for that
ctc scores can then be used with the backpropagation algorithm to update the neural network weights
alternative approaches to a ctcfitted neural network include a hidden markov model hmm
roboearth is a network and database repository where robots can share information and learn from each other and a cloud for outsourcing heavy computation tasks
the project that has been described as a world wide web for robots
the project brings together researchers from five major universities in germany the netherlands and spain and is backed by the european union
it allows robots toin addition to the cloudbased infrastructure roboearth offers roscompatible robotunspecific components for high level control of the robot
see softwarecomponents for more details
roboearth offers a cloud robotics infrastructure which includes everything needed to close the loop from robot to the cloud and back to the robot
roboearths worldwideweb style database stores knowledge generated by humans  and robots  in a machinereadable format
data stored in the roboearth knowledge base include software components maps for navigation e
object locations world models task knowledge e
action recipes manipulation strategies and object recognition models e
images object models
the roboearth cloud engine also called rapyuta makes powerful computation available to robots
it allows robots to offload their heavy computation to secure computing environments in the cloud with minimal configuration
the cloud engines computing environments provide high bandwidth access to the roboearth knowledge repository enabling robots to benefit from the experience of other robotsin late  the roboearth project was awarded a year funding grant from the european commissions cognitive systems and robotics initiative in order to develop their networked database platform rapyuta and to develop proofofconcept systems to demonstrate its use
in january  it was officially announced that wikipedia for robots had been launched
roboearth has a spin off called robohow
in statistics kernel density estimation kde is a nonparametric way to estimate the probability density function of a random variable
kernel density estimation is a fundamental data smoothing problem where inferences about the population are made based on a finite data sample
in some fields such as signal processing and econometrics it is also termed the parzenrosenblatt window method after emanuel parzen and murray rosenblatt who are usually credited with independently creating it in its current form
let x x  x be a univariate independent and identically distributed sample drawn from some distribution with an unknown density
we are interested in estimating the shape of this function
its kernel density estimator iswhere k is the kernel  a nonnegative function that integrates to one  and is a smoothing parameter called the bandwidth
a kernel with subscript h is called the scaled kernel and defined as
intuitively one wants to choose h as small as the data will allow however there is always a tradeoff between the bias of the estimator and its variance
the choice of bandwidth is discussed in more detail below
a range of kernel functions are commonly used uniform triangular biweight triweight epanechnikov normal and others
the epanechnikov kernel is optimal in a mean square error sense though the loss of efficiency is small for the kernels listed previously and due to its convenient mathematical properties the normal kernel is often used which means  where  is the standard normal density function
the construction of a kernel density estimate finds interpretations in fields outside of density estimation
for example in thermodynamics this is equivalent to the amount of heat generated when heat kernels the fundamental solution to the heat equation are placed at each data point locations x
similar methods are used to construct discrete laplace operators on point clouds for manifold learning
kernel density estimates are closely related to histograms but can be endowed with properties such as smoothness or continuity by using a suitable kernel
to see this we compare the construction of histogram and kernel density estimators using these  data pointsfor the histogram first the horizontal axis is divided into subintervals or bins which cover the range of the data
in this case we have  bins each of width
whenever a data point falls inside this interval we place a box of height
if more than one data point falls inside the same bin we stack the boxes on top of each other
for the kernel density estimate we place a normal kernel with variance
indicated by the red dashed lines on each of the data points x
the kernels are summed to make the kernel density estimate solid blue curve
the smoothness of the kernel density estimate is evident compared to the discreteness of the histogram as kernel density estimates converge faster to the true underlying density for continuous random variables
the bandwidth of the kernel is a free parameter which exhibits a strong influence on the resulting estimate
to illustrate its effect we take a simulated random sample from the standard normal distribution plotted at the blue spikes in the rug plot on the horizontal axis
the grey curve is the true density a normal density with mean  and variance
in comparison the red curve is undersmoothed since it contains too many spurious data artifacts arising from using a bandwidth h
which is too small
the green curve is oversmoothed since using the bandwidth h   obscures much of the underlying structure
the black curve with a bandwidth of h
is considered to be optimally smoothed since its density estimate is close to the true density
the most common optimality criterion used to select this parameter is the expected l risk function also termed the mean integrated squared errorunder weak assumptions on  and k  is the generally unknown real density functionmise h  amiseh  onh  h where o is the little o notation
the amise is the asymptotic mise which consists of the two leading termswhere formula for a function g formulaand  is the second derivative of
the minimum of this amise is the solution to this differential equationorneither the amise nor the h formulas are able to be used directly since they involve the unknown density function  or its second derivative  so a variety of automatic databased methods have been developed for selecting the bandwidth
many review studies have been carried out to compare their efficacies with the general consensus that the plugin selectorssubstituting any bandwidth h which has the same asymptotic order n as h into the amisegives that amiseh  on where o is the big o notation
it can be shown that under weak assumptions there cannot exist a nonparametric estimator that converges at a faster rate than the kernel estimator
note that the n rate is slower than the typical n convergence rate of parametric methods
if the bandwidth is not held fixed but is varied depending upon the location of either the estimate balloon estimator or the samples pointwise estimator this produces a particularly powerful method termed adaptive or variable bandwidth kernel density estimation
bandwidth selection for kernel density estimation of heavytailed distributions is said to be relatively difficult
if gaussian basis functions are used to approximate univariate data and the underlying density being estimated is gaussian the optimal choice for h that is the bandwidth that minimises the mean integrated squared error iswhere formula is the standard deviation of the samples
this approximation is termed the normal distribution approximation gaussian approximation or silvermans  rule of thumb
while this rule of thumb is easy to compute it should be used with caution as it can yield widely inaccurate estimates when the density is not close to being normal
for example consider estimating the bimodal gaussian mixturefrom a sample of  points
the figure on the right below shows the true density and two kernel density estimates  one using the ruleofthumb bandwidth and the other using a solvetheequation bandwidth
the estimate based on the ruleofthumb bandwidth is significantly oversmoothed
the matlab script for this example uses kde
m and is given below
given the sample x x  x it is natural to estimate the characteristic function asknowing the characteristic function it is possible to find the corresponding probability density function through the fourier transform formula
one difficulty with applying this inversion formula is that it leads to a diverging integral since the estimate formula is unreliable for large ts
to circumvent this problem the estimator formula is multiplied by a damping function  which is equal to  at the origin and then falls to  at infinity
the bandwidth parameter h controls how fast we try to dampen the function formula
in particular when h is small then t will be approximately one for a large range of ts which means that formula remains practically unaltered in the most important region of ts
the most common choice for function  is either the uniform function  which effectively means truncating the interval of integration in the inversion formula to  or the gaussian function
once the function  has been chosen the inversion formula may be applied and the density estimator will bewhere k is the fourier transform of the damping function
thus the kernel density estimator coincides with the characteristic function density estimator
a nonexhaustive list of software implementations of kernel density estimators includes
deep reinforcement learning drl is a machine learning method that extends reinforcement learning approach to learning of the entire process from sensors to motors endtoend using deep learning techniques
therefore it is often called endtoend reinforcement learning from the point of the aim
the traditional reinforcement learning aims to solve problems of how agents can learn to take the best actions on the environment to get the maximum cumulative reward over time
a major part of this process is carefully engineering feature representations
the recent advances of deep learning for learning feature representations have paved the way for endtoend reinforcement learning
some of the main algorithms include
formulaformula with formulaand formula going to zero for formulax and y  r being respectively an input and an output space we consider a training set formulaof size m in formula drawn i
from an unknown distribution d
a learning algorithm is a function formula from formula into formulawhich maps a learning set s onto a function formula from x to y
to avoid complex notation we consider only deterministic algorithms
it is also assumed that the algorithm formula is symmetric with respect to s i
it does not depend on the order of the elements in the training set
furthermore we assume that all functions are measurable and all sets are countable which does not limit the interest of the results presented here
the loss of an hypothesis f with respect to an example formula is then defined as formula
the empirical error of f is formula
the true error of f is formulagiven a training set s of size m we will build for all i
m modified training sets as followsformulaformula
bigdl is a distributed deep learning framework for apache spark created by jason dai at intel
it is hosted at github
unsupervised machine learning is the machine learning task of inferring a function to describe hidden structure from unlabeled data a classification or categorization is not included in the observations
since the examples given to the learner are unlabeled there is no evaluation of the accuracy of the structure that is output by the relevant algorithmwhich is one way of distinguishing unsupervised learning from supervised learning and reinforcement learning
a central case of unsupervised learning is the problem of density estimation in statistics though unsupervised learning encompasses many other problems and solutions involving summarizing and explaining key features of the data
approaches to unsupervised learning includethe classical example of unsupervised learning in the study of both natural and artificial neural networks is subsumed by donald hebbs principle that is neurons that fire together wire together
in hebbian learning the connection is reinforced irrespective of an error but is exclusively a function of the coincidence between action potentials between the two neurons
a similar version that modifies synaptic weights takes into account the time between the action potentials spiketimingdependent plasticity or stdp
hebbian learning has been hypothesized to underlie a range of cognitive functions such as pattern recognition and experiential learning
among neural network models the selforganizing map som and adaptive resonance theory art are commonly used in unsupervised learning algorithms
the som is a topographic organization in which nearby locations in the map represent inputs with similar properties
the art model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a userdefined constant called the vigilance parameter
art networks are also used for many pattern recognition tasks such as automatic target recognition and seismic signal processing
the first version of art was art developed by carpenter and grossberg
one of the statistical approaches for unsupervised learning is the method of moments
in the method of moments the unknown parameters of interest in the model are related to the moments of one or more random variables and thus these unknown parameters can be estimated given the moments
the moments are usually estimated from samples empirically
the basic moments are first and second order moments
for a random vector the first order moment is the mean vector and the second order moment is the covariance matrix when the mean is zero
higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multidimensional arrays
in particular the method of moments is shown to be effective in learning the parameters of latent variable models
latent variable models are statistical models where in addition to the observed variables a set of latent variables also exists which is not observed
a highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words observed variables in the document based on the topic latent variable of the document
in the topic modeling the words in the document are generated according to different statistical parameters when the topic of the document is changed
it is shown that method of moments tensor decomposition techniques consistently recover the parameters of a large class of latent variable models under some assumptions
the expectationmaximization algorithm em is also one of the most practical methods for learning latent variable models
however it can get stuck in local optima and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model
in contrast for the method of moments the global convergence is guaranteed under some conditions
behavioralbased detection in network security has become a good application area for a combination of supervised and unsupervised machine learning
this is because the amount of data for a human security analyst to analyze is impossible measured in terabytes per day to review to find patterns and anomalies
according to giora engel cofounder of lightcyber in a dark reading article the great promise machine learning holds for the security industry is its ability to detect advanced and unknown attacksparticularly those leading to data breaches
the basic premise is that a motivated attacker will find their way into a network generally by compromising a users computer or network account through phishing social engineering or malware
the security challenge then becomes finding the attacker by their operational activities which include reconnaissance lateral movement command  control and exfiltration
these activitiesespecially reconnaissance and lateral movementstand in contrast to an established baseline of normal or good activity for each user and device on the network
the role of machine learning is to create ongoing profiles for users and devices and then find meaningful anomalies
formal concept analysis fca is a principled way of deriving a concept hierarchy or formal ontology from a collection of objects and their properties
each concept in the hierarchy represents the objects sharing some set of properties and each subconcept in the hierarchy represents a subset of the objects as well as a superset of the properties in the concepts above it
the term was introduced by rudolf wille in  and builds on the mathematical theory of lattices and ordered sets that was developed by garrett birkhoff and others in the s
formal concept analysis finds practical application in fields including data mining text mining machine learning knowledge management semantic web software development chemistry and biology
the original motivation of formal concept analysis was the search for realworld meaning of mathematical order theory
one such possibility of very general nature is that data tables can be transformed into algebraic structures called complete lattices and that these can be utilized for data visualization and interpretation
data tables that represent binary relations between objects and attributes thus tabulating pairs of the form object g has attribute m are considered as the basic data type and are referred to as formal contexts
in this theory a formal concept is defined to be a pair a b where a is a set of objects called the extent and b is a set of attributes the intent such thatin this way formal concept analysis formalizes the semantic notions of extension and intension
the formal concepts of any formal context canas explained belowbe ordered in a hierarchy called more formally the contexts concept lattice
the concept lattice can be graphically visualized as a line diagram which then may be helpful for understanding the data
often however these lattices get too large for visualization
then the mathematical theory of formal concept analysis may be helpful e
for decomposing the lattice into smaller pieces without information loss or for embedding it into another structure which is easier to interpret
the theory in its present form goes back to the early s and a research group led by rudolf wille bernhard ganter and peter burmeister at the technische universitt darmstadt
its basic mathematical definitions however were already introduced in the s by garrett birkhoff as part of general lattice theory
other previous approaches to the same idea arose from various french research groups but the darmstadt group normalised the field and systematically worked out both its mathematical theory and its philosophical foundations
the latter refer in particular to charles s
peirce but also to the logic of portroyal
in his article restructuring lattice theory  initiating formal concept analysis as a mathematical discipline wille starts from a discontent with the current lattice theory and pure mathematics in general the production of theoretical results  often achieved by elaborate mental gymnastics  were impressive but the connections between neighboring domains even parts of a theory were getting weaker
this aim traces back to the educationalist hartmut von hentig who in  pleaded for restructuring sciences in view of better teaching and in order to make sciences mutually available and more generally i
also without specialized knowledge critiqueable
hence by its origins formal concept analysis aims at interdisciplinarity and democratic control of research
it corrects the starting point of lattice theory during the development of formal logic in the th century
then  and later in model theory  a concept as unary predicate had been reduced to its extent
now again the philosophy of concepts should become less abstract by considering the intent
hence formal concept analysis is oriented towards the categories extension and intension of linguistics and classical conceptual logic
formal concept analysis aims at the clarity of concepts according to charles s
peirces pragmatic maxim by unfolding observable elementary properties of the subsumed objects
in his late philosophy peirce assumed that logical thinking aims at perceiving reality by the triade concept judgement and conclusion
mathematics is an abstraction of logic develops patterns of possible realities and therefore may support rational communication
on this background wille definesthe data in the example is taken from a semantic field study where different kinds of bodies of water were systematically categorized by their attributes
for the purpose here it has been simplified
the data table represents a formal context the line diagram next to it shows its concept lattice
formal definitions follow below
the above line diagram consists of circles connecting line segments and labels
circles represent formal concepts
the lines allow to read off the subconceptsuperconcept hierarchy
each object and attribute name is used as a labelexactly once in the diagram with objects below and attributes above concept circles
this is done in a way that an attribute can be reached from an object via an ascending path if and only if the object has the attribute
in the diagram shown e
the object reservoir has the attributes stagnant and constant but not the attributes temporary running natural maritime
accordingly puddle has exactly the characteristics temporary stagnant and natural
the original formal context can be reconstructed from the labelled diagram as well as the formal concepts
the extent of a concept consists of those objects from which an ascending path leads to the circle representing the concept
the intent consists of those attributes to which there is an ascending path from that concept circle in the diagram
in this diagram the concept immediately to the left of the label reservoir has the intent stagnant and natural and the extent puddle maar lake pond tarn pool lagoon and sea
a formal context is a triple k  g m i where g is a set of objects m is a set of attributes and i  g  m is a binary relation called incidence that expresses which objects have which attributes
for subsets a  g of objects and subsets b  m of attributes one defines two derivation operators as followsa  m  m  gim for all g  a and duallyb  g  g  gim for all m  b
applying either derivation operator and then the other constitutes two closure operatorsa   a  a  for a  g  extent closure andb   b  b  for b  m  intent closure
the derivation operators define a galois connection between sets of objects and of attributes
this is why infrench a concept lattice is sometimes called a treillis de galois galois lattice
with these derivation operators wille gave an elegant definition of a formal concepta pair ab is a formal concept of a context g m i provided thata  g  b  m  a  b  and b  a
equivalently and more intuitively ab is a formal concept precisely whenfor computing purposes a formal context may be naturally represented as a matrix k in which the rows correspond to the objects the columns correspond to the attributes and each entry k equals to  if object i has attribute j
in this matrix representation each formal concept corresponds to a maximal submatrix not necessarily contiguous all of whose elements equal
it is however misleading to consider a formal context as boolean because the negated incidence object g does not have attribute m is not concept forming in the same way as defined above
for this reason the values  and  or true and false are usually avoided when representing formal contexts and a symbol like formula is used to express incidence
the concepts a b of a context k can be partially ordered by the inclusion of extents or equivalently by the dual inclusion of intents
an order  on the concepts is defined as follows for any two concepts a b and a b of k we say that a b  a b precisely when a  a
equivalently a b  a b whenever b  b
in this order every set of formal concepts has a greatest common subconcept or meet
its extent consists of those objects that are common to all extents of the set
dually every set of formal concepts has a least common superconcept the intent of which comprises all attributes which all objects of that set of concepts have
these meet and join operations satisfy the axioms defining a lattice in fact a complete lattice
conversely it can be shown that every complete lattice is the concept lattice of some formal context up to isomorphism
realworld data is often given in the form of an objectattribute table where the attributes have values
formal concept analysis handles such data by transforming them into the basic type of a onevalued formal context
the method is called conceptual scaling
the negation of an attribute m is an attribute m the extent of which is just the complement of the extent of m i
with m  gm
it is in general not assumed that negated attributes are available for concept formation
but pairs of attributes which are negations of each other often naturally occur for example in contexts derived from conceptual scaling
for possible negations of formal concepts see the section concept algebras below
an implication a  b relates two sets a and b of attributes and expresses that every object possessing each attribute from a also has each attribute from b
when gmi is a formal context and a b are subsets of the set m of attributes i
ab  m then the implication a  b is valid if a  b
for each finite formal context the set of all valid implications has a canonical basis an irredundant set of implications from which all valid implications can be derived by the natural inference armstrong rules
this is used in attribute exploration a knowledge acquisition method based on implications
formal concept analysis has elaborate mathematical foundations making the field versatile
as a basic example we mention the arrow relations which are simple and easy to compute but very useful
they are defined as follows for g  g and m  m letg  m  gim and if mn  and m  n  then ginand duallyg  m  gim and if gh  and g  h  then him
since only nonincident objectattribute pairs can be related these relations can conveniently be recorded in the table representing a formal context
many lattice properties can be read off from the arrow relations including distributivity and several of its generalizations
they also reveal structural information and can be used for determining e
the congruence relations of the lattice
there is a number of simple and fast algorithms for generating formal concepts and for constructing and navigating concept lattices
for a survey see kuznetsov and obiedkov or the book by ganter and obiedkov where also some pseudocode can be found
since the number of formal concepts may be exponential in the size of the formal context the complexity of the algorithms usually is given with respect to the output size
concept lattices with a few million elements can be handled without problems
many fca software applications are available today
the main purpose of these tools varies from formal context creation to formal concept mining and generating the concepts lattice of a given formal context and the corresponding implications and association rules
most of these tools are academic opensource applications such asthe formal concept analysis can be used as a qualitative method for data analysis
since the early beginnings of fba in the early s the fba research group at tu darmstadt has gained experience from more than  projects using the fba as of
including the fields of medicine and cell biology genetics ecology software engineering ontology information and library sciences office administration law linguistics political science
many more examples are e
described in formal concept analysis
foundations and applications conference papers at regular conferences such as international conference on formal concept analysis icfca concept lattices and their applications cla or international conference on conceptual structures iccs
darkforest is a computer go program developed by facebook based on deep learning techniques using a convolutional neural network
its updated version darkfores combines the techniques of its predecessor with monte carlo tree search
the mcts effectively takes tree search methods commonly seen in computer chess programs and randomizes them
with the update the system is known as darkfmcts
darkforest is of similar strength to programs like crazystone and zen
it has been tested against a professional human player at the  uec cup
googles alphago program won against a professional player in october  using a similar combination of techniques
darkforest is named after liu cixins science fiction novel the dark forest
competing with top human players in the ancient game of go has been a longterm goal of artificial intelligence
gos high branching factor makes traditional search techniques ineffective even on cuttingedge hardware and gos evaluation function could change drastically with one stone change
however by using a deep convolutional neural network designed for longterm predictions darkforest has been able to substantially improve the win rate for bots over more traditional monte carlo tree search based approaches
against human players darkfores achieves a stable d ranking on kgs go server which roughly corresponds to an advanced amateur human player
however after adding monte carlo tree search to darkfores to create a much stronger player named darkfmcts it can achieve a d ranking on the kgs go server
darkfmcts is on par with stateoftheart go ais such as zen dolbaram and crazy stone but lags behind alphago
it won rd place in january  kgs bot tournament against other go ais
after googles alphago won against fan hui in  facebook made its ais hardware designs public alongside releasing the code behind darkforest as opensource along with heavy recruiting to strengthen its team of ai engineers
darkforest uses a neural network to sort through the  board positions and find the most powerful next move
however neural networks alone cannot match the level of good amateur players or the best searchbased go engines and so darkfores combines the neural network approach with a searchbased machine
a database of  real go games were used in the development of darkforest with  used as a training set and the rest used to test the neural networks ability to predict the next moves played in the real games
this allows darkforest to accurately evaluate the global state of the board but local tactics were still poor
searchbased engines have poor global evaluation but are good at local tactics
combining these two approaches is difficult because searchbased engines work much faster than neural networks a problem which was solved in darkfores by running the processes in parallel with frequent communication between the two
go is generally played by analyzing the position of the stones on the board
some advanced players have described it as playing in some part subconsciously
unlike chess and checkers where ai players can simply look farther forward at moves than human players but with each round of go having on average  possible moves that approach is ineffective
instead neural networks copy human play by training the ai systems on images of successful moves the ai can effectively learn how to interpret how the board looks as many grandmasters do
in november  facebook demonstrated the combination of mcts with neural networks which played with a style that felt human
it has been noted that darkforest still has flaws in its play style
sometimes the bot plays tenuki move elsewhere pointlessly when local powerful moves are required
when the bot is losing it shows the typical behavior of mcts it plays bad moves and loses more
the facebook ai team has acknowledged these as areas of future improvement
the family of darkforest computer go programs is based on convolution neural networks
the most recent advances in darkfmcts combined convolutional neural networks with more traditional monte carlo tree search
darkfmcts is the most advanced version of darkforest which combines facebooks most advanced convolutional neural network architecture from darkfores with a monte carlo tree search
darkfmcts relies on a convolution neural networks that predicts the next k moves based on the current state of play
it treats the board as a x image with multiple channels
each channel represents a different aspect of board information based upon the specific style of play
for standard and extended play there are  and  different channels respectively
in standard play each players liberties are represented as six binary channels or planes
the respective plane is true if the player one two or three or more liberties available
ko i
illegal moves is represented as one binary plane
stone placement for each opponent and empty board positions are represented as three binary planes and the duration since a stone has been placed is represented as real numbers on two planes one for each player
lastly the opponents rank is represented by nine binary planes where if all are true the player is a d level if  are true a d level and so forth
extended play additionally considers the boarder binary plane that is true at the border position mask represented as distance from the board center i
formula where formula is a real number at a position and each players territory binary based on which player a location is closer to
darkfmct uses a layer full convolutional network with a width of  nodes without weight sharing or pooling
each convolutional layer is followed by a rectified linear unit a popular activation function for deep neural networks
a key innovation of darkfmct compared to previous approaches is that it uses only one softmax function to predict the next move which enables the approach to reduce the overall number of parameters
darkfmct was trained against  random selected games from an empirical dataset representing different game stages
the learning rate was determined by vanilla stochastic gradient descent
darkfmct synchronously couples a convolutional neural network with a monte carlo tree search
because the convolutional neural network is computationally taxing the monte carlo tree search focuses computation on the more likely game play trajectories
by running the neural network synchronously with the monte carlo tree search it is possible to guarantee that each node is expanded by the moves predicted by the neural network
darkfores beats darkforest its neural networkonly predecessor around  of the time and pachi one of the best searchbased engines around  of the time
on the kyu rating system darkforest holds a d level
darkfores achieves a stable d level on kgs go server as a ranked bot
with the added monte carlo tree search darkfmcts with  rollouts beats pachi with k rollouts in all  games with k rollouts it achieves a stable d level in kgs server on par with stateoftheart go ais e
zen dolbaram crazystone with k rollouts it won the rd place in january kgs go tournament
in euclidean geometry linear separability is a property of a pair of sets of points
this is most easily visualized in two dimensions the euclidean plane by thinking of one set of points as being colored blue and the other set of points as being colored red
these two sets are linearly separable if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side
this idea immediately generalizes to higherdimensional euclidean spaces if line is replaced by hyperplane
the problem of determining if a pair of sets is linearly separable and finding a separating hyperplane if they are arises in several areas
in statistics and machine learning classifying certain types of data is a problem for which good algorithms exist that are based on this concept
let formula and formula be two sets of points in an ndimensional euclidean space
then formula and formula are linearly separable if there exist n   real numbers formula such that every point formula satisfies formula and every point formula satisfies formula where formula is the formulath component of formula
equivalently two sets are linearly separable precisely when their respective convex hulls are disjoint colloquially do not overlap
three noncollinear points in two classes  and  are always linearly separable in two dimensions
this is illustrated by the three examples in the following figure the all  case is not shown but is similar to the all  casehowever not all sets of four points no three collinear are linearly separable in two dimensions
the following example would need two straight lines and thus is not linearly separable notice that three points which are collinear and of the form      are also not linearly separable
a boolean function in n variables can be thought of as an assignment of  or  to each vertex of a boolean hypercube in n dimensions
this gives a natural division of the vertices into two sets
the boolean function is said to be linearly separable provided these two sets of points are linearly separable
classifying data is a common task in machine learning
suppose some data points each belonging to one of two sets are given and we wish to create a model that will decide which set a new data point will be in
in the case of support vector machines a data point is viewed as a pdimensional vector a list of p numbers and we want to know whether we can separate such points with a pdimensional hyperplane
this is called a linear classifier
there are many hyperplanes that might classify separate the data
one reasonable choice as the best hyperplane is the one that represents the largest separation or margin between the two sets
so we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized
if such a hyperplane exists it is known as the maximummargin hyperplane and the linear classifier it defines is known as a maximum margin classifier
more formally given some training data formula a set of n points of the formwhere the y is either  or  indicating the set to which the point formula belongs
each formula is a pdimensional real vector
we want to find the maximummargin hyperplane that divides the points having formula from those having formula
any hyperplane can be written as the set of points formula satisfyingwhere formula denotes the dot product and formula the not necessarily normalized normal vector to the hyperplane
the parameter formula determines the offset of the hyperplane from the origin along the normal vector formula
if the training data are linearly separable we can select two hyperplanes in such a way that they separate the data and there are no points between them and then try to maximize their distance
rule induction is an area of machine learning in which formal rules are extracted from a set of observations
the rules extracted may represent a full scientific model of the data or merely represent local patterns in the data
some major rule induction paradigms aresome rule induction algorithms are
decision lists are a representation for boolean functions which can be easily learnable from examples
single term decision lists are more expressive than disjunctions and conjunctions however term decision lists are less expressive than the general disjunctive normal form and the conjunctive normal form
the language specified by a klength decision list includes as a subset the language specified by a kdepth decision tree
learning decision lists can be used for attribute efficient learning
a decision list dl of length is of the formwhere is the th formula and is the th boolean for formula
the last ifthenelse is the default case which means formula is always equal to true
a dl is a decision list where all of formulas have at most terms
sometimes decision list is used to refer to a dl where all of the formulas are either a variable or its negation
machine learning is a field of computer science that often uses statistical techniques to give computers the ability to learn i
progressively improve performance on a specific task with data without being explicitly programmed
the name machine learning was coined in  by arthur samuel
evolved from the study of pattern recognition and computational learning theory in artificial intelligence machine learning explores the study and construction of algorithms that can learn from and make predictions on data  such algorithms overcome following strictly static program instructions by making datadriven predictions or decisions through building a model from sample inputs
machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible example applications include email filtering detection of network intruders or malicious insiders working towards a data breach optical character recognition ocr learning to rank and computer vision
machine learning is closely related to and often overlaps with computational statistics which also focuses on predictionmaking through the use of computers
it has strong ties to mathematical optimization which delivers methods theory and application domains to the field
machine learning is sometimes conflated with data mining where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning
machine learning can also be unsupervised and be used to learn and establish baseline behavioral profiles for various entities and then used to find meaningful anomalies
within the field of data analytics machine learning is a method used to devise complex models and algorithms that lend themselves to prediction in commercial use this is known as predictive analytics
these analytical models allow researchers data scientists engineers and analysts to produce reliable repeatable decisions and results and uncover hidden insights through learning from historical relationships and trends in the data
tom m
mitchell provided a widely quoted more formal definition of the algorithms studied in the machine learning field a computer program is said to learn from experience e with respect to some class of tasks t and performance measure p if its performance at tasks in t as measured by p improves with experience e
this definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms
this follows alan turings proposal in his paper computing machinery and intelligence in which the question can machines think is replaced with the question can machines do what we as thinking entities can do
in turings proposal the various characteristics that could be possessed by a thinking machine and the various implications in constructing one are exposed
machine learning tasks are typically classified into two broad categories depending on whether there is a learning signal or feedback available to a learning systemanother categorization of machine learning tasks arises when one considers the desired output of a machinelearned systemamong other categories of machine learning problems learning to learn learns its own inductive bias based on previous experience
developmental learning elaborated for robot learning generates its own sequences also called curriculum of learning situations to cumulatively acquire repertoires of novel skills through autonomous selfexploration and social interaction with human teachers and using guidance mechanisms such as active learning maturation motor synergies and imitation
arthur samuel an american pioneer in the field of computer gaming and artificial intelligence coined the term machine learning in  while at ibm
as a scientific endeavour machine learning grew out of the quest for artificial intelligence
already in the early days of ai as an academic discipline some researchers were interested in having machines learn from data
they attempted to approach the problem with various symbolic methods as well as what were then termed neural networks these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics
probabilistic reasoning was also employed especially in automated medical diagnosis
however an increasing emphasis on the logical knowledgebased approach caused a rift between ai and machine learning
probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation
by  expert systems had come to dominate ai and statistics was out of favor
work on symbolicknowledgebased learning did continue within ai leading to inductive logic programming but the more statistical line of research was now outside the field of ai proper in pattern recognition and information retrieval
neural networks research had been abandoned by ai and computer science around the same time
this line too was continued outside the aics field as connectionism by researchers from other disciplines including hopfield rumelhart and hinton
their main success came in the mids with the reinvention of backpropagation
machine learning reorganized as a separate field started to flourish in the s
the field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature
it shifted focus away from the symbolic approaches it had inherited from ai and toward methods and models borrowed from statistics and probability theory
it also benefited from the increasing availability of digitized information and the ability to distribute it via the internet
machine learning and data mining often employ the same methods and overlap significantly but while machine learning focuses on prediction based on known properties learned from the training data data mining focuses on the discovery of previously unknown properties in the data this is the analysis step of knowledge discovery in databases
data mining uses many machine learning methods but with different goals on the other hand machine learning also employs data mining methods as unsupervised learning or as a preprocessing step to improve learner accuracy
much of the confusion between these two research communities which do often have separate conferences and separate journals ecml pkdd being a major exception comes from the basic assumptions they work with in machine learning performance is usually evaluated with respect to the ability to reproduce known knowledge while in knowledge discovery and data mining kdd the key task is the discovery of previously unknown knowledge
evaluated with respect to known knowledge an uninformed unsupervised method will easily be outperformed by other supervised methods while in a typical kdd task supervised methods cannot be used due to the unavailability of training data
machine learning also has intimate ties to optimization many learning problems are formulated as minimization of some loss function on a training set of examples
loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances for example in classification one wants to assign a label to instances and models are trained to correctly predict the preassigned labels of a set of examples
the difference between the two fields arises from the goal of generalization while optimization algorithms can minimize the loss on a training set machine learning is concerned with minimizing the loss on unseen samples
machine learning and statistics are closely related fields
according to michael i
jordan the ideas of machine learning from methodological principles to theoretical tools have had a long prehistory in statistics
he also suggested the term data science as a placeholder to call the overall field
leo breiman distinguished two statistical modelling paradigms data model and algorithmic model wherein algorithmic model means more or less the machine learning algorithms like random forest
some statisticians have adopted methods from machine learning leading to a combined field that they call statistical learning
a core objective of a learner is to generalize from its experience
generalization in this context is the ability of a learning machine to perform accurately on new unseen examplestasks after having experienced a learning data set
the training examples come from some generally unknown probability distribution considered representative of the space of occurrences and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases
the computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory
because training sets are finite and the future is uncertain learning theory usually does not yield guarantees of the performance of algorithms
instead probabilistic bounds on the performance are quite common
the biasvariance decomposition is one way to quantify generalization error
for the best performance in the context of generalization the complexity of the hypothesis should match the complexity of the function underlying the data
if the hypothesis is less complex than the function then the model has underfit the data
if the complexity of the model is increased in response then the training error decreases
but if the hypothesis is too complex then the model is subject to overfitting and generalization will be poorer
in addition to performance bounds computational learning theorists study the time complexity and feasibility of learning
in computational learning theory a computation is considered feasible if it can be done in polynomial time
there are two kinds of time complexity results
positive results show that a certain class of functions can be learned in polynomial time
negative results show that certain classes cannot be learned in polynomial time
decision tree learning uses a decision tree as a predictive model which maps observations about an item to conclusions about the items target value
association rule learning is a method for discovering interesting relations between variables in large databases
an artificial neural network ann learning algorithm usually called neural network nn is a learning algorithm that is vaguely inspired by biological neural networks
computations are structured in terms of an interconnected group of artificial neurons processing information using a connectionist approach to computation
modern neural networks are nonlinear statistical data modeling tools
they are usually used to model complex relationships between inputs and outputs to find patterns in data or to capture the statistical structure in an unknown joint probability distribution between observed variables
falling hardware prices and the development of gpus for personal use in the last few years have contributed to the development of the concept of deep learning which consists of multiple hidden layers in an artificial neural network
this approach tries to model the way the human brain processes light and sound into vision and hearing
some successful applications of deep learning are computer vision and speech recognition
inductive logic programming ilp is an approach to rule learning using logic programming as a uniform representation for input examples background knowledge and hypotheses
given an encoding of the known background knowledge and a set of examples represented as a logical database of facts an ilp system will derive a hypothesized logic program that entails all positive and no negative examples
inductive programming is a related field that considers any kind of programming languages for representing hypotheses and not only logic programming such as functional programs
support vector machines svms are a set of related supervised learning methods used for classification and regression
given a set of training examples each marked as belonging to one of two categories an svm training algorithm builds a model that predicts whether a new example falls into one category or the other
cluster analysis is the assignment of a set of observations into subsets called clusters so that observations within the same cluster are similar according to some predesignated criterion or criteria while observations drawn from different clusters are dissimilar
different clustering techniques make different assumptions on the structure of the data often defined by some similarity metric and evaluated for example by internal compactness similarity between members of the same cluster and separation between different clusters
other methods are based on estimated density and graph connectivity
clustering is a method of unsupervised learning and a common technique for statistical data analysis
a bayesian network belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph dag
for example a bayesian network could represent the probabilistic relationships between diseases and symptoms
given symptoms the network can be used to compute the probabilities of the presence of various diseases
efficient algorithms exist that perform inference and learning
reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of longterm reward
reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states
reinforcement learning differs from the supervised learning problem in that correct inputoutput pairs are never presented nor suboptimal actions explicitly corrected
several learning algorithms mostly unsupervised learning algorithms aim at discovering better representations of the inputs provided during training
classical examples include principal components analysis and cluster analysis
representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful often as a preprocessing step before performing classification or predictions allowing reconstruction of the inputs coming from the unknown data generating distribution while not being necessarily faithful for configurations that are implausible under that distribution
manifold learning algorithms attempt to do so under the constraint that the learned representation is lowdimensional
sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse has many zeros
multilinear subspace learning algorithms aim to learn lowdimensional representations directly from tensor representations for multidimensional data without reshaping them into highdimensional vectors
deep learning algorithms discover multiple levels of representation or a hierarchy of features with higherlevel more abstract features defined in terms of or generating lowerlevel features
it has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data
in this problem the learning machine is given pairs of examples that are considered similar and pairs of less similar objects
it then needs to learn a similarity function or a distance metric function that can predict if new objects are similar
it is sometimes used in recommendation systems
in this method a datum is represented as a linear combination of basis functions and the coefficients are assumed to be sparse
let x be a ddimensional datum d be a d by n matrix where each column of d represents a basis function
r is the coefficient to represent x using d
mathematically sparse dictionary learning means solving formula where r is sparse
generally speaking n is assumed to be larger than d to allow the freedom for a sparse representation
learning a dictionary along with sparse representations is strongly nphard and also difficult to solve approximately
a popular heuristic method for sparse dictionary learning is ksvd
sparse dictionary learning has been applied in several contexts
in classification the problem is to determine which classes a previously unseen datum belongs to
suppose a dictionary for each class has already been built
then a new datum is associated with the class such that its best sparsely represented by the corresponding dictionary
sparse dictionary learning has also been applied in image denoising
the key idea is that a clean image patch can be sparsely represented by an image dictionary but the noise cannot
a genetic algorithm ga is a search heuristic that mimics the process of natural selection and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem
in machine learning genetic algorithms found some uses in the s and s
conversely machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms
rulebased machine learning is a general term for any machine learning method that identifies learns or evolves rules to store manipulate or apply knowledge
the defining characteristic of a rulebased machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system
this is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction
rulebased machine learning approaches include learning classifier systems association rule learning and artificial immune systems
learning classifier systems lcs are a family of rulebased machine learning algorithms that combine a discovery component e
typically a genetic algorithm with a learning component performing either supervised learning reinforcement learning or unsupervised learning
they seek to identify a set of contextdependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions
applications for machine learning includein  the online movie company netflix held the first netflix prize competition to find a program to better predict user preferences and improve the accuracy on its existing cinematch movie recommendation algorithm by at least
a joint team made up of researchers from att labsresearch in collaboration with the teams big chaos and pragmatic theory built an ensemble model to win the grand prize in  for million
shortly after the prize was awarded netflix realized that viewers ratings were not the best indicators of their viewing patterns everything is a recommendation and they changed their recommendation engine accordingly
in  the wall street journal wrote about the firm rebellion research and their use of machine learning to predict the financial crisis
in  cofounder of sun microsystems vinod khosla predicted that  of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software
in  it has been reported that a machine learning algorithm has been applied in art history to study fine art paintings and that it may have revealed previously unrecognized influences between artists
although machine learning has been very transformative in some fields effective machine learning is difficult because finding patterns is hard and often not enough training data are available as a result machinelearning programs often fail to deliver
classification machine learning models can be validated by accuracy estimation techniques like the holdout method which splits the data in a training and test set conventionally  training set and  test set designation and evaluates the performance of the training model on the test set
in comparison the nfoldcrossvalidation method randomly splits the data in k subsets where the k instances of the data are used to train the model while the kth instance is used to test the predictive ability of the training model
in addition to the holdout and crossvalidation methods bootstrap which samples n instances with replacement from the dataset can be used to assess model accuracy
in addition to overall accuracy investigators frequently report sensitivity and specificity meaning true positive rate tpr and true negative rate tnr respectively
similarly investigators sometimes report the false positive rate fpr as well as the false negative rate fnr
however these rates are ratios that fail to reveal their numerators and denominators
the total operating characteristic toc is an effective method to express a models diagnostic ability
toc shows the numerators and denominators of the previously mentioned rates thus toc provides more information than the commonly used receiver operating characteristic roc and rocs associated area under the curve auc
machine learning poses a host of ethical questions
systems which are trained on datasets collected with biases may exhibit these biases upon use algorithmic bias thus digitizing cultural prejudices
for example using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants
responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning
because language contains biases machines trained on language corpora will necessarily also learn bias
other forms of ethical challenges not related to personal biases are more seen in health care
there are concerns among health care professionals that these systems might not be designed in the publics interest but as income generating machines
this is especially true in the united states where there is a perpetual ethical dilemma of improving health care but also increasing profits
for example the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithms proprietary owners hold stakes in
there is huge potential for machine learning in health care to provide professionals a great tool to diagnose medicate and even plan recovery paths for patients but this will not happen until the personal biases mentioned previously and these greed biases are addressed
software suites containing a variety of machine learning algorithms include the following
the term evolvability is used for a recent framework of computational learning introduced by leslie valiant in his paper of the same name and described below
the aim of this theory is to model biological evolution and categorize which types of mechanisms are evolvable
evolution is an extension of pac learning and learning from statistical queries
let formula and formula be collections of functions on formula variables
given an ideal function formula the goal is to find by local search a representation formula that closely approximates formula
this closeness is measured by the performance formula of formula with respect to formula
as is the case in the biological world there is a difference between genotype and phenotype
in general there can be multiple representations genotypes that correspond to the same function phenotype
that is for some formula with formula still formula for all formula
however this need not be the case
the goal then is to find a representation that closely matches the phenotype of the ideal function and the spirit of the local search is to allow only small changes in the genotype
let the neighborhood formula of a representation formula be the set of possible mutations of formula
for simplicity consider boolean functions on formula and let formula be a probability distribution on formula
define the performance in terms of this
specificallynote that formula in general for nonboolean functions the performance will not correspond directly to the probability that the functions agree although it will have some relationship
throughout an organisms life it will only experience a limited number of environments so its performance cannot be determined exactly
the empirical performance is defined byformulawhere formula is a multiset of formula independent selections from formula according to formula
if formula is large enough evidently formula will be close to the actual performance formula
given an ideal function formula initial representation formula sample size formula and tolerance formula the mutator formula is a random variable defined as follows
each formula is classified as beneficial neutral or deleterious depending on its empirical performance
specificallyif there are any beneficial mutations then formula is equal to one of these at random
if there are no beneficial mutations then formula is equal to a random neutral mutation
in light of the similarity to biology formula itself is required to be available as a mutation so there will always be at least one neutral mutation
the intention of this definition is that at each stage of evolution all possible mutations of the current genome are tested in the environment
out of the ones who thrive or at least survive one is chosen to be the candidate for the next stage
given formula we define the sequence formula by formula
thus formula is a random variable representing what formula has evolved to after formula generations
let formula be a class of functions formula be a class of representations and formula a class of distributions on formula
we say that formula is evolvable by formula over formula if there exists polynomials formula formula formula and formula such that for all formula and all formula for all ideal functions formula and representations formula with probability at least formulawhere the sizes of neighborhoods formula for formula are at most formula the sample size is formula the tolerance is formula and the generation size is formula
formula is evolvable over formula if it is evolvable by some formula over formula
formula is evolvable if it is evolvable over all distributions formula
the class of conjunctions and the class of disjunctions are evolvable over the uniform distribution for short conjunctions and disjunctions respectively
the class of parity functions which evaluate to the parity of the number of true literals in a given subset of literals are not evolvable even for the uniform distribution
evolvability implies pac learnability
structured sparsity regularization is a class of methods and an area of research in statistical learning theory that extend and generalize sparsity regularization learning methods
both sparsity and structured sparsity regularization methods seek to exploit the assumption that the output variable formula i
response or dependent variable to be learned can be described by a reduced number of variables in the input space formula i
the domain space of features or explanatory variables
sparsity regularization methods focus on selecting the input variables that best describe the output
structured sparsity regularization methods generalize and extend sparsity regularization methods by allowing for optimal selection over structures like groups or networks of input variables in formula
common motivation for the use of structured sparsity methods are model interpretability highdimensional learning where dimensionality of formula may be higher than the number of observations formula and reduction of computational complexity
moreover structured sparsity methods allow to incorporate prior assumptions on the structure of the input variables such as overlapping groups nonoverlapping groups and acyclic graphs
examples of uses of structured sparsity methods include face recognition magnetic resonance image mri processing sociolinguistic analysis in natural language processing and analysis of genetic expression in breast cancer
consider the linear kernel regularized empirical risk minimization problem with a loss function formula and the formula norm as the regularization penaltywhere formula and formula denotes the formula norm defined as the number of nonzero entries of the vector formula
formula is said to be sparse if formula
which means that the output formula can be described by a small subset of input variables
more generally assume a dictionary formula with formula is given such that the target function formula of a learning problem can be written asthe formula norm formula as the number of nonzero components of formula is defined as formula is said to be sparse if formula
however while using the formula norm for regularization favors sparser solutions it is computationally difficult to use and additionally is not convex
a computationally more feasible norm that favors sparser solutions is the formula norm this has been shown to still favor sparser solutions and is additionally convex
structured sparsity regularization extends and generalizes the variable selection problem that characterizes sparsity regularization
consider the above regularized empirical risk minimization problem with a general kernel and associated feature map formula with formula
the regularization term formula penalizes each formula component independently which means that the algorithm will suppress input variables independently from each other
in several situations we may want to impose more structure in the regularization process so that for example input variables are suppressed according to predefined groups
structured sparsity regularization methods allow to impose such structure by adding structure to the norms defining the regularization term
the nonoverlapping group case is the most basic instance of structured sparsity
in it an a priori partition of the coefficient vector formula in formula nonoverlapping groups is assumed
let formula be the vector of coefficients in group formula we can define a regularization term and its group norm aswhere formula is the group formula norm formula  formula is group formula and formula is the jth component of group formula
the above norm is also referred to as group lasso
this regularizer will force entire coefficient groups towards zero rather than individual coefficients
as the groups are nonoverlapping the set of nonzero coefficients can be obtained as the union of the groups that were not set to zero and conversely for the set of zero coefficients
overlapping groups is the structure sparsity case where a variable can belong to more than one group formula
this case is often of interest as it can represent a more general class of relationships among variables than nonoverlapping groups can such as tree structures or other type of graphs
there are two types of overlapping group sparsity regularization approaches which are used to model different types of input variable relationshipsthe intersection of complements approach is used in cases when we want to select only those input variables that have positive coefficients in all groups they belong to
consider again the group lasso for a regularized empirical risk minimization problemwhere formula is the group formula norm formula is group formula and formula is the jth component of group formula
as in the nonoverlapping groups case the group lasso regularizer will potentially set entire groups of coefficients to zero
selected variables are those with coefficients formula
however as in this case groups may overlap we take the intersection of the complements of those groups that are not set to zero
this intersection of complements selection criteria implies the modeling choice that we allow some coefficients within a particular group formula to be set to zero while others within the same group formula may remain positive
in other words coefficients within a group may differ depending on the several group memberships that each variable within the group may have
a different approach is to consider union of groups for variable selection
this approach captures the modeling situation where variables can be selected as long as they belong at least to one group with positive coefficients
this modeling perspective implies that we want to preserve group structure
the formulation of the union of groups approach is also referred to as latent group lasso and requires to modify the group formula norm considered above and introduce the following regularizer where formula formula is the vector of coefficients of group g and formula is a vector with coefficients formula for all variables formula in group formula  and formula in all others i
formula if formula in group formula and formula otherwise
this regularizer can be interpreted as effectively replicating variables that belong to more than one group therefore conserving group structure
as intended by the union of groups approach requiring formula produces a vector of weights w that effectively sums up the weights of all variables across all groups they belong to
the objective function using group lasso consists of an error function which is generally required to be convex but not necessarily strongly convex and a group formula regularization term
an issue with this objective function is that it is convex but not necessarily strongly convex and thus generally does not lead to unique solutions
an example of a way to fix this is to introduce the squared formula norm of the weight vector as an additional regularization term while keeping the formula regularization term from the group lasso approach
if the coefficient of the squared formula norm term is greater than formula then because the squared formula norm term is strongly convex the resulting objective function will also be strongly convex
provided that the formula coefficient is suitably small but still positive the weight vector minimizing the resulting objective function is generally very close to a weight vector that minimizes the objective function that would result from removing the group formula regularization term altogether from the original objective function the latter scenario corresponds to the group lasso approach
thus this approach allows for simpler optimization while maintaining sparsity
see submodular set functionbesides the norms discussed above other norms used in structured sparsity methods include hierarchical norms and norms defined on grids
these norms arise from submodular functions and allow the incorporation of prior assumptions on the structure of the input variables
in the context of hierarchical norms this structure can be represented as a directed acyclic graph over the variables while in the context of gridbased norms the structure can be represented using a grid
see unsupervised learningunsupervised learning methods are often used to learn the parameters of latent variable models
latent variable models are statistical models where in addition to the observed variables a set of latent variables also exists which is not observed
often in such models hierarchies are assumed between the variables of the system this system of hierarchies can be represented using directed acyclic graphs
hierarchies of latent variables have emerged as a natural structure in several applications notably to model text documents
hierarchical models using bayesian nonparametric methods have been used to learn topic models which are statistical models for discovering the abstract topics that occur in a collection of documents
hierarchies have also been considered in the context of kernel methods
hierarchical norms have been applied to bioinformatics computer vision and topic models
if the structure assumed over variables is in the form of a d d or d grid then submodular functions based on overlapping groups can be considered as norms leading to stable sets equal to rectangular or convex shapes
such methods have applications in computer visionthe problem of choosing the best subset of input variables can be naturally formulated under a penalization framework aswhere formula denotes the formula norm defined as the number of nonzero entries of the vector formula
although this formulation makes sense from a modeling perspective it is computationally unfeasible as it is equivalent to an exhaustive search evaluating all possible subsets of variables
two main approaches for solving the optimization problem are  greedy methods such as stepwise regression in statistics or matching pursuit in signal processing and  convex relaxation formulation approaches and proximal gradient optimization methods
a natural approximation for the best subset selection problem is the formula norm regularizationsuch as scheme is called basis pursuit or the lasso which substitutes the formula norm for the convex nondifferentiable formula norm
proximal gradient methods also called forwardbackward splitting are optimization methods useful for minimizing functions with a convex and differentiable component and a convex potentially nondifferentiable component
as such proximal gradient methods are useful for solving sparsity and structured sparsity regularization problems of the following form where formula is a convex and differentiable loss function like the quadratic loss and formula is a convex potentially nondifferentiable regularizer such as the formula norm
structured sparsity regularization can be applied in the context of multiple kernel learning
multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or nonlinear combination of kernels as part of the algorithm
in the algorithms mentioned above a whole space was taken into consideration at once and was partitioned into groups i
subspaces
a complementary point of view is to consider the case in which distinct spaces are combined to obtain a new one
it is useful to discuss this idea considering finite dictionaries
finite dictionaries with linearly independent elements  these elements are also known as atoms  refer to finite sets of linearly independent basis functions the linear combinations of which define hypothesis spaces
finite dictionaries can be used to define specific kernels as will be shown
assume for this example that rather than only one dictionary several finite dictionaries are considered
for simplicity the case in which there are only two dictionaries formula and formula where formula and formula are integers will be considered
the atoms in formula as well as the atoms in formula are assumed to be linearly independent
let formula be the union of the two dictionaries
consider the linear space of functions formula given by linear combinations of the formformulafor some coefficient vectors formula where formula
assume the atoms in formula to still be linearly independent or equivalently that the map formula is one to one
the functions in the space formula can be seen as the sums of two components one in the space formula the linear combinations of atoms in formula and one in formula the linear combinations of the atoms in formula
one choice of norm on this space is formula
note that we can now view formula as a function space in which formula formula are subspaces
in view of the linear independence assumption formula can be identified with formula and formula with formula respectively
the norm mentioned above can be seen as the group norm in formulaassociated to the subspaces formula formula providing a connection to structured sparsity regularization
here formula formula and formula can be seen to be the reproducing kernel hilbert spaces with corresponding feature maps formula given by formula formula given by formula and formula given by the concatenation of formula respectively
in the structured sparsity regularization approach to this scenario the relevant groups of variables which the group norms consider correspond to the subspaces formula and formula
this approach promotes setting the groups of coefficients corresponding to these subspaces to zero as opposed to only individual coefficients promoting sparse multiple kernel learning
the above reasoning directly generalizes to any finite number of dictionaries or feature maps
it can be extended to feature maps inducing infinite dimensional hypothesisspaces
considering sparse multiple kernel learning is useful in several situations including the following data fusion when each kernel corresponds to a different kind of modalityfeature
nonlinear variable selection consider kernels formula depending only one dimension of the input
generally sparse multiple kernel learning is particularly useful when there are many kernels and model selection and interpretability are important
structured sparsity regularization methods have been used in a number of settings where it is desired to impose an a priori input variable structure to the regularization process
some such applications are
caffe convolutional architecture for fast feature embedding is a deep learning framework originally developed at uc berkeley
it is open source under a bsd license
it is written in c with a python interface
yangqing jia created the caffe project during his phd at uc berkeley
now there are many contributors to the project and it is hosted at github
caffe supports many different types of deep learning architectures geared towards image classification and image segmentation
it supports cnn rcnn lstm and fully connected neural network designs
caffe supports gpu and cpubased acceleration computational kernel libraries such as nvidia cudnn and intel mkl
caffe is being used in academic research projects startup prototypes and even largescale industrial applications in vision speech and multimedia
yahoo has also integrated caffe with apache spark to create caffeonspark a distributed deep learning framework
in april  facebook announced caffe which includes new features such as recurrent neural networks
end of march  caffe was merged into pytorch
relational data mining is the data mining technique for relationaldatabases
unlike traditional data mining algorithms which look forpatterns in a single table propositional patterns relational data mining algorithms look for patterns among multiple tablesrelational patterns
for most types of propositionalpatterns there are corresponding relational patterns
for examplethere are relational classification rules relational classification relational regression tree and relational association rules
there are several approaches to relational data miningmultirelation association rules multirelation association rules mrar is a new class of association rules which in contrast to primitive simple and even multirelational association rules that are usually extracted from multirelational databases each rule item consists of one entity but several relations
these relations indicate indirect relationship between the entities
consider the following mrar where the first item consists of three relations live in nearby and humid those who live in a place which is near by a city with humid climate type and also are younger than   their health condition is good
such association rules are extractable from rdbms data or semantic web data
the bagofwords model is a simplifying representation used in natural language processing and information retrieval ir
also known as the vector space model
in this model a text such as a sentence or a document is represented as the bag multiset of its words disregarding grammar and even word order but keeping multiplicity
the bagofwords model has also been used for computer vision
the bagofwords model is commonly used in methods of document classification where the frequency of occurrence of each word is used as a feature for training a classifier
an early reference to bag of words in a linguistic context can be found in zellig harriss  article on distributional structure
the following models a text document using bagofwords
here are two simple text documents john likes to watch movies
mary likes movies too
john also likes to watch football games
based on these two text documents a list constructed as follows for each documentjohnlikestowatchmoviesmarylikesmoviestoojohnalsolikestowatchfootballgamesrepresenting each bagofwords as a json object and attributing to the respective javascript variablebow  johnlikestowatchmoviesmarytoobow  johnalsolikestowatchfootballgameseach key is the word and each value is the number of occurrences of that word in the given text document
the order of elements is free so for example codice is also bow
it is also what we expect from a strict json object representation
note if another document is like a union of these two  john likes to watch movies
mary likes movies too
john also likes to watch football games
its javascript representation will bebow  johnlikestowatchmoviesmarytooalsofootballgamesso as we see in the bag algebra the union of two documents in the bagsofwords representation is formally the disjoint union summing the multiplicities of each element
formula
in practice the bagofwords model is mainly used as a tool of feature generation
after transforming the text into a bag of words we can calculate various measures to characterize the text
the most common type of characteristics or features calculated from the bagofwords model is term frequency namely the number of times a term appears in the text
for the example above we can construct the following two lists to record the term frequencies of all the distinct words                    each entry of the lists refers to count of the corresponding entry in the list this is also the histogram representation
for example in the first list which represents document  the first two entries are
the first entry corresponds to the word john which is the first word in the list and its value is  because john appears in the first document  time
similarly the second entry corresponds to the word likes which is the second word in the list and its value is  because likes appears in the first document  times
this list or vector representation does not preserve the order of the words in the original sentences which is just the main feature of the bagofwords model
this kind of representation has several successful applications for example email filtering
however term frequencies are not necessarily the best representation for the text
common words like the a to are almost always the terms with highest frequency in the text
thus having a high raw count does not necessarily mean that the corresponding word is more important
to address this problem one of the most popular ways to normalize the term frequencies is to weight a term by the inverse of document frequency or tfidf
additionally for the specific purpose of classification supervised alternatives have been developed that take into account the class label of a document
lastly binary presenceabsence or  weighting is used in place of frequencies for some problems
for instance this option is implemented in the weka machine learning software system
bagofword model is an orderless document representationonly the counts of words mattered
for instance in the above example john likes to watch movies
mary likes movies too the bagofwords representation will not reveal the fact that a persons name is always followed by the verb likes in this text
as an alternative the ngram model can be used to store this spatial information within the text
applying to the same example above a bigram model will parse the text into following units and store the term frequency of each unit as before
conceptually we can view bagofword model as a special case of the ngram model with n
for n the model is named wshingling where w is equivalent to n denoting the number of grouped words
see language model for a more detailed discussion
a common alternative to the use of dictionaries is the hashing trick where words are directly mapped to indices with a hashing function
by mapping words to indices directly with a hash function no memory is required to store a dictionary
hash collisions are typically dealt with by using freedup memory to increase the number of hash buckets
in practice hashing greatly simplifies the implementation of bagofwords models and improves their scalability
in bayesian spam filtering an email message is modeled as an unordered collection of words selected from one of two probability distributions one representing spam and one representing legitimate email ham
imagine that there are two literal bags full of words
one bag is filled with words found in spam messages and the other bag is filled with words found in legitimate email
while any given word is likely to be found somewhere in both bags the spam bag will contain spamrelated words such as stock viagra and buy much more frequently while the ham bag will contain more words related to the users friends or workplace
to classify an email message the bayesian spam filter assumes that the message is a pile of words that has been poured out randomly from one of the two bags and uses bayesian probability to determine which bag it is more likely to be
sparse dictionary learning is a representation learning method which aims at finding a sparse representation of the input data also known as sparse coding in the form of a linear combination of basic elements as well as those basic elements themselves
these elements are called atoms and they compose a dictionary
atoms in the dictionary are not required to be orthogonal and they may be an overcomplete spanning set
this problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed
the above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in sparsity and flexibility of the representation
one of the most important applications of sparse dictionary learning is in the field of compressed sensing or signal recovery
in compressed sensing a high dimensional signal can be recovered with only a few linear measurements provided that the signal is sparse or nearly sparse
since not all signals satisfy this sparsity condition it is of great importance to find a sparse representation of that signal such as the wavelet transform or the directional gradient of a rasterized matrix
once a matrix or a high dimensional vector is transferred to a sparse space different recovery algorithms like basis pursuit cosamp or fast noniterative algorithms can be used to recover the signal
one of the key principles of dictionary learning is that the dictionary has to be inferred from the input data
the emergence of sparse dictionary learning methods was stimulated by the fact that in signal processing one typically wants to represent the input data using as few components as possible
before this approach the general practice was to use predefined dictionaries such as fourier or wavelet transforms
however in certain cases a dictionary that is trained to fit the input data can significantly improve the sparsity which has applications in data decomposition compression and analysis and has been used in the fields of image denoising and classification video and audio processing
sparsity and overcomplete dictionaries have immense applications in image compression image fusion and inpainting
given the input dataset formula we wish to find a dictionary formula and a representation formula such that both formula is minimized and the representations formula are sparse enough
this can be formulated as the following optimization problemformula where formulaformula is required to constrain formula so that its atoms would not reach arbitrarily high values allowing for arbitrarily low but nonzero values of formula
the minimization problem above is not convex because of the norm and solving this problem is nphard
in some cases lnorm is known to ensure sparsity and so the above becomes a convex optimization problem with respect to each of the variables formula and formula when the other one is fixed but it is not jointly convex in formula
the dictionary formula defined above can be undercomplete if formula or overcomplete in case formula with the latter being a typical assumption for a sparse dictionary learning problem
the case of a complete dictionary does not provide any improvement from a representational point of view and thus isnt considered
undercomplete dictionaries represent the setup in which the actual input data lies in a lowerdimensional space
this case is strongly related to dimensionality reduction and techniques like principal component analysis which require atoms formula to be orthogonal
the choice of these subspaces is crucial for efficient dimensionality reduction but it is not trivial
and dimensionality reduction based on dictionary representation can be extended to address specific tasks such as data analysis or classification
however their main downside is limiting the choice of atoms
overcomplete dictionaries however do not require the atoms to be orthogonal they will never be a basis anyway thus allowing for more flexible dictionaries and richer data representations
an overcomplete dictionary which allows for sparse representation of signal can be a famous transform matrix wavelets transform fourier transform or it can be formulated so that its elements are changed in such a way that it sparsely represents given signal in a best way
learned dictionaries are capable to give more sparse solution as compared to predefined transform matrices
as the optimization problem described above can be solved as a convex problem with respect to either dictionary or sparse coding while the other one of the two is fixed most of the algorithms are based on the idea of iteratively updating one and then the other
the problem of finding an optimal sparse coding formula with a given dictionary formula is known as sparse approximation or sometimes just sparse coding problem
there has been developed a number of algorithms to solve it such as matching pursuit and lasso which are incorporated into the algorithms described below
the method of optimal directions or mod was one of the first methods introduced to tackle the sparse dictionary learning problem
the core idea of it is to solve the minimization problem subject to the limited number of nonzero components of the representation vectorformulahere formula denotes the frobenius norm
mod alternates between getting the sparse coding using a method such as matching pursuit and updating the dictionary by computing the analytical solution of the problem given by formula where formula is a moorepenrose pseudoinverse
after this update formula is renormalized to fit the constraints and the new sparse coding is obtained again
the process is repeated until convergence or until a sufficiently small residue
mod has proved to be a very efficient method for lowdimensional input data formula requiring just a few iterations to converge
however due to the high complexity of the matrixinversion operation computing the pseudoinverse in highdimensional cases is in many cases intractable
this shortcoming has inspired the development of other dictionary learning methods
ksvd is an algorithm that performs svd at its core to update the atoms of the dictionary one by one and basically is a generalization of kmeans
it enforces that each element of the input data formula is encoded by a linear combination of not more than formula elements in a way identical to the mod approachformulathis algorithms essence is to first fix the dictionary find the best possible formula under the above constraint using orthogonal matching pursuit and then iteratively update the atoms of dictionary formula in the following mannerformulathe next steps of the algorithm include rank approximation of the residual matrix formula updating formula and enforcing the sparsity of formula after the update
this algorithm is considered to be standard for dictionary learning and is used in a variety of applications
however it shares weaknesses with mod being efficient only for signals with relatively low dimensionality and having the possibility for being stuck at local minima
one can also apply a widespread stochastic gradient descent method with iterative projection to solve this problem
the idea of this method is to update the dictionary using the first order stochastic gradient and project it on the constraint set formula
the step that occurs at ith iteration is described by this expressionformula where formula is a random subset of formula and formula is a gradient step
an algorithm based on solving a dual lagrangian problem provides an efficient way to solve for the dictionary having no complications induced by the sparsity function
consider the following lagrangianformula where formula is a constraint on the norm of the atoms and formula are the socalled dual variables forming the diagonal matrix formula
we can then provide an analytical expression for the lagrange dual after minimization over formulaformula
after applying one of the optimization methods to the value of the dual such as newtons method or conjugate gradient we get the value of formulaformulasolving this problem is less computational hard because the amount of dual variables formula is a lot of times much less than the amount of variables in the primal problem
parametric training methods are aimed to incorporate the best of both worlds  the realm of analytically constructed dictionaries and the learned ones
this allows to construct more powerful generalized dictionaries that can potentially be applied to the cases of arbitrarysized signals
notable approaches include many common approaches to sparse dictionary learning rely on the fact that the whole input data formula or at least a large enough training dataset is available for the algorithm
however this might not be the case in the realworld scenario as the size of the input data might be too big to fit it into memory
the other case where this assumption can not be made is when the input data comes in a form of a stream
such cases lie in the field of study of online learning which essentially suggests iteratively updating the model upon the new data points formula becoming available
a dictionary can be learned in an online manner the following waythis method allows us to gradually update the dictionary as new data becomes available for sparse representation learning and helps drastically reduce the amount of memory needed to store the dataset which often has a huge size
the dictionary learning framework namely the linear decomposition of an input signal using a few basis elements learned from data itself has led to stateofart results in various image and video processing tasks
this technique can be applied to classification problems in a way that if we have built specific dictionaries for each class the input signal can be classified by finding the dictionary corresponding to the sparsest representation
it also has properties that are useful for signal denoising since usually one can learn a dictionary to represent the meaningful part of the input signal in a sparse way but the noise in the input will have a much less sparse representation
sparse dictionary learning has been successfully applied to various image video and audio processing tasks as well as to texture synthesis and unsupervised clustering
in evaluations with the bagofwords model sparse coding was found empirically to outperform other coding approaches on the object category recognition tasks
dictionary learning is used to analyse medical signals in detail
such medical signals include those from electroencephalography eeg electrocardiography ecg magnetic resonance imaging mri functional mri fmri and ultrasound computer tomography usct where different assumptions are used to analyze each signal
in machine learning multipleinstance learning mil is a variation on supervised learning
instead of receiving a set of instances which are individually labeled the learner receives a set of labeled bags each containing many instances
in the simple case of multipleinstance binary classification a bag may be labeled negative if all the instances in it are negative
on the other hand a bag is labeled positive if there is at least one instance in it which is positive
from a collection of labeled bags the learner tries to either i induce a concept that will label individual instances correctly or ii learn how to label bags without inducing the concept
take image classification for example in
given an image we want to know its target class based on its visual content
for instance the target class might be beach where the image contains both sand and water
in mil terms the image is described as a bag formula where eachformula is the feature vector called instance extracted from the corresponding ith region in the image and n is the total regions instances partitioning the image
the bag is labeled positive beach if it contains both sand region instances and water region instances
multipleinstance learning was originally proposed under this name by  but earlier examples of similar research exist for instance in the work on handwritten digit recognition by
recent reviews of the mil literature include  which provides an extensive review and comparative study of the different paradigms and  which provides a thorough review of the different assumptions used by different paradigms in the literature
examples of where mil is applied arenumerous researchers have worked on adapting classical classification techniques such as support vector machines or boosting to work within the context of multipleinstance learning
the ugly duckling theorem is an argument asserting that classification is impossible without some sort of bias
more particularly it assumes finitely many properties combinable by logical connectives and finitely many objects it asserts that any two different objects share the same number of extensional properties
the theorem is named after hans christian andersens story the ugly duckling because it shows that a duckling is just as similar to a swan as two duckling are to each other
it was proposed by satosi watanabe in
suppose there are n things in the universe and one wants to put them into classes or categories
one has no preconceived ideas or biases about what sorts of categories are natural or normal and what are not
so one has to consider all the possible classes that could be all the possible ways of making sets out of the n objects
there are formula such ways the size of the power set of n objects
one can use that to measure the similarity between two objects and one would see how many sets they have in common
however one can not
any two objects have exactly the same number of classes in common if we can form any possible class namely formula half the total number of classes there are
to see this is so one may imagine each class is a represented by an nbit string or binary encoded integer with a zero for each element not in the class and a one for each element in the class
as one finds there are formula such strings
as all possible choices of zeros and ones are there any two bitpositions will agree exactly half the time
one may pick two elements and reorder the bits so they are the first two and imagine the numbers sorted lexicographically
the first formula numbers will have bit  set to zero and the second formula will have it set to one
within each of those blocks the top formula will have bit  set to zero and the other formula will have it as one so they agree on two blocks of formula or on half of all the cases
no matter which two elements one picks
so if we have no preconceived bias about which categories are better everything is then equally similar or equally dissimilar
the number of predicates simultaneously satisfied by two nonidentical elements is constant over all such pairs and is the same as the number of those satisfied by one
thus some kind of inductive bias is needed to make judgements i
to prefer certain categories over others
let formula be a set of vectors of formula booleans each
the ugly duckling is the vector which is least like the others
given the booleans this can be computed using hamming distance
however the choice of boolean features to consider could have been somewhat arbitrary
perhaps there were features derivable from the original features that were important for identifying the ugly duckling
the set of booleans in the vector can be extended with new features computed as boolean functions of the formula original features
the only canonical way to do this is to extend it with all possible boolean functions
the resulting completed vectors have formula features
the ugly duckling theorem states that there is no ugly duckling because any two completed vectors will either be equal or differ in exactly half of the features
proof
let x and y be two vectors
if they are the same then their completed vectors must also be the same because any boolean function of x will agree with the same boolean function of y
if x and y are different then there exists a coordinate formula where the formulath coordinate of formula differs from the formulath coordinate of formula
now the completed features contain every boolean function on formula boolean variables with each one exactly once
viewing these boolean functions as polynomials in formula variables over gf segregate the functions into pairs formula where formula contains the formulath coordinate as a linear term and formula is formula without that linear term
now for every such pair formula formula and formula will agree on exactly one of the two functions
if they agree on one they must disagree on the other and vice versa
this proof is believed to be due to watanabe
would be to introduce a constraint on how similarity is measured by limiting the properties involved in classification say between a and b
however medin et al
point out that this does not actually resolve the arbitrariness or bias problem since in what respects a is similar to b varies with the stimulus context and task so that there is no unique answer to the question of how similar is one object to another
for example a barberpole and a zebra would be more similar than a horse and a zebra if the feature striped had sufficient weight
of course if these feature weights were fixed then these similarity relations would be constrained
yet the property striped as a weight fix or constraint is arbitrary itself meaning unless one can specify such criteria then the claim that categorization is based on attribute matching is almost entirely vacuous
stamos  has attempted to solve the ugly ducking theorem by showing some judgments of overall similarity are nonarbitrary in the sense they are usefulunless some properties are considered more salient or weighted more important than others everything will appear equally similar hence watanabe  wrote any objects in so far as they are distinguishable are equally similar
in a weaker setting that assumes infinitely many properties murphy and medin  give an example of two putative classified things plums and lawnmowers
cleverbot is a chatterbot web application that uses an artificial intelligence ai algorithm to have conversations with humans
it was created by british ai scientist rollo carpenter
it was preceded by jabberwacky a chatbot project that began in  and went online in
in its first decade cleverbot held several thousand conversations with carpenter and his associates
since launching on the web the number of conversations held has exceeded  million
besides the web application cleverbot is also available as an ios android and windows phone app
unlike some other chatterbots cleverbots responses are not preprogrammed
instead it learns from human input humans type into the box below the cleverbot logo and the system finds all keywords or an exact phrase matching the input
after searching through its saved conversations it responds to the input by finding how a human responded to that input when it was asked in part or in full by cleverbot
cleverbot participated in a formal turing test at the  techniche festival at the indian institute of technology guwahati on september
out of the  votes cast cleverbot was judged to be
human compared to the rating of
human achieved by human participants
a score of
or higher is often considered to be a passing grade
the software running for the event had to handle just  or  simultaneous requests whereas online cleverbot is usually talking to around  people at once
cleverbot is constantly learning growing in data size at a rate of  to  million interactions per second
updates to the software have been mostly behind the scenes
in  cleverbot was upgraded to use gpu serving techniques
the program chooses how to respond to users fuzzily the whole of the conversation being compared to the millions that have taken place before
cleverbot now uses over  million interactions about  of the data it has already accumulated
the developers of cleverbot are attempting to build a new version using machine learning techniques
a significant part of the engine behind cleverbot and an api for accessing it has been made available to developers in the form of cleverscript
a service for directly accessing cleverbot has been made available to developers in the form of cleverbot
io
an app that uses the cleverscript engine to play a game of  questions has been launched under the name clevernator
unlike other such games the player asks the questions and it is the role of the ai to understand and answer factually
an app that allows owners to create and talk to their own small cleverbotlike ai has been launched called cleverme for apple products
in early  a twitch stream of two google home devices modified to talk to each other using cleverbot
io garnered over  visitors and over  peak concurrent viewers
a time series is a series of data points indexed or listed or graphed in time order
most commonly a time series is a sequence taken at successive equally spaced points in time
thus it is a sequence of discretetime data
examples of time series are heights of ocean tides counts of sunspots and the daily closing value of the dow jones industrial average
time series are very frequently plotted via line charts
time series are used in statistics signal processing pattern recognition econometrics mathematical finance weather forecasting earthquake prediction electroencephalography control engineering astronomy communications engineering and largely in any domain of applied science and engineering which involves temporal measurements
time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data
time series forecasting is the use of a model to predict future values based on previously observed values
while regression analysis is often employed in such a way as to test theories that the current values of one or more independent time series affect the current value of another time series this type of analysis of time series is not called time series analysis which focuses on comparing values of a single time series or multiple dependent time series at different points in time
interrupted time series analysis is the analysis of interventions on a single time series
time series data have a natural temporal ordering
this makes time series analysis distinct from crosssectional studies in which there is no natural ordering of the observations e
explaining peoples wages by reference to their respective education levels where the individuals data could be entered in any order
time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations e
accounting for house prices by the location as well as the intrinsic characteristics of the houses
a stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart
in addition time series models will often make use of the natural oneway ordering of time so that values for a given period will be expressed as deriving in some way from past values rather than from future values see time reversibility
time series analysis can be applied to realvalued continuous data discrete numeric data or discrete symbolic data i
sequences of characters such as letters and words in the english language
methods for time series analysis may be divided into two classes frequencydomain methods and timedomain methods
the former include spectral analysis and wavelet analysis the latter include autocorrelation and crosscorrelation analysis
in the time domain correlation and analysis can be made in a filterlike manner using scaled correlation thereby mitigating the need to operate in the frequency domain
additionally time series analysis techniques may be divided into parametric and nonparametric methods
the parametric approaches assume that the underlying stationary stochastic process has a certain structure which can be described using a small number of parameters for example using an autoregressive or moving average model
in these approaches the task is to estimate the parameters of the model that describes the stochastic process
by contrast nonparametric approaches explicitly estimate the covariance or the spectrum of the process without assuming that the process has any particular structure
methods of time series analysis may also be divided into linear and nonlinear and univariate and multivariate
a time series is one type of panel data
panel data is the general class a multidimensional data set whereas a time series data set is a onedimensional panel as is a crosssectional dataset
a data set may exhibit characteristics of both panel data and time series data
one way to tell is to ask what makes one data record unique from the other records
if the answer is the time data field then this is a time series data set candidate
if determining a unique record requires a time data field and an additional identifier which is unrelated to time student id stock symbol country code then it is panel data candidate
if the differentiation lies on the nontime identifier then the data set is a crosssectional data set candidate
there are several types of motivation and data analysis available for time series which are appropriate for different purposes and etc
in the context of statistics econometrics quantitative finance seismology meteorology and geophysics the primary goal of time series analysis is forecasting
in the context of signal processing control engineering and communication engineering it is used for signal detection and estimation while in the context of data mining pattern recognition and machine learning time series analysis can be used for clustering classification query by content anomaly detection as well as forecasting
the clearest way to examine a regular time series manually is with a line chart such as the one shown for tuberculosis in the united states made with a spreadsheet program
the number of cases was standardized to a rate per  and the percent change per year in this rate was calculated
the nearly steadily dropping line shows that the tb incidence was decreasing in most years but the percent change in this rate varied by as much as   with surges in  and around the early s
the use of both vertical axes allows the comparison of two time series in one graphic
other techniques includecurve fitting is the process of constructing a curve or mathematical function that has the best fit to a series of data points possibly subject to constraints
curve fitting can involve either interpolation where an exact fit to the data is required or smoothing in which a smooth function is constructed that approximately fits the data
a related topic is regression analysis which focuses more on questions of statistical inference such as how much uncertainty is present in a curve that is fit to data observed with random errors
fitted curves can be used as an aid for data visualization to infer values of a function where no data are available and to summarize the relationships among two or more variables
extrapolation refers to the use of a fitted curve beyond the range of the observed data and is subject to a degree of uncertainty since it may reflect the method used to construct the curve as much as it reflects the observed data
the construction of economic time series involves the estimation of some components for some dates by interpolation between values benchmarks for earlier and later dates
interpolation is estimation of an unknown quantity between two known quantities historical data or drawing conclusions about missing information from the available information reading between the lines
interpolation is useful where the data surrounding the missing data is available and its trend seasonality and longerterm cycles are known
this is often done by using a related series known for all relevant dates
alternatively polynomial interpolation or spline interpolation is used where piecewise polynomial functions are fit into time intervals such that they fit smoothly together
a different problem which is closely related to interpolation is the approximation of a complicated function by a simple function also called regression
the main difference between regression and interpolation is that polynomial regression gives a single polynomial that models the entire data set
spline interpolation however yield a piecewise continuous function composed of many polynomials to model the data set
extrapolation is the process of estimating beyond the original observation range the value of a variable on the basis of its relationship with another variable
it is similar to interpolation which produces estimates between known observations but extrapolation is subject to greater uncertainty and a higher risk of producing meaningless results
in general a function approximation problem asks us to select a function among a welldefined class that closely matches approximates a target function in a taskspecific way
one can distinguish two major classes of function approximation problems first for known target functions approximation theory is the branch of numerical analysis that investigates how certain known functions for example special functions can be approximated by a specific class of functions for example polynomials or rational functions that often have desirable properties inexpensive computation continuity integral and limit values etc
second the target function call it g may be unknown instead of an explicit formula only a set of points a time series of the form x gx is provided
depending on the structure of the domain and codomain of g several techniques for approximating g may be applicable
for example if g is an operation on the real numbers techniques of interpolation extrapolation regression analysis and curve fitting can be used
if the codomain range or target set of g is a finite set one is dealing with a classification problem instead
a related problem of online time series approximation is to summarize the data in onepass and construct an approximate representation that can support a variety of time series queries with bounds on worstcase error
to some extent the different problems regression classification fitness approximation have received a unified treatment in statistical learning theory where they are viewed as supervised learning problems
in statistics prediction is a part of statistical inference
one particular approach to such inference is known as predictive inference but the prediction can be undertaken within any of the several approaches to statistical inference
indeed one description of statistics is that it provides a means of transferring knowledge about a sample of a population to the whole population and to other related populations which is not necessarily the same as prediction over time
when information is transferred across time often to specific points in time the process is known as forecasting
assigning time series pattern to a specific category for example identify a word based on series of hand movements in sign language
this approach is based on harmonic analysis and filtering of signals in the frequency domain using the fourier transform and spectral density estimation the development of which was significantly accelerated during world war ii by mathematician norbert wiener electrical engineers rudolf e
klmn dennis gabor and others for filtering signals from noise and predicting signal values at a certain point in time
see kalman filter estimation theory and digital signal processingsplitting a timeseries into a sequence of segments
it is often the case that a timeseries can be represented as a sequence of individual segments each with its own characteristic properties
for example the audio signal from a conference call can be partitioned into pieces corresponding to the times during which each person was speaking
in timeseries segmentation the goal is to identify the segment boundary points in the timeseries and to characterize the dynamical properties associated with each segment
one can approach this problem using changepoint detection or by modeling the timeseries as a more sophisticated system such as a markov jump linear system
models for time series data can have many forms and represent different stochastic processes
when modeling variations in the level of a process three broad classes of practical importance are the autoregressive ar models the integrated i models and the moving average ma models
these three classes depend linearly on previous data points
combinations of these ideas produce autoregressive moving average arma and autoregressive integrated moving average arima models
the autoregressive fractionally integrated moving average arfima model generalizes the former three
extensions of these classes to deal with vectorvalued data are available under the heading of multivariate timeseries models and sometimes the preceding acronyms are extended by including an initial v for vector as in var for vector autoregression
an additional set of extensions of these models is available for use where the observed timeseries is driven by some forcing timeseries which may not have a causal effect on the observed series the distinction from the multivariate case is that the forcing series may be deterministic or under the experimenters control
for these models the acronyms are extended with a final x for exogenous
nonlinear dependence of the level of a series on previous data points is of interest partly because of the possibility of producing a chaotic time series
however more importantly empirical investigations can indicate the advantage of using predictions derived from nonlinear models over those from linear models as for example in nonlinear autoregressive exogenous models
further references on nonlinear time series analysis kantz and schreiber and abarbanelamong other types of nonlinear time series models there are models to represent the changes of variance over time heteroskedasticity
these models represent autoregressive conditional heteroskedasticity arch and the collection comprises a wide variety of representation garch tarch egarch figarch cgarch etc
here changes in variability are related to or predicted by recent past values of the observed series
this is in contrast to other possible representations of locally varying variability where the variability might be modelled as being driven by a separate timevarying process as in a doubly stochastic model
in recent work on modelfree analyses wavelet transform based methods for example locally stationary wavelets and wavelet decomposed neural networks have gained favor
multiscale often referred to as multiresolution techniques decompose a given time series attempting to illustrate time dependence at multiple scales
see also markov switching multifractal msmf techniques for modeling volatility evolution
a hidden markov model hmm is a statistical markov model in which the system being modeled is assumed to be a markov process with unobserved hidden states
an hmm can be considered as the simplest dynamic bayesian network
hmm models are widely used in speech recognition for translating a time series of spoken words into text
a number of different notations are in use for timeseries analysis
a common notation specifying a time series x that is indexed by the natural numbers is writtenanother common notation iswhere t is the index set
there are two sets of conditions under which much of the theory is builthowever ideas of stationarity must be expanded to consider two important ideas strict stationarity and secondorder stationarity
both models and applications can be developed under each of these conditions although the models in the latter case might be considered as only partly specified
in addition timeseries analysis can be applied where the series are seasonally stationary or nonstationary
situations where the amplitudes of frequency components change with time can be dealt with in timefrequency analysis which makes use of a timefrequency representation of a timeseries or signal
tools for investigating timeseries data includetime series metrics or features that can be used for time series classification or regression analysistime series can be visualized with two categories of chart overlapping charts and separated charts
overlapping charts display alltime series on the same layout while separated charts presents them on different layouts but aligned for comparison purposeworking with time series data is a relatively common use for statistical analysis software
as a result of this there are many offerings both commercial and open source
some examples include
pattern recognition is a very active field of research intimately bound to machine learning
also known as classification or statistical classification pattern recognition aims at building a classifier that can determine the class of an input pattern
this procedure known as training corresponds to learning an unknown decision function based only on a set of inputoutput pairs formula that form the training data or training set
nonetheless in real world applications such as character recognition a certain amount of information on the problem is usually known beforehand
the incorporation of this prior knowledge into the training is the key element that will allow an increase of performance in many applications
prior knowledge refers to all information about the problem available in addition to the training data
however in this most general form determining a model from a finite set of samples without prior knowledge is an illposed problem in the sense that a unique model may not exist
many classifiers incorporate the general smoothness assumption that a test pattern similar to one of the training samples tends to be assigned to the same class
the importance of prior knowledge in machine learning is suggested by its role in search and optimization
loosely the no free lunch theorem states that all search algorithms have the same average performance over all problems and thus implies that to gain in performance on a certain application one must use a specialized algorithm that includes some prior knowledge about the problem
the different types of prior knowledge encountered in pattern recognition are now regrouped under two main categories classinvariance and knowledge on the data
a very common type of prior knowledge in pattern recognition is the invariance of the class or the output of the classifier to a transformation of the input pattern
this type of knowledge is referred to as transformationinvariance
the mostly used transformations used in image recognition areincorporating the invariance to a transformation formula parametrized in formula into a classifier of output formula for an input pattern formula corresponds to enforcing the equalitylocal invariance can also be considered for a transformation centered at formula so that formula by using the constraintthe function formula in these equations can be either the decision function of the classifier or its realvalued output
another approach is to consider classinvariance with respect to a domain of the input space instead of a transformation
in this case the problem becomes finding formula so thatwhere formula is the membership class of the region formula of the input space
a different type of classinvariance found in pattern recognition is permutationinvariance i
invariance of the class to a permutation of elements in a structured input
a typical application of this type of prior knowledge is a classifier invariant to permutations of rows of the matrix inputs
other forms of prior knowledge than classinvariance concern the data more specifically and are thus of particular interest for realworld applications
the three particular cases that most often occur when gathering data areprior knowledge of these can enhance the quality of the recognition if included in the learning
moreover not taking into account the poor quality of some data or a large imbalance between the classes can mislead the decision of a classifier
in machine learning a hyperparameter is a parameter whose value is set before the learning process begins
by contrast the values of other parameters are derived via training
different model training algorithms require different hyperparameters some simple algorithms such as ordinary least squares regression require none
given these hyperparameters the training algorithm learns the parameters from the data
for instance lasso is an algorithm that adds a regularization hyperparameter to ordinary least squares regression which has to be set before estimating the parameters through the training algorithm
the time required to train and test a model can depend upon the choice of its hyperparameters
a hyperparameter is usually of continuous or integer type leading to mixedtype optimization problems
the existence of some hyperparameters is conditional upon the value of others e
the size of each hidden layer in a neural network can be conditional upon the number of layers
most performance variation can be attributed to just a few hyperparameters
the tunability of an algorithm hyperparameter or interacting hyperparameters is a measure of how much performance can be gained by tuning it
for an lstm while the learning rate followed by the network size are its most crucial hyperparameters whereas batching and momentum have no significant effect on its performance
although some research has advocated the use of minibatch sizes in the thousands other work has found the best performance with minibatch sizes between  and
an inherent stochasticity in learning directly implies that the empirical hyperparameter performance is not necessarily its true performance
methods that are not robust to simple changes in hyperparameters random seeds or even different implementations of the same algorithm cannot be integrated into mission critical control systems without significant simplification and robustification
reinforcement learning algorithms in particular require measuring their performance over a large number of random seeds and also measuring their sensitivity to choices of hyperparameters
their evaluation with a small number of random seeds does not capture performance adequately due to high variance
some reinforcement learning methods e
ddpg deep deterministic policy gradient are more sensitive to hyperparameter choices than others
hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given test data
the objective function takes a tuple of hyperparameters and returns the associated loss
apart from tuning hyperparameters machine learning involves storing and organizing the parameters and results and making sure they are reproducible
in the absence of a robust infrastructure for this purpose research code often evolves quickly and compromises essential aspects like bookkeeping and reproducibility
online collaboration platforms for machine learning go further by allowing scientists to automatically share organize and discuss experiments data and algorithms
a number of relevant services and open source software exist
in computer science a ball tree balltree or metric tree is a space partitioning data structure for organizing points in a multidimensional space
the ball tree gets its name from the fact that it partitions data points into a nested set of hyperspheres known as balls
the resulting data structure has characteristics that make it useful for a number of applications most notably nearest neighbor search
a ball tree is a binary tree in which every node defines a ddimensional hypersphere or ball containing a subset of the points to be searched
each internal node of the tree partitions the data points into two disjoint sets which are associated with different balls
while the balls themselves may intersect each point is assigned to one or the other ball in the partition according to its distance from the balls center
each leaf node in the tree defines a ball and enumerates all data points inside that ball
each node in the tree defines the smallest ball that contains all data points in its subtree
this gives rise to the useful property that for a given test point  the distance to any point in a ball in the tree is greater than or equal to the distance from to the ball
formallywhere formula is the minimum possible distance from any point in the ball to some point
balltrees are related to the mtree but only support binary splits whereas in the mtree each level splits formula to formula fold thus leading to a shallower tree structure therefore need fewer distance computations which usually yields faster queries
furthermore mtrees can better be stored on disk which is organized in pages
the mtree also keeps the distances from the parent node precomputed to speed up queries
vantagepoint trees are also similar but they binary split into one ball and the remaining data instead of using two balls
a number of ball tree construction algorithms are available
the goal of such an algorithm is to produce a tree that will efficiently support queries of the desired type e
nearestneighbor efficiently in the average case
the specific criteria of an ideal tree will depend on the type of question being answered and the distribution of the underlying data
however a generally applicable measure of an efficient tree is one that minimizes the total volume of its internal nodes
given the varied distributions of realworld data sets this is a difficult task but there are several heuristics that partition the data well in practice
in general there is a tradeoff between the cost of constructing a tree and the efficiency achieved by this metric
this section briefly describes the simplest of these algorithms
a more indepth discussion of five algorithms was given by stephen omohundro
the simplest such procedure is termed the kd construction algorithm by analogy with the process used to construct kd trees
this is an offline algorithm that is an algorithm that operates on the entire data set at once
the tree is built topdown by recursively splitting the data points into two sets
splits are chosen along the single dimension with the greatest spread of points with the sets partitioned by the median value of all points along that dimension
finding the split for each internal node requires linear time in the number of samples contained in that node yielding an algorithm with time complexity formula where n is the number of data points
function constructballtree isan important application of ball trees is expediting nearest neighbor search queries in which the objective is to find the k points in the tree that are closest to a given test point by some distance metric e
euclidean distance
a simple search algorithm sometimes called kns exploits the distance property of the ball tree
in particular if the algorithm is searching the data structure with a test point t and has already seen some point p that is closest to t among the points encountered so far then any subtree whose ball is further from t than p can be ignored for the rest of the search
the ball tree nearestneighbor algorithm examines nodes in depthfirst order starting at the root
during the search the algorithmmaintains a maxfirst priority queue often implemented with a heap denoted q here of the k nearest points encountered so far
at each node b it may perform one of three operations before finally returning an updated version of the priority queueperforming the recursive search in the order described in point  above increases likelihood that the further child will be pruned entirely during the search
function knnsearch isin comparison with several other data structures ball trees have been shown to perform fairly well on the nearestneighbor search problem particularly as their number of dimensions grows
however the best nearestneighbor data structure for a given application will depend on the dimensionality number of data points and underlying structure of the data
bayesian structural time series bsts model is a machine learning technique used for feature selection time series forecasting nowcasting inferring causal impact and other
the model is designed to work with time series data
the model has also promising application in the field of analytical marketing
in particular it can be used in order to assess how much different marketing campaigns have contributed to the change in web search volumes product sales brand popularity and other relevant indicators differenceindifferences model is a usual alternative approach in this case
in contrast to classical differenceindifferences schemes statespace models make it possible to i infer the temporal evolution of attributable impact ii incorporate empirical priors on the parameters in a fully bayesian treatment and iii flexibly accommodate multiple sources of variation including the timevarying influence of contemporaneous covariates i
synthetic controls
the model consists of three main partsthe model seems to discover not only correlations but also causations in the underlying data
a possible drawback of the model can be its relatively complicated mathematical underpinning and difficult implementation as a computer program
however the programming language r has readytouse packages for calculating the bsts model which do not require strong mathematical background from a researcher
a querylevel feature or qlf is a ranking feature utilized in a machinelearned ranking algorithm
example qlfs
adversarial machine learning is a research field that lies at the intersection of machine learning and computer security
it aims to enable the safe adoption of machine learning techniques in adversarial settings like spam filtering malware detection and biometric recognition
the problem arises from the fact that machine learning techniques were originally designed for stationary environments in which the training and test data are assumed to be generated from the same although possibly unknown distribution
in the presence of intelligent and adaptive adversaries however this working hypothesis is likely to be violated to at least some degree depending on the adversary
in fact a malicious adversary can carefully manipulate the input data exploiting specific vulnerabilities of learning algorithms to compromise the whole system security
examples include attacks in spam filtering where spam messages are obfuscated through misspelling of bad words or insertion of good words attacks in computer security e
to obfuscate malware code within network packets or mislead signature detection attacks in biometric recognition where fake biometric traits may be exploited to impersonate a legitimate user biometric spoofing or to compromise users template galleries that are adaptively updated over time
in  mit researchers d printed a toy turtle with a texture engineered to make googles object detection ai classify it as a rifle no matter the angle the turtle was viewed from
creating the turtle required only lowcost commercially available d printing technology
in  google brain published a machinetweaked image of a dog that looked like a cat both to computers and to humans
to understand the security properties of learning algorithms in adversarial settings one should address the following main issuesthis process amounts to simulating a proactive arms race instead of a reactive one as depicted in figures  and  where system designers try to anticipate the adversary in order to understand whether there are potential vulnerabilities that should be fixed in advance for instance by means of specific countermeasures such as additional features or different learning algorithms
however proactive approaches are not necessarily superior to reactive ones
for instance in the authors showed that under some circumstances reactive approaches are more suitable for improving system security
the first step of the abovesketched arms race is identifying potential attacks against machine learning algorithms
a substantial amount of work has been done in this direction
attacks against supervised machine learning algorithms have been categorized along three primary axes their influence on the classifier the security violation they cause and their specificity
this taxonomy has been extended into a more comprehensive threat model that allows one to make explicit assumptions on the adversarys goal knowledge of the attacked system capability of manipulating the input data andor the system components and on the corresponding potentially formallydefined attack strategy
details can be found here
two of the main attack scenarios identified according to this threat model are sketched below
evasion attacks are the most prevalent type of attack that may be encountered in adversarial settings during system operation
for instance spammers and hackers often attempt to evade detection by obfuscating the content of spam emails and malware code
in the evasion setting malicious samples are modified at test time to evade detection that is to be misclassified as legitimate
no influence over the training data is assumed
a clear example of evasion is imagebased spam in which the spam content is embedded within an attached image to evade the textual analysis performed by antispam filters
another example of evasion is given by spoofing attacks against biometric verification systems
machine learning algorithms are often retrained on data collected during operation to adapt to changes in the underlying data distribution
for instance intrusion detection systems idss are often retrained on a set of samples collected during network operation
within this scenario an attacker may poison the training data by injecting carefully designed samples to eventually compromise the whole learning process
poisoning may thus be regarded as an adversarial contamination of the training data
examples of poisoning attacks against machine learning algorithms including learning in the presence of worstcase adversarial label flips in the training data can be found in
clustering algorithms have been increasingly adopted in security applications to find dangerous or illicit activities
for instance clustering of malware and computer viruses aims to identify and categorize different existing malware families and to generate specific signatures for their detection by antiviruses or signaturebased intrusion detection systems like snort
however clustering algorithms have not been originally devised to deal with deliberate attack attempts that are designed to subvert the clustering process itself
whether clustering can be safely adopted in such settings thus remains questionable
preliminary work reporting some vulnerability of clustering can be found in
a number of defense mechanisms against evasion poisoning and privacy attacks have been proposed in the field of adversarial machine learning includingsome software libraries are available mainly for testing purposes and research
hierarchical deep learning is the machine learning task with the hierarchical labeled data a classification or categorization
since the examples given to the learner are hierarchical labeled the evaluation is based on the accuracy or fmeasure based on multi level of the model hierarchy
this model have been used for text classification as in hierarchical deep learning employs stacks of deep learning architectures to provide specialized understanding at each level of the data hierarchy
approaches as part of hdl includethe primary contribution of this technique is hierarchical classification of documents
a traditional multiclass classification technique can work well for a limited number classes but performance drops with increasing number of categories or classes as is present in hierarchically organized documents
many techniques works on hierarchical attention for text classification or multimodel deep learning for classification task
hence they provide extensions over current methods for document classification that only consider the multiclass problem
the methods described as hdltex can improved in multiple ways
additional training and testing with other hierarchically structured document data sets will continue to identify architectures that work best for these problems
also it is possible to extend the hierarchy to more than two levels to capture more of the complexity in the hierarchical classification
for example if keywords are treated as ordered then the hierarchy continues down multiple levels
hdltex can also be applied to unlabeled unsupervised documents such as those found in news or other media outlets
in hierarchical deep learning model this problem is solved by creating architectures that specialize deep learning approaches for their level of the document hierarchy
the structure of hierarchical deep learning for text hdltex architecture for each deep learning model is as followshierarchical deep learning can be used for sentiment analysis in any languages for social networks of documents
savi technology was founded in  and is based in alexandria virginia
savi provides the most complete sensor analytics solutions for organizations that face critical decisions based on the location and status of their assets
savi technology offers sensor analytics solutions for logistics and supply chain operations
it tracks shipment locations in real time and applies analytics to accurately predict arrival of goods
the company provides savi insight a solution that offers predictive and prescriptive supply chain analytics to forecast future outcomes prevent operational disruptions and reduce risk savi tracking a solution that monitors and provides operational intelligence for asset tracking journey management and electronic cargo tracking assets in motion etaaas a saas analytics solution that processes multiple realtime data sources enterprise resource planning erp and historical information and savi now a mobile application for tracking and tracing highvalue assets
it also offers tags that enable organizations to access realtime information on the location condition and security status of assets and shipments fixed and mobile readers radiofrequency identification devices and sensors and portable deployment kits
in addition the company provides professional services including program management systems integration system and network design support and hosting
it serves the u
department of defense the u
and allied militaries civilian governmental organizations and commercial companies as well as transportation pharmaceuticals retail life sciences and manufacturing industries worldwide
savi company websitelinkedinsavi technology overview
multilinear subspace learning is an approach to dimensionality reduction
dimensionality reduction can be performed on a data tensor whose observations have been vectorized and organized into a data tensor or whose observations are matrices that are concatenated into a data tensor
here are some examples of data tensors whose observations are vectorized or whose observations are matrices concatenated into data tensor images dd video sequences dd and hyperspectral cubes dd
the mapping from a highdimensional vector space to a set of lower dimensional vector spaces is a multilinear projection
multilinear subspace learning algorithms are higherorder generalizations of linear subspace learning methods such as principal component analysis pca independent component analysis ica linear discriminant analysis lda and canonical correlation analysis cca
with the advances in data acquisition and storage technology big data or massive data sets are being generated on a daily basis in a wide range of emerging applications
most of these big data are multidimensional
moreover they are usually veryhighdimensional with a large amount of redundancy and only occupying a part of the input space
therefore dimensionality reduction is frequently employed to map highdimensional data to a lowdimensional space while retaining as much information as possible
linear subspace learning algorithms are traditional dimensionality reduction techniques that represent input data as vectors and solve for an optimal linear mapping to a lowerdimensional space
unfortunately they often become inadequate when dealing with massive multidimensional data
they result in veryhighdimensional vectors lead to the estimation of a large number of parameters
multilinear subspace learning employ different types of data tensor analysis tools for dimensionality reduction
multilinear subspace learning can be applied to observations whose measurements were vectorized and organized into a data tensor or whose measurements are treated as a matrix and concatenated into a tensor
historically multilinear principal component analysis has been referred to as mmode pca a terminology which was coined by peter kroonenberg
in  vasilescu and terzopoulos introduced the multilinear pca terminology as a way to better differentiate between linear tensor decompositions and multilinear tensor decomposition as well as to better differentiate between analysis approaches that computed nd order statistics associated with each data tensor modeaxiss and subsequent work on multilinear independent component analysis that computed higher order statistics associated with each tensor modeaxis
mpca is an extension of pca
multilinear independent component analysis is an extension of ica
there are n sets of parameters to be solved one in each mode
the solution to one set often depends on the other sets except when n the linear case
therefore the suboptimal iterative procedure in is followed
this is originated from the alternating least square method for multiway data analysis
the advantages of msl over traditional linear subspace modeling in common domains where the representation is naturally somewhat tensorial arehowever msl algorithms are iterative and are not guaranteed to converge where an msl algorithm does converge it may do so at a local optimum
in contrast traditional linear subspace modeling techniques often produce an exact closedform solution
msl convergence problems can often be mitigated by choosing an appropriate subspace dimensionality and by appropriate strategies for initialization for termination and for choosing the order in which projections are solved
machine learning is a peerreviewed scientific journal published since
in  forty editors and members of the editorial board of machine learning resigned in order to support the journal of machine learning research jmlr saying that in the era of the internet it was detrimental for researchers to continue publishing their papers in expensive journals with payaccess archives
instead they wrote they supported the model of jmlr in which authors retained copyright over their papers and archives were freely available on the internet
following the mass resignation kluwer changed their publishing policy to allow authors to selfarchive their papers online after peerreview
bayesian optimization is a sequential design strategyfor global optimization of blackbox functions that doesnt require derivatives
the term is generally attributed to jonas mockus and is coined in his work from a series of publications on global optimization in the s and s
since the objective function is unknown the bayesian strategy is to treat it as a random function and place a prior over it
the prior captures our beliefs about the behaviour of the function
after gathering the function evaluations which are treated as data the prior is updatedto form the posterior distribution over the objective function
the posterior distribution in turn is used to constructan acquisition function often also referred to as infill sampling criteria that determines what the next query point should be
examples of acquisition functions include probability of improvementexpected improvement bayesian expected losses upper confidence bounds ucb thompson samplingand mixtures of these
they all tradeoff exploration and exploitation so as to minimize the number of function queries
as such bayesian optimization is well suited for functions that are very expensive to evaluate
the maximum of the acquisition function is typically found by resorting to discretization or by means of an auxiliary optimizer
the approach has been applied to solve a wide range of problems including learning to rank interactive animation robotics sensor networks automatic algorithm configuration automatic machine learning toolboxes reinforcement learning planning visual attention architecture configuration in deep learning static program analysis experimental particle physics etc
proximal gradient forward backward splitting methods for learning is an area of research in optimization and statistical learning theory which studies algorithms for a general class of convex regularization problems where the regularization penalty may not be differentiable
one such example is formula regularization also known as lasso of the formproximal gradient methods offer a general framework for solving regularization problems from statistical learning theory with penalties that are tailored to a specific problem application
such customized penalties can help to induce certain structure in problem solutions such as sparsity in the case of lasso or group structure in the case of group lasso
proximal gradient methods are applicable in a wide variety of scenarios for solving convex optimization problems of the formwhere formula is convex and differentiable with lipschitz continuous gradient formula is a convex lower semicontinuous function which is possibly nondifferentiable and formula is some set typically a hilbert space
the usual criterion of formula minimizes formula if and only if formula in the convex differentiable setting is now replaced bywhere formula denotes the subdifferential of a realvalued convex function formula
given a convex function formula an important operator to consider is its proximity operator formula defined bywhich is welldefined because of the strict convexity of the formula norm
the proximity operator can be seen as a generalization of a projection
we see that the proximity operator is important because formula is a minimizer to the problem formula if and only ifone important technique related to proximal gradient methods is the moreau decomposition which decomposes the identity operator as the sum of two proximity operators
namely let formula be a lower semicontinuous convex function on a vector space formula
we define its fenchel conjugate formula to be the functionthe general form of moreaus decomposition states that for any formula and any formula thatwhich for formula implies that formula
the moreau decomposition can be seen to be a generalization of the usual orthogonal decomposition of a vector space analogous with the fact that proximity operators are generalizations of projections
in certain situations it may be easier to compute the proximity operator for the conjugate formula instead of the function formula and therefore the moreau decomposition can be applied
this is the case for group lasso
consider the regularized empirical risk minimization problem with square loss and with the formula norm as the regularization penaltywhere formula the formula regularization problem is sometimes referred to as lasso least absolute shrinkage and selection operator
such formula regularization problems are interesting because they induce  sparse solutions that is solutions formula to the minimization problem have relatively few nonzero components
lasso can be seen to be a convex relaxation of the nonconvex problemwhere formula denotes the formula norm which is the number of nonzero entries of the vector formula
sparse solutions are of particular interest in learning theory for interpretability of results a sparse solution can identify a small number of important factors
for simplicity we restrict our attention to the problem where formula
to solve the problemwe consider our objective function in two parts a convex differentiable term formula and a convex function formula
note that formula is not strictly convex
let us compute the proximity operator for formula
first we find an alternative characterization of the proximity operator formula as followsformulafor formula it is easy to compute formula the formulath entry of formula is preciselyusing the recharacterization of the proximity operator given above for the choice of formula and formula we have that formula is defined entrywise bywhich is known as the soft thresholding operator formula
to finally solve the lasso problem we consider the fixed point equation shown earliergiven that we have computed the form of the proximity operator explicitly then we can define a standard fixed point iteration procedure
namely fix some initial formula and for formula definenote here the effective tradeoff between the empirical error term formula and the regularization penalty formula
this fixed point method has decoupled the effect of the two different convex functions which comprise the objective function into a gradient descent step formula and a soft thresholding step via formula
convergence of this fixed point scheme is wellstudied in the literature and is guaranteed under appropriate choice of step size formula and loss function such as the square loss taken here
accelerated methods were introduced by nesterov in  which improve the rate of convergence under certain regularity assumptions on formula
such methods have been studied extensively in previous years
for more general learning problems where the proximity operator cannot be computed explicitly for some regularization term formula such fixed point schemes can still be carried out using approximations to both the gradient and the proximity operator
there have been numerous developments within the past decade in convex optimization techniques which have influenced the application of proximal gradient methods in statistical learning theory
here we survey a few important topics which can greatly improve practical algorithmic performance of these methods
in the fixed point iteration schemeone can allow variable step size formula instead of a constant formula
numerous adaptive step size schemes have been proposed throughout the literature
applications of these schemes suggest that these can offer substantial improvement in number of iterations required for fixed point convergence
elastic net regularization offers an alternative to pure formula regularization
the problem of lasso formula regularization involves the penalty term formula which is not strictly convex
hence solutions to formula where formula is some empirical loss function need not be unique
this is often avoided by the inclusion of an additional strictly convex term such as an formula norm regularization penalty
for example one can consider the problemwhere formulafor formula the penalty term formula is now strictly convex and hence the minimization problem now admits a unique solution
it has been observed that for sufficiently small formula the additional penalty term formula acts as a preconditioner and can substantially improve convergence while not adversely affecting the sparsity of solutions
proximal gradient methods provide a general framework which is applicable to a wide variety of problems in statistical learning theory
certain problems in learning can often involve data which has additional structure that is known  a priori
in the past several years there have been new developments which incorporate information about group structure to provide methods which are tailored to different applications
here we survey a few such methods
group lasso is a generalization of the lasso method when features are grouped into disjoint blocks
suppose the features are grouped into blocks formula
here we take as a regularization penaltywhich is the sum of the formula norm on corresponding feature vectors for the different groups
a similar proximity operator analysis as above can be used to compute the proximity operator for this penalty
where the lasso penalty has a proximity operator which is soft thresholding on each individual component the proximity operator for the group lasso is soft thresholding on each group
for the group formula we have that proximity operator of formula is given bywhere formula is the formulath group
in contrast to lasso the derivation of the proximity operator for group lasso relies on the moreau decomposition
here the proximity operator of the conjugate of the group lasso penalty becomes a projection onto the ball of a dual norm
in contrast to the group lasso problem where features are grouped into disjoint blocks it may be the case that grouped features are overlapping or have a nested structure
such generalizations of group lasso have been considered in a variety of contexts
for overlapping groups one common approach is known as latent group lasso which introduces latent variables to account for overlap
nested group structures are studied in hierarchical structure prediction and with directed acyclic graphs
conditional random fields crfs are a class of statistical modeling method often applied in pattern recognition and machine learning and used for structured prediction
crfs fall into the sequence modeling family
whereas a discrete classifier predicts a label for a single sample without considering neighboring samples a crf can take context into account e
the linear chain crf which is popular in natural language processing predicts sequences of labels for sequences of input samples
crfs are a type of discriminative undirected probabilistic graphical model
it is used to encode known relationships between observations and construct consistent interpretations
it is often used for labeling or parsing of sequential data such as natural language processing or biological sequencesand in computer vision
specifically crfs find applications in pos tagging shallow parsingnamed entity recognitiongene finding and peptide critical functional region findingamong other tasks being an alternative to the related hidden markov models hmms
in computer vision crfs are often used for object recognition and image segmentation
lafferty mccallum and pereira define a crf on observations formula and random variables formula as followslet formula be a graph such thatformulathen formula is a conditional random field when the random variables formula conditioned on formula obey the markov property withrespect to the graph formula where formula meansthat formula and formula are neighbors in formula
what this means is that a crf is an undirected graphical model whose nodes can be divided into exactly two disjoint sets formula and formula the observed and output variables respectively the conditional distribution formula is then modeled
for general graphs the problem of exact inference in crfs is intractable
the inference problem for a crf is basically the same as for an mrf and the same arguments hold
however there exist special cases for which exact inference is feasibleif exact inference is impossible several algorithms can be used to obtain approximate solutions
these includelearning the parameters formula is usually done by maximum likelihood learning for formula
if all nodes have exponential family distributions and all nodes are observed during training this optimization is convex
it can be solved for example using gradient descent algorithms or quasinewton methods such as the lbfgs algorithm
on the other hand if some variables are unobserved the inference problem has to be solved for these variables
exact inference is intractable in general graphs so approximations have to be used
in sequence modeling the graph of interest is usually a chain graph
an input sequence of observed variables formula represents a sequence of observations and formula represents a hidden or unknown state variable that needs to be inferred given the observations
the formula are structured to form a chain with an edge between each formula and formula
as well as having a simple interpretation of the formula as labels for each element in the input sequence this layout admits efficient algorithms forthe conditional dependency of each formula on formula is defined through a fixed set of feature functions of the form formula which can informally be thought of as measurements on the input sequence that partially determine the likelihood of each possible value for formula
the model assigns each feature a numerical weight and combines them to determine the probability of a certain value for formula
linearchain crfs have many of the same applications as conceptually simpler hidden markov models hmms but relax certain assumptions about the input and output sequence distributions
an hmm can loosely be understood as a crf with very specific feature functions that use constant probabilities to model state transitions and emissions
conversely a crf can loosely be understood as a generalization of an hmm that makes the constant transition probabilities into arbitrary functions that vary across the positions in the sequence of hidden states depending on the input sequence
notably in contrast to hmms crfs can contain any number of feature functions the feature functions can inspect the entire input sequence formula at any point during inference and the range of the feature functions need not have a probabilistic interpretation
crfs can be extended into higher order models by making each formula dependent on a fixed number formula of previous variables formula
in conventional formulations of higher order crfs training and inference are only practical for small values of formula such as o   since their computational cost increases exponentially with formula
however another recent advance has managed to ameliorate these issues by leveraging concepts and tools from the field of bayesian nonparametrics
specifically the crfinfinity approach constitutes a crftype model that is capable of learning infinitelylong temporal dynamics in a scalable fashion
this is effected by introducing a novel potential function for crfs that is based on the sequence memoizer sm a nonparametric bayesian model for learning infinitelylong dynamics in sequential observations
to render such a model computationally tractable crfinfinity employs a meanfield approximation of the postulated novel potential functions which are driven by an sm
this allows for devising efficient approximate training and inference algorithms for the model without undermining its capability to capture and model temporal dependencies of arbitrary length
there exists another generalization of crfs the semimarkov conditional random field semicrf which models variablelength segmentations of the label sequence formula
this provides much of the power of higherorder crfs to model longrange dependencies of the formula at a reasonable computational cost
finally largemargin models for structured prediction such as the structured support vector machine can be seen as an alternative training procedure to crfs
latentdynamic conditional random fields ldcrf or discriminative probabilistic latent variable models dplvm are a type of crfs for sequence tagging tasks
they are latent variable models that are trained discriminatively
in an ldcrf like in any sequence tagging task given a sequence of observations x  formula the main problem the model must solve is how to assign a sequence of labels y  formula from one finite set of labels
instead of directly modeling yx as an ordinary linearchain crf would do a set of latent variables h is inserted between x and y using the chain rule of probabilitythis allows capturing latent structure between the observations and labels
while ldcrfs can be trained using quasinewton methods a specialized version of the perceptron algorithm called the latentvariable perceptron has been developed for them as well based on collins structured perceptron algorithm
these models find applications in computer vision specifically gesture recognition from video streams and shallow parsing
this is a partial list of software that implement generic crf tools
this is a partial list of software that implement crf related tools
this page is a timeline of machine learning
major discoveries achievements milestones and other major events are included
in machine learning a highway network is an approach to optimizing networks and increasing their depth
highway networks use learned gating mechanisms to regulate information flow inspired by long shortterm memory lstm recurrent neural networks
the gating mechanisms allow neural networks to have paths for information to follow across different layers information highways
highway networks have been used as part of text sequence labeling and speech recognition tasks
in machine learning systems which employ offline learning do not change their approximation of the target function when the initial training phase has been completed
these systems are also typically examples of eager learning
while in online learning only the set of possible elements is known in offline learning the identity of the elements as well as the order in which they are presented is known to the learner
uniform convergence in probability is a form of convergence in probability in statistical asymptotic theory and probability theory
it means that under certain conditions the empirical frequencies of all events in a certain eventfamily converge to their theoretical probabilities
uniform convergence in probability has applications to statistics as well as machine learning as part of statistical learning theory
the law of large numbers says that for each single event its empirical frequency in a sequence of independent trials converges with high probability to its theoretical probability
but in some applications we are interested not in a single event but in a whole family of events
we would like to know whether the empirical frequency of every event in the family converges to its theoretical probability simultaneously
the uniform convergence theorem gives a sufficient condition for this convergence to hold
roughly if the eventfamily is sufficiently simple its vc dimension is sufficiently small then uniform convergence holds
for a class of predicates formula defined on a set formula and a set of samples formula where formula the empirical frequency of formula on formula isthe theoretical probability of formula is defined as formulathe uniform convergence theorem states roughly that if formula is simple and we draw samples independently with replacement from formula according to any distribution formula then with high probability the empirical frequency will be close to its expected value which is the theoretical probability
here simple means that the vapnikchervonenkis dimension of the class formula is small relative to the size of the sample
in other words a sufficiently simple collection of functions behaves roughly the same on a small random sample as it does on the distribution as a whole
the uniform convergence theorem was first proved by vapnik and chervonenkis using the concept of growth function
the statement of the uniform convergence theorem is as followsif formula is a set of formulavalued functions defined on a set formula and formula is a probability distribution on formula then for formula and formula a positive integer we havethe maximum is guaranteed to exist since there is only a finite set of values that probability under a random permutation can take
lemma basing on the previous lemmaprooflet us define formula and formula which is at most formula
this means there are functions formula such that for any formula between formula and formula with formula for formulawe see that formula iff for some formula in formula satisfiesformula
hence if we define formula if formula and formula otherwise
for formula and formula we have that formula iff for some formula in formula satisfies formula
by union bound we getsince the distribution over the permutations formula is uniform for each formula so formula equals formula with equal probability
thuswhere the probability on the right is over formula and both the possibilities are equally likely
by hoeffdings inequality this is at most formula
finally combining all the three parts of the proof we get the uniform convergence theorem
in computational learning theory occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data
this is closely related to probably approximately correct pac learning where the learner is evaluated on its predictive power of a test set
occam learnability implies pac learning and for a wide variety of concept classes the converse is also true pac learnability implies occam learnability
occam learning is named after occams razor which is a principle stating that given all other things being equal a shorter explanation for observed data should be favored over a lengthier explanation
the theory of occam learning is a formal and mathematical justification for this principle
it was first shown by blumer et al
that occam learning implies pac learning which is the standard model of learning in computational learning theory
in other words parsimony of the output hypothesis implies predictive power
the succinctness of a concept formula in concept class formula can be expressed by the length formula of the shortest bit string that can represent formula in formula
occam learning connects the succinctness of a learning algorithms output to its predictive power on unseen data
let formula and formula be concept classes containing target concepts and hypotheses respectively
then for constants formula and formula a learning algorithm formula is an formulaoccam algorithm for formula using formula iff given a set formula of formula samples labeled according to a concept formula formula outputs a hypothesis formula such thatwhere formula is the maximum length of any sample formula
an occam algorithm is called efficient if it runs in time polynomial in formula formula and formula we say a concept class formula is occam learnable with respect to a hypothesis class formula if there exists an efficient occam algorithm for formula using formulaoccam learnability implies pac learnability as the following theorem of blumer et al
showslet formula be an efficient formulaoccam algorithm for formula using formula
then there exists a constant formula such that for any formula for any distribution formula given formula samples drawn from formula and labelled according to a concept formula of length formula bits each the algorithm formula will output a hypothesis formula such that formula with probability at least formula
here formula is with respect to the concept formula and distribution formula
this implies that the algorithm formula is also a pac learner for the concept class formula using hypothesis class formula
a slightly more general formulation is as followslet formula
let formula be an algorithm such that given formula samples drawn from a fixed but unknown distribution formula and labeled according to a concept formula of length formula bits each outputs a hypothesis formula that is consistent with the labeled samples
then there exists a constant formula such that if formula then formula is guaranteed to output a hypothesis formula such that formula with probability at least formula
while the above theorems show that occam learning is sufficient for pac learning it doesnt say anything about necessity
board and pitt show that for a wide variety of concept classes occam learning is in fact necessary for pac learning
they proved that for any concept class that is polynomially closed under exception lists pac learnability implies the existence of an occam algorithm for that concept class
concept classes that are polynomially closed under exception lists include boolean formulas circuits deterministic finite automata decisionlists decisiontrees and other geometricallydefined concept classes
a concept class formula is polynomially closed under exception lists if there exists a polynomialtime algorithm formula such that when given the representation of a concept formula and a finite list formula of exceptions outputs a representation of a concept formula such that the concepts formula and formula agree except on the set formula
we first prove the cardinality version
call a hypothesis formula bad if formula where again formula is with respect to the true concept formula and the underlying distribution formula
the probability that a set of samples formula is consistent with formula is at most formula by the independence of the samples
by the union bound the probability that there exists a bad hypothesis in formula is at most formula which is less than formula if formula
this concludes the proof of the second theorem above
using the second theorem we can prove the first theorem
since we have a formulaoccam algorithm this means that any hypothesis output by formula can be represented by at most formula bits and thus formula
this is less than formula if we set formula for some constant formula
thus by the cardinality version theorem formula will output a consistent hypothesis formula with probability at least formula
this concludes the proof of the first theorem above
though occam and pac learnability are equivalent the occam framework can be used to produce tighter bounds on the sample complexity of classical problems including conjunctions conjunctions with few relevant variables and decision lists
occam algorithms have also been shown to be successful for pac learning in the presence of errors probabilistic concepts function learning and markovian nonindependent examples
in predictive analytics and machine learning the concept drift means that the statistical properties of the target variable which the model is trying to predict change over time in unforeseen ways
this causes problems because the predictions become less accurate as time passes
the term concept refers to the quantity to be predicted
more generally it can also refer to other phenomena of interest besides the target concept such as an input but in the context of concept drift the term commonly refers to the target variable
in a fraud detection application the target concept may be a binary attribute fraudulent with values yes or no that indicates whether a given transaction is fraudulent
or in a weather prediction application there may be several target concepts such as temperature pressure and humidity
the behavior of the customers in an online shop may change over time
for example if weekly merchandise sales are to be predicted and a predictive model has been developed that works satisfactorily
the model may use inputs such as the amount of money spent on advertising promotions being run and other metrics that may affect sales
the model is likely to become less and less accurate over time  this is concept drift
in the merchandise sales application one reason for concept drift may be seasonality which means that shopping behavior changes seasonally
perhaps there will be higher sales in the winter holiday season than during the summer for example
to prevent deterioration in prediction accuracy because of concept drift both active and passive solutions can be adopted
active solutions rely on triggering mechanisms e
changedetection tests basseville and nikiforov  alippi and roveri  to explicitly detect concept drift as a change in the statistics of the datagenerating process
in stationary conditions any fresh information made available can be integrated to improve the model
differently when concept drift is detected the current model is no more uptodate and must be substituted with a new one to maintain the prediction accuracy gama et al
alippi et al
on the contrary in passive solutions the model is continuously updated e
by retraining the model on the most recently observed samples widmer and kubat  or enforcing an ensemble of classifiers elwell and polikar
contextual information when available can be used to better explain the causes of the concept drift for instance in the sales prediction application concept drift might be compensated by adding information about the season to the model
by providing information about the time of the year the rate of deterioration of your model is likely to decrease concept drift is unlikely to be eliminated altogether
this is because actual shopping behavior does not follow any static finite model
new factors may arise at any time that influence shopping behavior the influence of the known factors or their interactions may change
concept drift cannot be avoided for complex phenomena that are not governed by fixed laws of nature
all processes that arise from human activity such as socioeconomic processes and biological processes are likely to experience concept drift
therefore periodic retraining also known as refreshing of any model is necessary
announcements discussions job postings related to the topic of concept driftin data mining  machine learning
posts are moderated
to subscribe go to the group home page httpsgroups
google
comgroupconceptdriftmany papers have been published describing algorithms for concept drift detection
only reviews surveys and overviews are here
supervised learning is the machine learning task of learning a function that maps an input to an output based on example inputoutput pairs
it infers a function from  consisting of a set of training examples
in supervised learning each example is a pair consisting of an input object typically a vector and a desired output value also called the supervisory signal
a supervised learning algorithm analyzes the training data and produces an inferred function which can be used for mapping new examples
an optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances
this requires the learning algorithm to generalize from the training data to unseen situations in a reasonable way see inductive bias
the parallel task in human and animal psychology is often referred to as concept learning
in order to solve a given problem of supervised learning one has to perform the following stepsa wide range of supervised learning algorithms are available each with its strengths and weaknesses
there is no single learning algorithm that works best on all supervised learning problems see the no free lunch theorem
there are four major issues to consider in supervised learninga first issue is the tradeoff between bias and variance
imagine that we have available several different but equally good training data sets
a learning algorithm is biased for a particular input formula if when trained on each of these data sets it is systematically incorrect when predicting the correct output for formula
a learning algorithm has high variance for a particular input formula if it predicts different output values when trained on different training sets
the prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm
generally there is a tradeoff between bias and variance
a learning algorithm with low bias must be flexible so that it can fit the data well
but if the learning algorithm is too flexible it will fit each training data set differently and hence have high variance
a key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance either automatically or by providing a biasvariance parameter that the user can adjust
the second issue is the amount of training data available relative to the complexity of the true function classifier or regression function
if the true function is simple then an inflexible learning algorithm with high bias and low variance will be able to learn it from a small amount of data
but if the true function is highly complex e
because it involves complex interactions among many different input features and behaves differently in different parts of the input space then the function will only be learnable from a very large amount of training data and using a flexible learning algorithm with low bias and high variance
a third issue is the dimensionality of the input space
if the input feature vectors have very high dimension the learning problem can be difficult even if the true function only depends on a small number of those features
this is because the many extra dimensions can confuse the learning algorithm and cause it to have high variance
hence high input dimensionality typically requires tuning the classifier to have low variance and high bias
in practice if the engineer can manually remove irrelevant features from the input data this is likely to improve the accuracy of the learned function
in addition there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones
this is an instance of the more general strategy of dimensionality reduction which seeks to map the input data into a lowerdimensional space prior to running the supervised learning algorithm
a fourth issue is the degree of noise in the desired output values the supervisory target variables
if the desired output values are often incorrect because of human error or sensor errors then the learning algorithm should not attempt to find a function that exactly matches the training examples
attempting to fit the data too carefully leads to overfitting
you can overfit even when there are no measurement errors stochastic noise if the function you are trying to learn is too complex for your learning model
in such a situation the part of the target function that cannot be modeled corrupts your training data  this phenomenon has been called deterministic noise
when either type of noise is present it is better to go with a higher bias lower variance estimator
in practice there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm
there are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance
other factors to consider when choosing and applying a learning algorithm include the followingwhen considering a new application the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand see cross validation
tuning the performance of a learning algorithm can be very timeconsuming
given fixed resources it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms
the most widely used learning algorithms are given a set of formula training examples of the form formula such that formula is the feature vector of the ith example and formula is its label i
class a learning algorithm seeks a function formula where formula is the input space andformula is the output space
the function formula is an element of some space of possible functions formula usually called the hypothesis space
it is sometimes convenient torepresent formula using a scoring function formula such that formula is defined as returning the formula value that gives the highest score formula
let formula denote the space of scoring functions
although formula and formula can be any space of functions many learning algorithms are probabilistic models where formula takes the form of a conditional probability model formula or formula takes the form of a joint probability model formula
for example naive bayes and linear discriminant analysis are joint probability models whereas logistic regression is a conditional probability model
there are two basic approaches to choosing formula or formula empirical risk minimization and structural risk minimization
empirical risk minimization seeks the function that best fits the training data
structural risk minimization includes a penalty function that controls the biasvariance tradeoff
in both cases it is assumed that the training set consists of a sample of independent and identically distributed pairs formula
in order to measure how well a function fits the training data a loss function formula is defined
for training example formula the loss of predicting the value formula is formula
the risk formula of function formula is defined as the expected loss of formula
this can be estimated from the training data asin empirical risk minimization the supervised learning algorithm seeks the function formula that minimizes formula
hence a supervised learning algorithm can be constructed by applying an optimization algorithm to find formula
when formula is a conditional probability distribution formula and the loss function is the negative log likelihood formula then empirical risk minimization is equivalent to maximum likelihood estimation
when formula contains many candidate functions or the training set is not sufficiently large empirical risk minimization leads to high variance and poor generalization
the learning algorithm is ableto memorize the training examples without generalizing well
this is called overfitting
structural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization
the regularization penalty can be viewed as implementing a form of occams razor that prefers simpler functions over more complex ones
a wide variety of penalties have been employed that correspond to different definitions of complexity
for example consider the case where the function formula is a linear function of the forma popular regularization penalty is formula which is the squared euclidean norm of the weights also known as the formula norm
other norms include the formula norm formula and the formula norm which is the number of nonzero formulas
the penalty will be denoted by formula
the supervised learning optimization problem is to find the function formula that minimizesthe parameter formula controls the biasvariance tradeoff
when formula this gives empirical risk minimization with low bias and high variance
when formula is large the learning algorithm will have high bias and low variance
the value of formula can be chosen empirically via cross validation
the complexity penalty has a bayesian interpretation as the negative log prior probability of formula formula in which case formula is the posterior probabability of formula
the training methods described above are discriminative training methods because they seek to find a function formula that discriminates well between the different output values see discriminative model
for the special case where formula is a joint probability distribution and the loss function is the negative log likelihood formula a risk minimization algorithm is said to perform generative training because formula can be regarded as a generative model that explains how the data were generated
generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms
in some cases the solution can be computed in closed form as in naive bayes and linear discriminant analysis
there are several ways in which the standard supervised learning problem can be generalized
eclipse deeplearningj is a deep learning programming library written for java and the java virtual machine jvm and a computing framework with wide support for deep learning algorithms
deeplearningj includes implementations of the restricted boltzmann machine deep belief net deep autoencoder stacked denoising autoencoder and recursive neural tensor network wordvec docvec and glove
these algorithms all include distributed parallel versions that integrate with apache hadoop and spark
deeplearningj is opensource software released under apache license
developed mainly by a machine learning group headquartered in san francisco and tokyo and led by adam gibson
it is supported commercially by the startup skymind which bundles dlj tensorflow keras and other deep learning libraries in an enterprise distribution called the skymind intelligence layer
deeplearningj was contributed to the eclipse foundation in october
deeplearningj relies on the widely used programming language java though it is compatible with clojure and includes a scala application programming interface api
it is powered by its own opensource numerical computing library ndj and works with both central processing units cpus and graphics processing units gpus
deeplearningj has been used in several commercial and academic applications
the code is hosted on github
a support forum is maintained on gitter
the framework is composable meaning shallow neural nets such as restricted boltzmann machines convolutional nets autoencoders and recurrent nets can be added to one another to create deep nets of varying types
it also has extensive visualization tools and a computation graph
training with deeplearningj occurs in a cluster
neural nets are trained in parallel via iterative reduce which works on hadoopyarn and on spark
deeplearningj also integrates with cuda kernels to conduct pure gpu operations and works with distributed gpus
deeplearningj includes an ndimensional array class using ndj that allows scientific computing in java and scala similar to the functions that numpy provides to python
its effectively based on a library for linear algebra and matrix manipulation in a production environment
datavec vectorizes various file formats and data types using an inputoutput format system similar to hadoops use of mapreduce that is it turns various data types into columns of scalars termed vectors
datavec is designed to vectorize csvs images sound text video and time series
deeplearningj includes a vector space modeling and topic modeling toolkit implemented in java and integrating with parallel gpus for performance
it is designed to handle large text sets
deeplearningj includes implementations of term frequencyinverse document frequency tfidf deep learning and mikolovs wordvec algorithm docvec and glove reimplemented and optimized in java
it relies on tdistributed stochastic neighbor embedding tsne for wordcloud visualizations
realworld use cases for deeplearningj include network intrusion detection and cybersecurity fraud detection for the financial sector anomaly detection in industries such as manufacturing recommender systems in ecommerce and advertising and image recognition
deeplearningj has integrated with other machinelearning platforms such as rapidminer prediction
io and weka
deeplearningj serves machinelearning models for inference in production using the free developer edition of skil the skymind intelligence layer
a model server serves the parametric machinelearning models that makes decisions about data
it is used for the inference stage of a machinelearning workflow after data pipelines and model training
a model server is the tool that allows data science research to be deployed in a realworld production environment
what a web server is to the internet a model server is to ai
where a web server receives an http request and returns data about a web site a model server receives data and returns a decision or prediction about that data e
sent an image a model server might return a label for that image identifying faces or animals in photographs
the skil model server is able to import models from python frameworks such as tensorflow keras theano and cntk overcoming a major barrier in deploying deep learning models
deeplearningj is as fast as caffe for nontrivial image recognition tasks using multiple gpus
for programmers unfamiliar with hpc on the jvm there are several parameters that must be adjusted to optimize neural network training time
these include setting the heap space the garbage collection algorithm employing offheap memory and presaving data pickling for faster etl
together these optimizations can lead to a x acceleration in performance with deeplearningj
deeplearningj can be used via multiple api languages including java scala python and clojure
its scala api is called scalnet
keras serves as its python api
and its clojure wrapper is known as dlclj
the core languages performing the largescale mathematical operations necessary for deep learning are c c and cuda c
tensorflow keras and deeplearningj work together
deeplearningj can import models from tensorflow and other python frameworks if they have been created with keras
learning with errors lwe is a problem in machine learning that is conjectured to be hard to solve
introduced by oded regev in  who won the  gdel prize for this work it is a generalization of the parity learning problem
regev showed furthermore that the lwe problem is as hard to solve as several worstcase lattice problems
the lwe problem has recently been used as a hardness assumption to create publickey cryptosystems such as the ring learning with errors key exchange by peikert
an algorithm is said to solve the lwe problem if when given access to samples formula where formula a vector of formula integers modulo formula and formula with the assurance for some fixed linear function formula that formula with high probability and deviates from it according to some known noise model the algorithm can recreate formula or some close approximation of it with high probability
denote by formula the additive group on reals modulo one
denote by formula the distribution on formula obtained by choosing a vector formula uniformly at random choosing formula according to a probability distribution formula on formula and outputting formula for some fixed vector formula
here formula is the standard inner product formula the division is done in the field of reals or more formally this division by formula is notation for the group homomorphism formula mapping formula to formula and the final addition is in formula
the learning with errors problem formula is to find formula given access to polynomially many samples of choice from formula
for every formula denote by formula the onedimensional gaussian with density function formula where formula and let formula be the distribution on formula obtained by considering formula modulo one
the version of lwe considered in most of the results would be formulathe lwe problem described above is the search version of the problem
in the decision version dlwe the goal is to distinguish between noisy inner products and uniformly random samples from formula practically some discretized version of it
regev showed that the decision and search versions are equivalent when formula is a prime bounded by some polynomial in formula
intuitively if we have a procedure for the search problem the decision version can be solved easily just feed the input samples for the decision problem to the solver for the search problem
denote the given samples by formula
if the solver returns a candidate formula for all formula calculate formula
if the samples are from an lwe distribution then the results of this calculation will be distributed according formula but if the samples are uniformly random these quantities will be distributed uniformly as well
for the other direction given a solver for the decision problem the search version can be solved as follows recover formula one coordinate at a time
to obtain the first coordinate formula make a guess formula and do the following
choose a number formula uniformly at random
transform the given samples formula as follows
calculate formula
send the transformed samples to the decision solver
if the guess formula was correct the transformation takes the distribution formula to itself and otherwise since formula is prime it takes it to the uniform distribution
so given a polynomialtime solver for the decision problem that errs with very small probability since formula is bounded by some polynomial in formula it only takes polynomial time to guess every possible value for formula and use the solver to see which one is correct
after obtaining formula we follow an analogous procedure for each other coordinate formula
namely we transform our formula samples the same way and transform our formula samples by calculating formula where the formula is in the formula coordinate
peikert showed that this reduction with a small modification works for any formula that is a product of distinct small polynomial in formula primes
the main idea is if formula for each formula guess and check to see if formula is congruent to formula and then use the chinese remainder theorem to recover formula
regev showed the random selfreducibility of the lwe and dlwe problems for arbitrary formula and formula
given samples formula from formula it is easy to see that formula are samples from formula
so suppose there was some set formula such that formula and for distributions formula with formula dlwe was easy
then there would be some distinguisher formula who given samples formula could tell whether they were uniformly random or from formula
if we need to distinguish uniformly random samples from formula where formula is chosen uniformly at random from formula we could simply try different values formula sampled uniformly at random from formula calculate formula and feed these samples to formula
since formula comprises a large fraction of formula with high probability if we choose a polynomial number of values for formula we will find one such that formula and formula will successfully distinguish the samples
thus no such formula can exist meaning lwe and dlwe are up to a polynomial factor as hard in the average case as they are in the worst case
for a ndimensional lattice formula let smoothing parameter formula denote the smallest formula such that formula where formula is the dual of formula and formula is extended to sets by summing over function values at each element in the set
let formula denote the discrete gaussian distribution on formula of width formula for a lattice formula and real formula
the probability of each formula is proportional to formula
the discrete gaussian sampling problemdgs is defined as follows an instance of formula is given by an formuladimensional lattice formula and a number formula
the goal is to output a sample from formula
regev shows that there is a reduction from formula to formula for any function formula
regev then shows that there exists an efficient quantum algorithm for formula given access to an oracle for formula for integer formula and formula such that formula
this implies the hardness for formula
although the proof of this assertion works for any formula for creating a cryptosystem the formula has to be polynomial in formula
peikert proves that there is a probabilistic polynomial time reduction from the formula problem in the worst case to solving formula using formula samples for parameters formula formula formula and formula
the lwe problem serves as a versatile problem used in construction of several cryptosystems
in  regev showed that the decision version of lwe is hard assuming quantum hardness of the lattice problems formula for formula as above and formula with tnformula
in  peikert proved a similar result assuming only the classical hardness of the related problem formula
the disadvantage of peikerts result is that it bases itself on a nonstandard version of an easier when compared to sivp problem gapsvp
regev proposed a publickey cryptosystem based on the hardness of the lwe problem
the cryptosystem as well as the proof of security and correctness are completely classical
the system is characterized by formula and a probability distribution formula on formula
the setting of the parameters used in proofs of correctness and security isthe cryptosystem is then defined bythe proof of correctness follows from choice of parameters and some probability analysis
the proof of security is by reduction to the decision version of lwe an algorithm for distinguishing between encryptions with above parameters of formula and formula can be used to distinguish between formula and the uniform distribution over formulapeikert proposed a system that is secure even against any chosenciphertext attack
the idea of using lwe and ring lwe for key exchange was proposed and filed at the university of cincinnati in  by jintai ding
the idea comes from the associativity of matrix multiplications and the errors are used to provide the security
the paper appeared in  after a provisional patent application was filed in
the security of the protocol is proven based on the hardness of solving the lwe problem
in  peikert presented a keytransport scheme following the same basic idea of dings where the new idea of sending an additional bit signal for rounding in dings construction is also used
the new hope implementation selected for googles postquantum experiment uses peikerts scheme with variation in the error distribution
discriminative models also called conditional models are a class of models used in machine learning for modeling the dependence of unobserved target variables formula on observed variables formula
within a probabilistic framework this is done by modeling the conditional probability distribution formula which can be used for predicting formula from formula
discriminative models as opposed to generative models do not allow one to generate samples from the joint distribution of observed and target variables
however for tasks such as classification and regression that do not require the joint distribution discriminative models can yield superior performance in part because they have fewer variables to compute
on the other hand generative models are typically more flexible than discriminative models in expressing dependencies in complex learning tasks
in addition most discriminative models are inherently supervised and cannot easily support unsupervised learning
applicationspecific details ultimately dictate the suitability of selecting a discriminative versus generative model
examples of discriminative models used in machine learning include
in statistics and machine learning the biasvariance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples and vice versa
the biasvariance dilemma or problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training setthe biasvariance decomposition is a way of analyzing a learning algorithms expected generalization error with respect to a particular problem as a sum of three terms the bias variance and a quantity called the irreducible error resulting from noise in the problem itself
this tradeoff applies to all forms of supervised learning classification regression function fitting and structured output learning
it has also been invoked to explain the effectiveness of heuristics in human learning
the biasvariance tradeoff is a central problem in supervised learning
ideally one wants to choose a model that both accurately capture the regularities in its training data but also generalizes well to unseen data
unfortunately it is typically impossible to do both simultaneously
highvariance learning methods may be able to represent their training set well but are at risk of overfitting to noisy or unrepresentative training data
in contrast algorithms with high bias typically produce simpler models that dont tend to overfit but may underfit their training data failing to capture important regularities
models with low bias are usually more complex e
higherorder regression polynomials enabling them to represent the training set more accurately
in the process however they may also represent a large noise component in the training set making their predictions less accurate  despite their added complexity
in contrast models with higher bias tend to be relatively simple loworder or even linear regression polynomials but may produce lower variance predictions when applied beyond the training set
suppose that we have a training set consisting of a set of points formula and real values formula associated with each point formula
we assume that there is a function with noise formula where the noise formula has zero mean and variance formula
we want to find a function formula that approximates the true function formula as well as possible by means of some learning algorithm
we make as well as possible precise by measuring the mean squared error between formula and formula we want formula to be minimal both for formula and for points outside of our sample
of course we cannot hope to do so perfectly since the formula contain noise formula this means we must be prepared to accept an irreducible error in any function we come up with
finding an formula that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning
it turns out that whichever function formula we select we can decompose its expected error on an unseen sample formula as followswhereandthe expectation ranges over different choices of the training set formula all sampled from the same joint distribution formula
the three terms representthe more complex the model formula is the more data points it will capture and the lower the bias will be
however complexity will make the model move more to capture the data points and hence its variance will be larger
the derivation of the biasvariance decomposition for squared error proceeds as follows
for notational convenience abbreviate formula and formula
first recall that by definition for any random variable formula we haverearranging we getsince formula is deterministicthis given formula and formula implies formula
also since formulathus since formula and formula are independent we can writethe biasvariance decomposition forms the conceptual basis for regression regularization methods such as lasso and ridge regression
regularization methods introduce bias into the regression solution that can reduce variance considerably relative to the ols solution
although the ols solution provides nonbiased regression estimates the lower variance solutions produced by regularization techniques provide superior mse performance
the biasvariance decomposition was originally formulated for leastsquares regression
for the case of classification under the  loss misclassification rate it is possible to find a similar decomposition
alternatively if the classification problem can be phrased as probabilistic classification then the expected squared error of the predicted probabilities with respect to the true probabilities can be decomposed as before
dimensionality reduction and feature selection can decrease variance by simplifying models
similarly a larger training set tends to decrease variance
adding features predictors tends to decrease bias at the expense of introducing additional variance
learning algorithms typically have some tunable parameters that control bias and variance for exampleone way of resolving the tradeoff is to use mixture models and ensemble learning
for example boosting combines many weak high bias models in an ensemble that has lower bias than the individual models while bagging combines strong learners in a way that reduces their variance
in the case of nearest neighbors regression a closedform expression exists that relates the biasvariance decomposition to the parameter where formula are the nearest neighbors of in the training set
the bias first term is a monotone rising function of  while the variance second term drops off as is increased
in fact under reasonable assumptions the bias of the firstnearest neighbor nn estimator vanishes entirely as the size of the training set approaches infinity
while widely discussed in the context of machine learning the biasvariance dilemma has been examined in the context of human cognition most notably by gerd gigerenzer and coworkers in the context of learned heuristics
they have argued see references below that the human brain resolves the dilemma in the case of the typically sparse poorlycharacterised trainingsets provided by experience by adopting highbiaslow variance heuristics
this reflects the fact that a zerobias approach has poor generalisability to new situations and also unreasonably presumes precise knowledge of the true state of the world
the resulting heuristics are relatively simple but produce better inferences in a wider variety of situations
geman et al
argue that the biasvariance dilemma implies that abilities such as generic object recognition cannot be learned from scratch but require a certain degree of hard wiring that is later tuned by experience
this is because modelfree approaches to inference require impractically large training sets if they are to avoid high variance
in machine learning sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values
a common example of a sequence labeling task is part of speech tagging which seeks to assign a part of speech to each word in an input sentence or document
sequence labeling can be treated as a set of independent classification tasks one per member of the sequence
however accuracy is generally improved by making the optimal label for a given element dependent on the choices of nearby elements using special algorithms to choose the globally best set of labels for the entire sequence at once
as an example of why finding the globally best label sequence might produce better results than labeling one item at a time consider the partofspeech tagging task just described
frequently many words are members of multiple parts of speech and the correct label of such a word can often be deduced from the correct label of the word to the immediate left or right
for example the word sets can be either a noun or verb
in a phrase like he sets the books down the word he is unambiguously a pronoun and the unambiguously a determiner and using either of these labels sets can be deduced to be a verb since nouns very rarely follow pronouns and are less likely to precede determiners than verbs are
but in other cases only one of the adjacent words is similarly helpful
in he sets and then knocks over the table only the word he to the left is helpful cf
picks up the sets and then knocks over
conversely in
and also sets the table only the word the to the right is helpful cf
and also sets of books were
an algorithm that proceeds from left to right labeling one word at a time can only use the tags of leftadjacent words and might fail in the second example above vice versa for an algorithm that proceeds from right to left
most sequence labeling algorithms are probabilistic in nature relying on statistical inference to find the best sequence
the most common statistical models in use for sequence labeling make a markov assumption i
that the choice of label for a particular word is directly dependent only on the immediately adjacent labels hence the set of labels forms a markov chain
this leads naturally to the hidden markov model hmm one of the most common statistical models used for sequence labeling
other common models in use are the maximum entropy markov model and conditional random field
in statistical learning theory a representer theorem is any of several related results stating that a minimizer formula of a regularized empirical risk function defined over a reproducing kernel hilbert space can be represented as a finite linear combination of kernel products evaluated on the input points in the training set data
the following representer theorem and its proof are due to schlkopf herbrich and smolatheorem let formula be a nonempty set and formula a positivedefinite realvalued kernel on formula with corresponding reproducing kernel hilbert space formula
given a training sample formula a strictly monotonically increasing realvalued function formula and an arbitrary empirical risk function formula then for any formula satisfyingformula admits a representation of the formwhere formula for all formula
proofdefine a mappingso that formula is itself a map formula
since formula is a reproducing kernel thenwhere formula is the inner product on formula
given any formula one can use orthogonal projection to decompose any formula into a sum of two functions one lying in formula and the other lying in the orthogonal complementwhere formula for all formula
the above orthogonal decomposition and the reproducing property together show that applying formula to any training point formula produceswhich we observe is independent of formula
consequently the value of the empirical risk formula in  is likewise independent of formula
for the second term the regularization term since formula is orthogonal to formula and formula is strictly monotonic we havetherefore setting formula does not affect the first term of  while it strictly decreasing the second term
consequently any minimizer formula in  must have formula i
it must be of the formwhich is the desired result
the theorem stated above is a particular example of a family of results that are collectively referred to as representer theorems here we describe several such
the first statement of a representer theorem was due to kimeldorf and wahba for the special case in whichfor formula
schlkopf herbrich and smola generalized this result by relaxing the assumption of the squaredloss cost and allowing the regularizer to be any strictly monotonically increasing function formula of the hilbert space norm
it is possible to generalize further by augmenting the regularized empirical risk function through the addition of unpenalized offset terms
for example schlkopf herbrich and smola also consider the minimizationi
we consider functions of the form formula where formula and formula is an unpenalized function lying in the span of a finite set of realvalued functions formula
under the assumption that the formula matrix formula has rank formula they show that the minimizer formula in formulaadmits a representation of the formwhere formula and the formula are all uniquely determined
the conditions under which a representer theorem exists were investigated by argyriou miccheli and pontil who proved the followingtheorem let formula be a nonempty set formula a positivedefinite realvalued kernel on formula with corresponding reproducing kernel hilbert space formula and let formula be a differentiable regularization function
then given a training sample formula and an arbitrary empirical risk function formula a minimizerof the regularized empirical risk minimization problem admits a representation of the formwhere formula for all formula if and only if there exists a nondecreasing function formula for whicheffectively this result provides a necessary and sufficient condition on a differentiable regularizer formula under which the corresponding regularized empirical risk minimization formula will have a representer theorem
in particular this shows that a broad class of regularized risk minimizations much broader than those originally considered by kimeldorf and wahba have representer theorems
representer theorems are useful from a practical standpoint because they dramatically simplify the regularized empirical risk minimization problem formula
in most interesting applications the search domain formula for the minimization will be an infinitedimensional subspace of formula and therefore the search as written does not admit implementation on finitememory and finiteprecision computers
in contrast the representation of formula afforded by a representer theorem reduces the original infinitedimensional minimization problem to a search for the optimal formuladimensional vector of coefficients formula formula can then be obtained by applying any standard function minimization algorithm
consequently representer theorems provide the theoretical basis for the reduction of the general machine learning problem to algorithms that can actually be implemented on computers in practice
the sample complexity of a machine learning algorithm represents the number of trainingsamples that it needs in order to successfully learn a target function
more precisely the sample complexity is the number of trainingsamples that we need to supply to the algorithm so that the function returned by the algorithm is within an arbitrarily small error of the best possible function with probability arbitrarily close to
there are two variants of sample complexitythe no free lunch theorem discussed below proves that in general the strong sample complexity is infinite i
that there is no algorithm that can learn the globallyoptimal target function using a finite number of training samples
however if we are only interested in a particular class of target functions e
g only linear functions then the sample complexity is finite and it depends linearly on the vc dimension on the class of target functions
let formula be a space which we call the input space and formula be a space which we call the output space and let formula denote the product formula
for example in the setting of binary classification formula is typically a finitedimensional vector space and formula is the set formula
fix a hypothesis space formula of functions formula
a learning algorithm over formula is a computable map from formula to formula
in other words it is an algorithm that takes as input a finite sequence of training samples and outputs a function from formula to formula
typical learning algorithms include empirical risk minimization without or with tikhonov regularization
fix a loss function formula for example the square loss formula
for a given distribution formula on formula the expected risk of a hypothesis a function formula isin our setting we have formula where formula is a learning algorithm and formula is a sequence of vectors which are all drawn independently from formula
define the optimal riskformulaset formula for each formula
note that formula is a random variable and depends on the random variable formula which is drawn from the distribution formula
the algorithm formula is called consistent if formula probabilistically converges to formula in other words for all     there exists a positive integer n such that for all n  n we haveformulathe sample complexity of formula is then the minimum n for which this holds as a function of   and
we write the sample complexity as formula to emphasize that this value of n depends on   and
if formula is not consistent then we set formula
if there exists an algorithm for which formula is finite then we say that the hypothesis space formula is learnable
in words the sample complexity formula defines the rate of consistency of the algorithm given a desired accuracy  and confidence  one needs to sample formula data points to guarantee that the risk of the output function is within  of the best possible with probability at least
in probabilistically approximately correct pac learning one is concerned with whether the sample complexity is polynomial that is whether formula is bounded by a polynomial in  and
if formula is polynomial for some learning algorithm then one says that the hypothesis space formula is paclearnable
note that this is a stronger notion than being learnable
one can ask whether there exists a learning algorithm so that the sample complexity is finite in the strong sense that is there is a bound on the number of samples needed so that the algorithm can learn any distribution over the inputoutput space with a specified target error
more formally one asks whether there exists a learning algorithm formula such that for all     there exists a positive integer n such that for all n  n we haveformulawhere formula with formula as above
the no free lunch theorem says that without restrictions on the hypothesis space formula this is not the case i
there always exist bad distributions for which the sample complexity is arbitrarily large
thus in order to make statements about the rate of convergence of the quantityformulaone must eitherthe latter approach leads to concepts such as vc dimension and rademacher complexity which control the complexity of the space formula
a smaller hypothesis space introduces more bias into the inference process meaning that formula may be greater than the best possible risk in a larger space
however by restricting the complexity of the hypothesis space it becomes possible for an algorithm to produce more uniformly consistent functions
this tradeoff leads to the concept of regularization
it is a theorem from vc theory that the following three statements are equivalent for a hypothesis space formulathis gives a way to prove that certain hypothesis spaces are pac learnable and by extension learnable
let x  r y    and let formula be the space of affine functions on x that is functions of the form formula for some formula
this is the linear classification with offset learning problem
now note that four coplanar points in a square cannot be shattered by any affine function since no affine function can be positive on two diagonally opposite vertices and negative on the remaining two
thus the vc dimension of formula is formula in particular finite
it follows by the above characterization of paclearnable classes that formula is paclearnable and by extension learnable
suppose formula is a class of binary functions functions to
then formula is formulapaclearnable with a sample of sizeformulawhere formula is the vc dimension of formula
moreover any formulapaclearning algorithm for formula must have samplecomplexityformulathus the samplecomplexity is a linear function of the vc dimension of the hypothesis space
suppose formula is a class of realvalued functions with range in t
then formula is formulapaclearnable with a sample of sizeformulawhere formula is pollards pseudodimension of formula
in addition to the supervised learning setting sample complexity is relevant to semisupervised learning problems including active learning where the algorithm can ask for labels to specifically chosen inputs in order to reduce the cost of obtaining many labels
the concept of sample complexity also shows up in reinforcement learning online learning and unsupervised algorithms e
for dictionary learning
transfer learning or inductive transfer is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem
for example knowledge gained while learning to recognize cars could apply when trying to recognize trucks
this area of research bears some relation to the long history of psychological literature on transfer of learning although formal ties between the two fields are limited
the earliest cited work on transfer in machine learning is attributed to lorien pratt who formulated the discriminabilitybased transfer dbt algorithm in
in  the journal machine learning published a special issue devoted to transfer learning and by  the field had advanced to include multitask learning along with a more formal analysis of its theoretical foundations
learning to learn edited by pratt and sebastian thrun is a  review of the subject
transfer learning has also been applied in cognitive science with the journal connection sciencepublishing a special issue on reuse of neural networks through transfer in
notably scientists have developed algorithms for transfer learning in markov logic networks and bayesian networks
researchers have also applied techniques for transfer to problems in building utilization text classification and spam filtering
qloo pronounced clue is a company that uses artificial intelligence ai to understand taste and cultural correlations
it provides companies with an application programming interface api
it received funding from leonardo dicaprio elton john barry sternlicht pierre lagrange and others
qloo establishes consumer preference correlations via machine learning across data spanning cultural domains including music film television dining nightlife fashion books and travel
the recommender system uses ai to predict correlations for further applications
qloo was founded in  by chief executive officer alex elias and chief operating officer jay alger
qloo was tested on a private website in april
in  qloo raised
million in seed funding from investors including cedric the entertainer danny masterson and venture capital firm kindler capital
qloo had a public beta release in november  after its initial funding
in  the company raised an additional
million from cross creek pictures founding partner tommy thompson and samih toukan and hussam khoury founders of maktoob an internet services company purchased by yahoo for  million in
on november   a website and an iphone app were announced
the company later released an android app and tablet versions in mid
in  qloo secured
million in venture capital investment
the
million was split between a number of investors including barry sternlicht pierre lagrange and leonardo dicaprio
in july  qloo raised
million in funding rounds from axa strategic ventures and elton john
following the investment the founders stated in an interview with tech crunch that they would use the investment to expand qloos database
they hoped the move would secure larger contracts with corporate clients
at the time clients already included fortune  companies such as twitter pepsico and bmw
qloo calls itself a cultural ai platform to provide realtime correlation data across domains of culture and entertainment including film music television dining nightlife fashion books and travel
each category contains subcategories
qloos knowledge of a users taste in one category can be utilized to offer suggestions in other categories
users then rate the suggestions providing it with feedback for future suggestions
qloo has partnerships with companies such as expedia and itunes
domain adaptation is a field associated with machine learning and transfer learning
this scenario arises when we aim at learning from a source data distribution a well performing model on a different but related target data distribution
for instance one of the tasks of the common spam filtering problem consists in adapting a model from one user the source distribution to a new one who receives significantly different emails the target distribution
note that when more than one source distribution is available the problem is referred to as multisource domain adaptation
let formula be the input space or description space and let formula be the output space or label space
the objective of a machine learning algorithm is to learn a mathematical model a hypothesis formula able to affect a label of formula to an example from formula
this model is learned from a learning sample formula
usually in supervised learning without domain adaptation we suppose that the examples formula are drawn i
from a distribution formula of support formula unknown and fixed
the objective is then to learn formula from formula such that it commits the least error as possible for labelling new examples coming from the distribution formula
the main difference between supervised learning and domain adaptation is that in the latter situation we study two different but related distributions formula and formula on formula
the domain adaptation task then consists of the transfer of knowledge from the source domain formula to the target one formula
the goal is then to learn formula from labeled or unlabelled samples coming from the two domains such that it commits as little error as possible on the target domain formula
the major issue is the following if a model is learned from a source domain what is its capacity to correctly label data coming from the target domainthere are several contexts of domain adaptation
they differ in the informations considered for the target task
the objective is to reweight the source labeled sample such that it looks like the target sample in term of the error measure considereda method for adapting consists in iteratively autolabeling the target examples
the principle is simplenote that there exists other iterative approaches but they usually need target labeled examples
the goal is to find or construct a common representation space for the two domains
the objective is to obtain a space in which the domains are close to each other while keeping good performances on the source labeling task
this can be achieved through the use of adversarial machine learning techniques where feature representations from samples in different domains are encouraged to be indistinguishable
paraphrase or paraphrasing in computational linguistics is the natural language processing task of detecting and generating paraphrases
applications of paraphrasing are varied including information retrieval question answering text summarization and plagiarism detection
paraphrasing is also useful in the evaluation of machine translation as well as generation of new samples to expand existing corpora
barzilay and lee proposed a method to generate paraphrases through the usage of monolingual parallel corpora namely news articles covering the same event on the same day
training consists of using multisequence alignment to generate sentencelevel paraphrases from an unannotated corpus
this is done bythis is achieved by first clustering similar sentences together using ngram overlap
recurring patterns are found within clusters by using multisequence alignment
then the position of argument words are determined by finding areas of high variability within each clusters aka between words shared by more than  of a clusters sentences
pairings between patterns are then found by comparing similar variable words between different corpora
finally new paraphrases can be generated by choosing a matching cluster for a source sentence then substituting the source sentences argument into any number of patterns in the cluster
paraphrase can also be generated through the use of phrasebased translation as proposed by bannard and callisonburch
the chief concept consists of aligning phrases in a pivot language to produce potential paraphrases in the original language
for example the phrase under control in an english sentence is aligned with the phrase unter kontrolle in its german counterpart
the phrase unter kontrolle is then found in another german sentence with the aligned english phrase being in check a paraphrase of under control
the probability distribution can be modeled as formula the probability phrase formula is a paraphrase of formula which is equivalent to formula summed over all formula a potential phrase translation in the pivot language
additionally the sentence formula is added as a prior to add context to the paraphrase
thus the optimal paraphrase formula can be modeled asformula and formula can be approximated by simply taking their frequencies
adding formula as a prior is modeled by calculating the probability of forming the formula when formula is substituted with there has been success in using long shortterm memory lstm models to generate paraphrases
in short the model consists of an encoder and decoder component both implemented using variations of a stacked residual lstm
first the encoding lstm takes a onehot encoding of all the words in a sentence as input and produces a final hidden vector which can be viewed as a representation of the input sentence
the decoding lstm then takes the hidden vector as input and generates new sentence terminating in an endofsentence token
the encoder and decoder are trained to take a phrase and reproduce the onehot distribution of a corresponding paraphrase by minimizing perplexity using simple stochastic gradient descent
new paraphrases are generated by inputting a new phrase to the encoder and passing the output to the decoder
paraphrase recognition has been attempted by socher et al through the use of recursive autoencoders
the main concept is to produce a vector representation of a sentence along with its components through recursively using an autoencoder
the vector representations of paraphrases should have similar vector representations they are processed then fed as input into a neural network for classification
given a sentence formula with formula words the autoencoder is designed to take  formuladimensional word embeddings as input and produce an formuladimensional vector as output
the same autoencoder is applied to every pair of words in formula to produce formula vectors
the autoencoder is then applied recursively with the new vectors as inputs until a single vector is produced
given an odd number of inputs the first vector is forwarded as is to the next level of recursion
the autoencoder is then trained to reproduce every vector in the full recursion tree including the initial word embeddings
given two sentences formula and formula of length  and  respectively the autoencoders would produce  and  vector representations including the initial word embeddings
the euclidean distance is then taken between every combination of vectors in formula and formula to produce a similarity matrix formula
formula is then subject to a dynamic minpooling layer to produce a fixed size formula matrix
since formula are not uniform in size among all potential sentences formula is split into formula roughly even sections
the output is then normalized to have mean  and standard deviation  and is fed into a fully connected layer with a softmax output
the dynamic pooling to softmax model is trained using pairs of known paraphrases
skipthought vectors are an attempt to create a vector representation of the semantic meaning of a sentence in a similar fashion as the skip gram model
skipthought vectors are produced through the use of a skipthought model which consists of three key components an encoder and two decoders
given a corpus of documents the skipthought model is trained to take a sentence as input and encode it into a skipthought vector
the skipthought vector is used as input for both decoders one of which attempts to reproduce the previous sentence and the other the following sentence in its entirety
the encoder and decoder can be implemented through the use of a recursive neural network rnn or an lstm
since paraphrases carry the same semantic meaning between one another they should have similar skipthought vectors
thus a simple logistic regression can be trained to a good performance with the absolute difference and componentwise product of two skipthought vectors as input
there are multiple methods that can be used to evaluate paraphrases
since paraphrase recognition can be posed as a classification problem most standard evaluations metrics such as accuracy f score or an roc curve do relatively well
however there is difficulty calculating fscores due to trouble produce a complete list of paraphrases for a given phrase along with the fact that good paraphrases are dependent upon context
a metric designed to counter these problems is parametric
parametric aims to calculate the precision and recall of an automatic paraphrase system by comparing the automatic alignment of paraphrases to a manual alignment of similar phrases
since parametric is simply rating the quality of phrase alignment it can be used to rate paraphrase generation systems as well assuming it uses phrase alignment as part of its generation process
a noted drawback to parametric is the large and exhaustive set of manual alignments that must be initially created before a rating can be produced
the evaluation of paraphrase generation has similar difficulties as the evaluation of machine translation
often the quality of a paraphrase is dependent upon its context whether it is being used as a summary and how it is generated among other factors
additionally a good paraphrase usually is lexically dissimilar from its source phrase
the simplest method used to evaluate paraphrase generation would be through the use of human judges
unfortunately evaluation through human judges tends to be time consuming
automated approaches to evaluation prove to be challenging as it is essentially a problem as difficult as paraphrase recognition
while originally used to evaluate machine translations bilingual evaluation understudy bleu has been used successfully to evaluate paraphrase generation models as well
however paraphrases often have several lexically different but equally valid solutions which hurts bleu and other similar evaluation metrics
metrics specifically designed to evaluate paraphrase generation include paraphrase in ngram change pinc and paraphrase evaluation metric pem along with the aforementioned parametric
pinc is designed to be used in conjunction with bleu and help cover its inadequacies
since bleu has difficulty measuring lexical dissimilarity pinc is a measurement of the lack of ngram overlap between a source sentence and a candidate paraphrase
it is essentially the jaccard distance between the sentence excluding ngrams that appear in the source sentence to maintain some semantic equivalence
pem on the other hand attempts to evaluate the adequacy fluency and lexical dissimilarity of paraphrases by returning a single value heuristic calculated using ngrams overlap in a pivot language
however a large drawback to pem is that must be trained using a large indomain parallel corpora as well as human judges
in other words it is tantamount to training a paraphrase recognition system in order to evaluate a paraphrase generation system
binary or binomial classification is the task of classifying the elements of a given set into two groups predicting which group each one belongs to on the basis of a classification rule
contexts requiring a decision as to whether or not an item has some qualitative property some specified characteristic or some typical binary classification includebinary classification is dichotomization applied to practical purposes and in many practical binary classification problems the two groups are not symmetric  rather than overall accuracy the relative proportion of different types of errors is of interest
for example in medical testing a false positive detecting a disease when it is not present is considered differently from a false negative not detecting a disease when it is present
statistical classification is a problem studied in machine learning
it is a type of supervised learning a method of machine learning where the categories are predefined and is used to categorize new probabilistic observations into said categories
when there are only two categories the problem is known as statistical binary classification
some of the methods commonly used for binary classification areeach classifier is best in only a select domain based upon the number of observations the dimensionality of the feature vector the noise in the data and many other factors
for example random forests perform better than svm classifiers for d point clouds
there are many metrics that can be used to measure the performance of a classifier or predictor different fields have different preferences for specific metrics due to different goals
for example in medicine sensitivity and specificity are often used while in information retrieval precision and recall are preferred
an important distinction is between metrics that are independent on the prevalence how often each category occurs in the population and metrics that depend on the prevalence  both types are useful but they have very different properties
given a classification of a specific data set there are four basic combinations of actual data category and assigned category true positives tp correct positive assignments true negatives tn correct negative assignments false positives fp incorrect positive assignments and false negatives fn incorrect negative assignments
these can be arranged into a  contingency table with columns corresponding to actual value  condition positive cp or condition negative cn  and rows corresponding to classification value  test outcome positive or test outcome negative
there are eight basic ratios that one can compute from this table which come in four complementary pairs each pair summing to
these are obtained by dividing each of the four numbers by the sum of its row or column yielding eight numbers which can be referred to generically in the form true positive row ratio or false negative column ratio though there are conventional terms
there are thus two pairs of column ratios and two pairs of row ratios and one can summarize these with four numbers by choosing one ratio from each pair  the other four numbers are the complements
the column ratios are true positive rate tpr aka sensitivity or recall with complement the false negative rate fnr and true negative rate tnr aka specificity spc with complement false positive rate fpr
these are the proportion of the population with the condition resp
without the condition for which the test is correct or complementarily for which the test is incorrect these are independent of prevalence
the row ratios are positive predictive value ppv aka precision with complement the false discovery rate fdr and negative predictive value npv with complement the false omission rate for
these are the proportion of the population with a given test result for which the test is correct or complementarily for which the test is incorrect these depend on prevalence
in diagnostic testing the main ratios used are the true column ratios  true positive rate and true negative rate  where they are known as sensitivity and specificity
in informational retrieval the main ratios are the true positive ratios row and column  positive predictive value and true positive rate  where they are known as precision and recall
one can take ratios of a complementary pair of ratios yielding four likelihood ratios two column ratio of ratios two row ratio of ratios
this is primarily done for the column condition ratios yielding likelihood ratios in diagnostic testing
taking the ratio of one of these groups of ratios yields a final ratio the diagnostic odds ratio dor
this can also be defined directly as tptnfpfn  tpfnfptn this has a useful interpretation  as an odds ratio  and is prevalenceindependent
there are a number of other metrics most simply the accuracy or fraction correct fc which measures the fraction of all instances that are correctly categorized the complement is the fraction incorrect fic
the fscore combines precision and recall into one number via a choice of weighing most simply equal weighing as the balanced fscore f score
some metrics come from regression coefficients the markedness and the informedness and their geometric mean the matthews correlation coefficient
other metrics include youdens j statistic the uncertainty coefficient the phi coefficient and cohens kappa
tests whose results are of continuous values such as most blood values can artificially be made binary by defining a cutoff value with test results being designated as positive or negative depending on whether the resultant value is higher or lower than the cutoff
however such conversion causes a loss of information as the resultant binary classification does not tell how much above or below the cutoff a value is
as a result when converting a continuous value that is close to the cutoff to a binary one the resultant positive or negative predictive value is generally higher than the predictive value given directly from the continuous value
in such cases the designation of the test of being either positive or negative gives the appearance of an inappropriately high certainty while the value is in fact in an interval of uncertainty
for example with the urine concentration of hcg as a continuous value a urine pregnancy test that measured  miuml of hcg may show as positive with  miuml as cutoff but is in fact in an interval of uncertainty which may be apparent only by knowing the original continuous value
on the other hand a test result very far from the cutoff generally has a resultant positive or negative predictive value that is lower than the predictive value given from the continuous value
for example a urine hcg value of  miuml confers a very high probability of pregnancy but conversion to binary values results in that it shows just as positive as the one of  miuml
in computer science computational learning theory or just learning theory is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms
theoretical results in machine learning mainly deal with a type of inductive learning called supervised learning
in supervisedlearning an algorithm is given samples that are labeled in some useful way
for example the samples might be descriptions ofmushrooms and the labels could be whether or not the mushrooms are edible
the algorithm takes these previously labeled samples anduses them to induce a classifier
this classifier is a function that assigns labels to samples including samples that have never beenpreviously seen by the algorithm
the goal of the supervised learning algorithm is to optimize some measure of performance such asminimizing the number of mistakes made on new samples
in addition to performance bounds computational learning theory studies the time complexity and feasibility of learning
incomputational learning theory a computation is considered feasible if it can be done in polynomial time
there are two kinds of timecomplexity resultsnegative results often rely on commonly believed but yet unproven assumptions such asthere are several different approaches to computational learning theory
these differences are based on making assumptions about theinference principles used to generalize from limited data
this includes different definitions of probability see frequency probability bayesian probability and different assumptions on the generation of samples
the different approaches include computational learning theory has led to several practical algorithms
for example pac theory inspired boosting vc theory led to support vector machines and bayesian inference led to belief networks by judea pearl
a description of some of these publications is given at important publications in machine learning
inductive probability attempts to give the probability of future events based on past events
it is the basis for inductive reasoning and gives the mathematical basis for learning and the perception of patterns
it is a source of knowledge about the world
there are three sources of knowledge inference communication and deduction
communication relays information found using other methods
deduction establishes new facts based on existing facts
only inference establishes new facts from data
the basis of inference is bayes theorem
but this theorem is sometimes hard to apply and understand
the simpler method to understand inference is in terms of quantities of information
information describing the world is written in a language
for example a simple mathematical language of propositions may be chosen
sentences may be written down in this language as strings of characters
but in the computer it is possible to encode these sentences as strings of bits s and s
then the language may be encoded so that the most commonly used sentences are the shortest
this internal language implicitly represents probabilities of statements
occams razor says the simplest theory consistent with the data is most likely to be correct
the simplest theory is interpreted as the representation of the theory written in this internal language
the theory with the shortest encoding in this internal language is most likely to be correct
probability and statistics was focused on probability distributions and tests of significance
probability was formal well defined but limited in scope
in particular its application was limited to situations that could be defined as an experiment or trial with a well defined population
bayess theorem is named after rev
thomas bayes
bayesian inference broadened the application of probability to many situations where a population was not well defined
but bayes theorem always depended on prior probabilities to generate new probabilities
it was unclear where these prior probabilities should come from
ray solomonoff developed algorithmic probability which gave an explanation for what randomness is and how patterns in the data may be represented by computer programs that give shorter representations of the data circa
chris wallace and d
boulton developed minimum message length circa
later jorma rissanen developed the minimum description length circa
these methods allow information theory to be related to probability in a way that can be compared to the application of bayes theorem but which give a source and explanation for the role of prior probabilities
marcus hutter combined decision theory with the work of ray solomonoff and andrey kolmogorov to give a theory for the pareto optimal behavior for an intelligent agent circa
the program with the shortest length that matches the data is the most likely to predict future data
this is the thesis behind the minimum message length and minimum description length methods
at first sight bayes theorem appears different from the minimimum messagedescription length principle
at closer inspection it turns out to be the same
bayes theorem is about conditional probabilities
what is the probability that event b happens if firstly event a happensbecomes in terms of message length lwhat this means is that in describing an event if all the information is given describing the event then the length of the information may be used to give the raw probability of the event
so if the information describing the occurrence of a is given along with the information describing b given a then all the information describing a and b has been given
overfitting is where the model matches the random noise and not the pattern in the data
for example take the situation where a curve is fitted to a set of points
if polynomial with many terms is fitted then it can more closely represent the data
then the fit will be better and the information needed to describe the deviances from the fitted curve will be smaller
smaller information length means more probable
however the information needed to describe the curve must also be considered
the total information for a curve with many terms may be greater than for a curve with fewer terms that has not as good a fit but needs less information to describe the polynomial
solomonoffs theory of inductive inference is also inductive inference
a bit string x is observed
then consider all programs that generate strings starting with x
cast in the form of inductive inference the programs are theories that imply the observation of the bit string x
the method used here to give probabilities for inductive inference is based on solomonoffs theory of inductive inference
if all the bits are  then people infer that there is a bias in the coin and that it is more likely also that the next bit is  also
this is described as learning from or detecting a pattern in the data
such a pattern may be represented by a computer program
a short computer program may be written that produces a series of bits which are all
if the length of the program k is formula bits then its prior probability isthe length of the shortest program that represents the string of bits is called the kolmogorov complexity
kolmogorov complexity is not computable
this is related to the halting problem
when searching for the shortest program some programs may go into an infinite loop
the greek philosopher epicurus is quoted as saying if more than one theory is consistent with the observations keep all theories
as in a crime novel all theories must be considered in determining the likely murderer so with inductive probability all programs must be considered in determining the likely future bits arising from the stream of bits
programs that are already longer than n have no predictive power
the raw or prior probability that the pattern of bits is random has no pattern is formula
each program that produces the sequence of bits but is shorter than the n is a theorypattern about the bits with a probability of formula where k is the length of the program
the probability of receiving a sequence of bits y after receiving a series of bits x is then the conditional probability of receiving y given x which is the probability of x with y appended divided by the probability of x
the programming language affects the predictions of the next bit in the string
the language acts as a prior probability
this is particularly a problem where the programming language codes for numbers and other data types
intuitively we think that  and  are simple numbers and that prime numbers are somehow more complex than numbers that may be composite
using the kolmogorov complexity gives an unbiased estimate a universal prior of the prior probability of a number
as a thought experiment an intelligent agent may be fitted with a data input device giving a series of numbers after applying some transformation function to the raw numbers
another agent might have the same input device with a different transformation function
the agents do not see or know about these transformation functions
then there appears no rational basis for preferring one function over another
a universal prior insures that although two agents may have different initial probability distributions for the data input the difference will be bounded by a constant
so universal priors do not eliminate an initial bias but they reduce and limit it
whenever we describe an event in a language either using a natural language or other the language has encoded in it our prior expectations
so some reliance on prior probabilities are inevitable
a problem arises where an intelligent agents prior expectations interact with the environment to form a self reinforcing feed back loop
this is the problem of bias or prejudice
universal priors reduce but do not eliminate this problem
the theory of universal artificial intelligence applies decision theory to inductive probabilities
the theory shows how the best actions to optimize a reward function may be chosen
the result is a theoretical model of intelligence
it is a fundamental theory of intelligence which optimizes the agents behavior inin general no agent will always provide the best actions in all situations
a particular choice made by an agent may be wrong and the environment may provide no way for the agent to recover from an initial bad choice
however the agent is pareto optimal in the sense that no other agent will do better than this agent in this environment without doing worse in another environment
no other agent may in this sense be said to be better
at present the theory is limited by incomputability the halting problem
approximations may be used to avoid this
processing speed and combinatorial explosion remain the primary limiting factors for artificial intelligence
probability is the representation of uncertain or partial knowledge about the truth of statements
probabilities are subjective and personal estimates of likely outcomes based on past experience and inferences made from the data
this description of probability may seem strange at first
in natural language we refer to the probability that the sun will rise tomorrow
we do not refer to your probability that the sun will rise
but in order for inference to be correctly modeled probability must be personal and the act of inference generates new posterior probabilities from prior probabilities
probabilities are personal because they are conditional on the knowledge of the individual
probabilities are subjective because they always depend to some extent on prior probabilities assigned by the individual
subjective should not be taken here to mean vague or undefined
the term intelligent agent is used to refer to the holder of the probabilities
the intelligent agent may be a human or a machine
if the intelligent agent does not interact with the environment then the probability will converge over time to the frequency of the event
if however the agent uses the probability to interact with the environment there may be a feedback so that two agents in the identical environment starting with only slightly different priors end up with completely different probabilities
in this case optimal decision theory as in marcus hutters universal artificial intelligence will give pareto optimal performance for the agent
this means that no other intelligent agent could do better in one environment without doing worse in another environment
in deductive probability theories probabilities are absolutes independent of the individual making the assessment
but deductive probabilities are based onfor example in a trial the participants are aware the outcome of all the previous history of trials
they also assume that each outcome is equally probable
together this allows a single unconditional value of probability to be defined
but in reality each individual does not have the same information
and in general the probability of each outcome is not equal
the dice may be loaded and this loading needs to be inferred from the data
the principle of indifference has played a key role in probability theory
it says that if n statements are symmetric so that one condition cannot be preferred over another then all statements are equally probable
taken seriously in evaluating probability this principle leads to contradictions
suppose there are  bags of gold in the distance and one is asked to select one
then because of the distance one cannot see the bag sizes
you estimate using the principle of indifference that each bag has equal amounts of gold and each bag has one third of the gold
now while one of us is not looking the other takes one of the bags and divide it into  bags
now there are  bags of gold
the principle of indifference now says each bag has one fifth of the gold
a bag that was estimated to have one third of the gold is now estimated to have one fifth of the gold
taken as a value associated with the bag the values are different therefore contradictory
but taken as an estimate given under a particular scenario both values are separate estimates given under different circumstances and there is no reason to believe they are equal
estimates of prior probabilities are particularly suspect
estimates will be constructed that do not follow any consistent frequency distribution
for this reason prior probabilities are considered as estimates of probabilities rather than probabilities
a full theoretical treatment would associate with each probabilityinductive probability combines two different approaches to probability
each approach gives a slightly different viewpoint
information theory is used in relating probabilities to quantities of information
this approach is often used in giving estimates of prior probabilities
frequentist probability defines probabilities as objective statements about how often an event occurs
this approach may be stretched by defining the trials to be over possible worlds
statements about possible worlds define events
whereas logic represents only two values true and false as the values of statement probability associates a number in  to each statement
if the probability of a statement is  the statement is false
if the probability of a statement is  the statement is true
in considering some data as a string of bits the prior probabilities for a sequence of s and s the probability of  and  is equal
therefore each extra bit halves the probability of a sequence of bits
this leads to the conclusion thatwhere formula is the probability of the string of bits formula and formula is its length
the prior probability of any statement is calculated from the number of bits needed to state it
see also information theory
two statements formula and formula may be represented by two separate encodings
then the length of the encoding isor in terms of probabilitybut this law is not always true because there may be a shorter method of encoding formula if we assume formula
so the above probability law applies only if formula and formula are independent
the primary use of the information approach to probability is to provide estimates of the complexity of statements
recall that occams razor states that all things being equal the simplest theory is the most likely to be correct
in order to apply this rule first there needs to be a definition of what simplest means
information theory defines simplest to mean having the shortest encoding
knowledge is represented as statements
each statement is a boolean expression
expressions are encoded by a function that takes a description as against the value of the expression and encodes it as a bit string
the length of the encoding of a statement gives an estimate of the probability of a statement
this probability estimate will often be used as the prior probability of a statement
technically this estimate is not a probability because it is not constructed from a frequency distribution
the probability estimates given by it do not always obey the law of total of probability
applying the law of total probability to various scenarios will usually give a more accurate probability estimate of the prior probability than the estimate from the length of the statement
an expression is constructed from sub expressionsa huffman code must distinguish the  cases
the length of each code is based on the frequency of each type of sub expressions
initially constants are all assigned the same lengthprobability
later constants may be assigned a probability using the huffman code based on the number of uses of the function id in all expressions recorded so far
in using a huffman code the goal is to estimate probabilities not to compress the data
the length of a function application is the length of the function identifier constant plus the sum of the sizes of the expressions for each parameter
the length of a quantifier is the length of the expression being quantified over
no explicit representation of natural numbers is given
however natural numbers may be constructed by applying the successor function to  and then applying other arithmetic functions
a distribution of natural numbers is implied by this based on the complexity of constructing each number
rational numbers are constructed by the division of natural numbers
the simplest representation has no common factors between the numerator and the denominator
this allows the probability distribution of natural numbers may be extended to rational numbers
the probability of an event may be interpreted as the frequencies of outcomes where the statement is true divided by the total number of outcomes
if the outcomes form a continuum the frequency may need to be replaced with a measure
events are sets of outcomes
statements may be related to events
a boolean statement b about outcomes defines a set of outcomes beach probability is always associated with the state of knowledge at a particular point in the argument
probabilities before an inference are known as prior probabilities and probabilities after are known as posterior probabilities
probability depends on the facts known
the truth of a fact limits the domain of outcomes to the outcomes consistent with the fact
prior probabilities are the probabilities before a fact is known
posterior probabilities are after a fact is known
the posterior probabilities are said to be conditional on the fact
the probability that formula is true given that formula is true is written as formulaall probabilities are in some sense conditional
the prior probability of formula isin the frequentist approach probabilities are defined as the ratio of the number of outcomes within an event to the total number of outcomes
in the possible world model each possible world is an outcome and statements about possible worlds define events
the probability of a statement being true is the number of possible worlds divided by the total number of worlds
the probability of a statement formula being true about possible worlds is thenfor a conditional probability
thenusing symmetry this equation may be written out as bayes law
this law describes the relationship between prior and posterior probabilities when new facts are learnt
written as quantities of information bayes theorem becomestwo statements a and b are said to be independent if knowing the truth of a does not change the probability of b
mathematically this isthen bayes theorem reduces tofor a set of mutually exclusive possibilities formula the sum of the posterior probabilities must be
substituting using bayes theorem gives the law of total probabilitythis result is used to give the extended form of bayes theoremthis is the usual form of bayes theorem used in practice because it guarantees the sum of all the posterior probabilities for formula is
for mutually exclusive possibilities the probabilities add
usingthen the alternativesare all mutually exclusive
alsoso putting it all togetherasthenimplication is related to conditional probability by the following equationderivationbayes theorem may be used to estimate the probability of a hypothesis or theory h given some facts f
the posterior probability of h is thenor in terms of informationby assuming the hypothesis is true a simpler representation of the statement f may be given
the length of the encoding of this simpler representation is formulaformula represents the amount of information needed to represent the facts f if h is true
formula is the amount of information needed to represent f without the hypothesis h
the difference is how much the representation of the facts has been compressed by assuming that h is true
this is the evidence that the hypothesis h is true
if formula is estimated from encoding length then the probability obtained will not be between  and
the value obtained is proportional to the probability without being a good probability estimate
the number obtained is sometimes referred to as a relative probability being how much more probable the theory is than not holding the theory
if a full set of mutually exclusive hypothesis that provide evidence is known a proper estimate may be given for the prior probability formula
probabilities may be calculated from the extended form of bayes theorem
given all mutually exclusive hypothesis formula which give evidence such thatand also the hypothesis r that none of the hypothesis is true thenin terms of informationin most situations it is a good approximation to assume that formula is independent of formula which means formula givingabductive inference starts with a set of facts f which is a statement boolean expression
abductive reasoning is of the formthe theory t also called an explanation of the condition f is an answer to the ubiquitous factual why question
for example for the condition f is why do apples fall
the answer is a theory t that implies that apples fallinductive inference is of the formin terms of abductive inference all objects in a class c or set have a property p is a theory that implies the observed condition all observed objects in a class c have a property p
so inductive inference is a special case of abductive inference
in common usage the term inductive inference is often used to refer to both abductive and inductive inference
inductive inference is related to generalization
generalizations may be formed from statements by replacing a specific value with membership of a category or by replacing membership of a category with membership of a broader category
in deductive logic generalization is a powerful method of generating new theories that may be true
in inductive inference generalization generates theories that have a probability of being true
the opposite of generalization is specialization
specialization is used in applying a general rule to a specific case
specializations are created from generalizations by replacing membership of a category by a specific value or by replacing a category with a sub category
the linnaen classification of living things and objects forms the basis for generalization and specification
the ability to identify recognize and classify is the basis for generalization
perceiving the world as a collection of objects appears to be a key aspect of human intelligence
it is the object oriented model in the non computer science sense
the object oriented model is constructed from our perception
in particularly vision is based on the ability to compare two images and calculate how much information is needed to morph or map one image into another
computer vision uses this mapping to construct d images from stereo image pairs
inductive logic programming is a means of constructing theory that implies a condition
plotkins relative least general generalization rlgg approach constructs the simplest generalization consistent with the condition
isaac newton used inductive arguments in constructing his law of universal gravitation
starting with the statementgeneralizing by replacing apple for object and earth for object gives in a two body systemthe theory explains all objects falling so there is strong evidence for it
the second observationafter some complicated mathematical calculus it can be seen that if the acceleration follows the inverse square law then objects will follow an ellipse
so induction gives evidence for the inverse square law
using galileos observation that all objects drop with the same speedwhere formula and formula vectors towards the center of the other object
then using newtons third law formulaimplication determines condition probability assothis result may be used in the probabilities given for bayesian hypothesis testing
for a single theory h  t andor in terms of information the relative probability isnote that this estimate for ptf is not a true probability
if formula then the theory has evidence to support it
then for a set of theories formula such that formulagivingmake a list of all the shortest programs formula that each produce a distinct infinite string of bits and satisfy the relationwhere formula is the result of running the program formula and formula truncates the string after n bits
the problem is to calculate the probability that the source is produced by program formula given that the truncated source after n bits is x
this is represented by the conditional probabilityusing the extended form of bayes theoremthe extended form relies on the law of total probability
this means that the formula must be distinct possibilities which is given by the condition that each formula produce a different infinite string
also one of the conditions formula must be true
this must be true as in the limit as formula there is always at least one program that produces formula
as formula are chosen so that formula thenthe apriori probability of the string being produced from the program given no information about the string is based on the size of the programgivingprograms that are the same or longer than the length of x provide no predictive power
separate them out givingthen identify the two probabilities asbut the prior probability that x is a random set of bits is formula
sothe probability that the source is random or unpredictable isa model of how worlds are constructed is used in determining the probabilities of theoriesif w is the bit string then the world is created such that formula is true
an intelligent agent has some facts about the word represented by the bit string c which gives the conditionthe set of bit strings identical with any condition x is formula
a theory is a simpler condition that explains or implies c
the set of all such theories is called textended form of bayes theorem may be appliedwhereto apply bayes theorem the following must hold formula is a partition of the event space
for formula to be a partition no bit string n may belong to two theories
to prove this assume they can and derive a contradictionsecondly prove that t includes all outcomes consistent with the condition
as all theories consistent with c are included then formula must be in this set
so bayes theorem may be applied as specified givingusing the implication and condition probability law the definition of formula impliesthe probability of each theory in t is given bysofinally the probabilities of the events may be identified with the probabilities of the condition which the outcomes in the event satisfygivingthis is the probability of the theory t after observing that the condition c holds
theories that are less probable than the condition c have no predictive power
separate them out givingthe probability of the theories without predictive power on c is the same as the probability of c
soso the probability and the probability of no prediction for c written as formulathe probability of a condition was given asbit strings for theories that are more complex than the bit string given to the agent as input have no predictive power
there probabilities are better included in the random case
to implement this a new definition is given as f inusing f an improved version of the abductive probabilities is
ray solomonoffs theory of universal inductive inference is a theory of prediction based on logical observations such as predicting the next symbol based upon a given series of symbols
the only assumption that the theory makes is that the environment follows some unknown but computable probability distribution
it is a mathematical formalization of occams razor and the principle of multiple explanations
prediction is done using a completely bayesian framework
the universal prior is calculated for all computable sequencesthis is the universal a priori probability distributionno computable hypothesis will have a zero probability
this means that bayes rule of causation can be used in predicting the continuation of any particular computable sequence
the theory is based in philosophical foundations and was founded by ray solomonoff around
it is a mathematically formalized combination of occams razor and the principle of multiple explanations
all computable theories which perfectly describe previous observations are used to calculate the probability of the next observation with more weight put on the shorter computable theories
marcus hutters universal artificial intelligence builds upon this to calculate the expected value of an action
the proof of the razor is based on the known mathematical properties of a probability distribution over a countable set
these properties are relevant because the infinite set of all programs is a denumerable set
the sum s of the probabilities of all programs must be exactly equal to one as per the definition of probability thus the probabilities must roughly decrease as we enumerate the infinite set of all programs otherwise s will be strictly greater than one
to be more precise for every formula   there is some length l such that the probability of all programs longer than l is at most formula
this does not however preclude very long programs from having very high probability
fundamental ingredients of the theory are the concepts of algorithmic probability and kolmogorov complexity
the universal prior probability of any prefix p of a computable sequence x is the sum of the probabilities of all programs for a universal computer that compute something starting with p
given some p and any computable but unknown probability distribution from which x is sampled the universal prior and bayes theorem can be used to predict the yet unseen parts of x in optimal fashion
though solomonoffs inductive inference is not computable several aixiderived algorithms approximate it in order to make it run on a modern computer
the more computing power they are given the closer their predictions are to the predictions of inductive inference their mathematical limit is solomonoffs inductive inference
another direction of inductive inference is based on e
mark golds model of learning in the limit from  and has developed since then more and more models of learning
the general scenario is the following given a class s of computable functions is there a learner that is recursive functional which for any input of the form ff
fn outputs a hypothesis an index e with respect to a previously agreed on acceptable numbering of all computable functions the indexed function may be required consistent with the given values of f
a learner m learns a function f if almost all its hypotheses are the same index e which generates the function f m learns s if m learns every f in s
basic results are that all recursively enumerable classes of functions are learnable while the class rec of all computable functions is not learnable
many related models have been considered and also the learning of classes of recursively enumerable sets from positive data is a topic studied from golds pioneering paper in  onwards
a far reaching extension of the golds approach is developed by schmidhubers theory of generalized kolmogorov complexities which are kinds of superrecursive algorithms
the third mathematically based direction of inductive inference makes use of the theory of automata and computation
in this context the process of inductive inference is performed by an abstract automaton called an inductive turing machine burgin
inductive turing machines represent the next step in the development of computer science providing better models for contemporary computers and computer networks burgin  and forming an important class of superrecursive algorithms as they satisfy all conditions in the definition of algorithm
namely each inductive turing machines is a type of effective method in which a definite list of welldefined instructions for completing a task when given an initial state will proceed through a welldefined series of successive states eventually terminating in an endstate
the difference between an inductive turing machine and a turing machine is that to produce the result a turing machine has to stop while in some cases an inductive turing machine can do this without stopping
stephen kleene called procedures that could run forever without stopping by the name calculation procedure or algorithm kleene
kleene also demanded that such an algorithm must eventually exhibit some object kleene
this condition is satisfied by inductive turing machines as their results are exhibited after a finite number of steps but inductive turing machines do not always tell at which step the result has been obtained
simple inductive turing machines are equivalent to other models of computation
more advanced inductive turing machines are much more powerful
it is proved burgin  that limiting partial recursive functions trial and error predicates general turing machines and simple inductive turing machines are equivalent models of computation
however simple inductive turing machines and general turing machines give direct constructions of computing automata which are thoroughly grounded in physical machines
in contrast trial and error predicates limiting recursive functions and limiting partial recursive functions present syntactic systems of symbols with formal rules for their manipulation
simple inductive turing machines and general turing machines are related to limiting partial recursive functions and trial and error predicates as turing machines are related to partial recursive functions and lambdacalculus
note that only simple inductive turing machines have the same structure but different functioning semantics of the output mode as turing machines
other types of inductive turing machines have an essentially more advanced structure due to the structured memory and more powerful instructions
their utilization for inference and learning allows achieving higher efficiency and better reflects learning of people burgin and klinger
some researchers confuse computations of inductive turing machines with nonstopping computations or with infinite time computations
first some of computations of inductive turing machines halt
as in the case of conventional turing machines some halting computations give the result while others do not give
second some nonstopping computations of inductive turing machines give results while others do not give
rules of inductive turing machines determine when a computation stopping or nonstopping gives a result
namely an inductive turing machine produces output from time to time and once this output stops changing it is considered the result of the computation
it is necessary to know that descriptions of this rule in some papers are incorrect
for instance davis   formulates the rule when result is obtained without stopping as  once the correct output has been produced any subsequent output will simply repeat this correct result
third in contrast to the widespread misconception inductive turing machines give results when it happens always after a finite number of steps in finite time in contrast to infinite and infinitetime computations
there are two main distinctions between conventional turing machines and simple inductive turing machines
the first distinction is that even simple inductive turing machines can do much more than conventional turing machines
the second distinction is that a conventional turing machine always informs by halting or by coming to a final state when the result is obtained while a simple inductive turing machine in some cases does inform about reaching the result while in other cases where the conventional turing machine is helpless it does not inform
people have an illusion that a computer always itself informs by halting or by other means when the result is obtained
in contrast to this users themselves have to decide in many cases whether the computed result is what they need or it is necessary to continue computations
indeed everyday desktop computer applications like word processors and spreadsheets spend most of their time waiting in event loops and do not terminate until directed to do so by users
evolutionary approach to inductive inference is accomplished by another class of automata called evolutionary inductive turing machines burgin and eberbach
an evolutionary inductive turing machine is a possibly infinite sequence e  at t
of inductive turing machines at each working on generations xt which are coded as words in the alphabet of the machines at
the goal is to build a population z satisfying the inference condition
the automaton at called a component or a level automaton of e represents encodes a onelevel evolutionary algorithm that works with input generations xi of the population by applying the variation operators v and selection operator s
the first generation x is given as input to e and is processed by the automaton a which generatesproduces the first generation x as its transfer output which goes to the automaton a
for all t
the automaton at receives the generation xt as its input from at and then applies the variation operator v and selection operator s producing the generation xi and sending it to at to continue evolution
in statistics a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population without requiring that an observed data set should identify the subpopulation to which an individual observation belongs
formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population
however while problems associated with mixture distributions relate to deriving the properties of the overall population from those of the subpopulations mixture models are used to make statistical inferences about the properties of the subpopulations given only observations on the pooled population without subpopulation identity information
some ways of implementing mixture models involve steps that attribute postulated subpopulationidentities to individual observations or weights towards such subpopulations in which case these can be regarded as types of unsupervised learning or clustering procedures
however not all inference procedures involve such steps
mixture models should not be confused with models for compositional data i
data whose components are constrained to sum to a constant value   etc
however compositional models can be thought of as mixture models where members of the population are sampled at random
conversely mixture models can be thought of as compositional models where the total size reading population has been normalized to
a typical finitedimensional mixture model is a hierarchical model consisting of the following componentsin addition in a bayesian setting the mixture weights and parameters will themselves be random variables and prior distributions will be placed over the variables
in such a case the weights are typically viewed as a kdimensional random vector drawn from a dirichlet distribution the conjugate prior of the categorical distribution and the parameters will be distributed according to their respective conjugate priors
mathematically a basic parametric mixture model can be described as followsin a bayesian setting all parameters are associated with random variables as followsthis characterization uses f and h to describe arbitrary distributions over observations and parameters respectively
typically h will be the conjugate prior of f
the two most common choices of f are gaussian aka normal for realvalued observations and categorical for discrete observations
other common possibilities for the distribution of the mixture components area typical nonbayesian gaussian mixture model looks like thisa bayesian version of a gaussian mixture model is as followsa bayesian gaussian mixture model is commonly extended to fit a vector of unknown parameters denoted in bold or multivariate normal distributions
in a multivariate distribution i
one modelling a vector formula with n random variables one may model a vector of parameters such as several observations of a signal or patches within an image using a gaussian mixture model prior distribution on the vector of estimates given bywhere the i vector component is characterized by normal distributions with weights formula means formula and covariance matrices formula
to incorporate this prior into a bayesian estimation the prior is multiplied with the known distribution formula of the data formula conditioned on the parameters formula to be estimated
with this formulation the posterior distribution formula is also a gaussian mixture model of the form with new parameters formula and formula that are updated using the em algorithm
such distributions are useful for assuming patchwise shapes of images and clusters for example
in the case of image representation each gaussian may be tilted expanded and warped according to the covariance matrices formula
one gaussian distribution of the set is fit to each patch usually of size x pixels in the image
notably any distribution of points around a cluster see kmeans may be accurately given enough gaussian components but scarcely over k components are needed to accurately model a given image distribution or cluster of data
a typical nonbayesian mixture model with categorical observations looks like thisthe random variablesa typical bayesian mixture model with categorical observations looks like thisthe random variablesfinancial returns often behave differently in normal situations and during crisis times
a mixture model for return data seems reasonable
sometimes the model used is a jumpdiffusion model or as a mixture of two normal distributions
see financial economicschallenges and criticism for further context
assume that we observe the prices of n different houses
different types of houses in different neighborhoods will have vastly different prices but the price of a particular type of house in a particular neighborhood e
threebedroom house in moderately upscale neighborhood will tend to cluster fairly closely around the mean
one possible model of such prices would be to assume that the prices are accurately described by a mixture model with k different components each distributed as a normal distribution with unknown mean and variance with each component specifying a particular combination of house typeneighborhood
fitting this model to observed prices e
using the expectationmaximization algorithm would tend to cluster the prices according to house typeneighborhood and reveal the spread of prices in each typeneighborhood
note that for values such as prices or incomes that are guaranteed to be positive and which tend to grow exponentially a lognormal distribution might actually be a better model than a normal distribution
assume that a document is composed of n different words from a total vocabulary of size v where each word corresponds to one of k possible topics
the distribution of such words could be modelled as a mixture of k different vdimensional categorical distributions
a model of this sort is commonly termed a topic model
note that expectation maximization applied to such a model will typically fail to produce realistic results due among other things to the excessive number of parameters
some sorts of additional assumptions are typically necessary to get good results
typically two sorts of additional components are added to the modelthe following example is based on an example in christopher m
bishop pattern recognition and machine learning
imagine that we are given an nn blackandwhite image that is known to be a scan of a handwritten digit between  and  but we dont know which digit is written
we can create a mixture model with formula different components where each component is a vector of size formula of bernoulli distributions one per pixel
such a model can be trained with the expectationmaximization algorithm on an unlabeled set of handwritten digits and will effectively cluster the images according to the digit being written
the same model could then be used to recognize the digit of another image simply by holding the parameters constant computing the probability of the new image for each possible digit a trivial calculation and returning the digit that generated the highest probability
mixture models apply in the problem of directing multiple projectiles at a target as in air land or sea defense applications where the physical andor statistical characteristics of the projectiles differ within the multiple projectiles
an example might be shots from multiple munitions types or shots from multiple locations directed at one target
the combination of projectile types may be characterized as a gaussian mixture model
further a wellknown measure of accuracy for a group of projectiles is the circular error probable cep which is the number r such that on average half of the group of projectiles falls within the circle of radius r about the target point
the mixture model can be used to determine or estimate the value r
the mixture model properly captures the different types of projectiles
the financial example above is one direct application of the mixture model a situation in which we assume an underlying mechanism so that each observation belongs to one of some number of different sources or categories
this underlying mechanism may or may not however be observable
in this form of mixture each of the sources is described by a component probability density function and its mixture weight is the probability that an observation comes from this component
in an indirect application of the mixture model we do not assume such a mechanism
the mixture model is simply used for its mathematical flexibilities
for example a mixture of two normal distributions with different means may result in a density with two modes which is not modeled by standard parametric distributions
another example is given by the possibility of mixture distributions to model fatter tails than the basic gaussian ones so as to be a candidate for modeling more extreme events
when combined with dynamical consistency this approach has been applied to financial derivatives valuation in presence of the volatility smile in the context of local volatility models
this defines our application
the mixture modelbased clustering is also predominantly used in identifying the state of the machine in predictive maintenance
density plots are used to analyze the density of high dimensional features
if multimodel densities are observed then it is assumed that a finite set of densities are formed by a finite set of normal mixtures
a multivariate gaussian mixture model is used to cluster the feature data into k number of groups where k represents each state of the machine
the machine state can be a normal state power off state or faulty state
each formed cluster can be diagnosed using techniques such as spectral analysis
in the recent years this has also been widely used in other areas such as early fault detection
in image processing and computer vision traditional image segmentation models often assign to one pixel only one exclusive pattern
in fuzzy or soft segmentation any pattern can have certain ownership over any single pixel
if the patterns are gaussian fuzzy segmentation naturally results in gaussian mixtures
combined with other analytic or geometric tools e
phase transitions over diffusive boundaries such spatially regularized mixture models could lead to more realistic and computationally efficient segmentation methods
identifiability refers to the existence of a unique characterization for any one of the models in the class family being considered
estimation procedures may not be welldefined and asymptotic theory may not hold if a model is not identifiable
let j be the class of all binomial distributions with
then a mixture of two members of j would haveand
clearly given p and p it is not possible to determine the above mixture model uniquely as there are three parameters    to be determined
consider a mixture of parametric distributions of the same class
letbe the class of all component distributions
then the convex hull k of j defines the class of all finite mixture of distributions in jk is said to be identifiable if all its members are unique that is given two members p and in k being mixtures of k distributions and distributions respectively in j we have if and only if first of all and secondly we can reorder the summations such that and for all i
parametric mixture models are often used when we know the distribution y and we can sample from x but we would like to determine the a and  values
such situations can arise in studies in which we sample from a population that is composed of several distinct subpopulations
it is common to think of probability mixture modeling as a missing data problem
one way to understand this is to assume that the data points under consideration have membership in one of the distributions we are using to model the data
when we start this membership is unknown or missing
the job of estimation is to devise appropriate parameters for the model functions we choose with the connection to the data points being represented as their membership in the individual model distributions
a variety of approaches to the problem of mixture decomposition have been proposed many of which focus on maximum likelihood methods such as expectation maximization em or maximum a posteriori estimation map
generally these methods consider separately the questions of system identification and parameter estimation methods to determine the number and functional form of components within a mixture are distinguished from methods to estimate the corresponding parameter values
some notable departures are the graphical methods as outlined in tarter and lock and more recently minimum message length mml techniques such as figueiredo and jain and to some extent the moment matching pattern analysis routines suggested by mcwilliam and loh
expectation maximization em is seemingly the most popular technique used to determine the parameters of a mixture with an a priori given number of components
this is a particular way of implementing maximum likelihood estimation for this problem
em is of particular appeal for finite normal mixtures where closedform expressions are possible such as in the following iterative algorithm by dempster et al
with the posterior probabilitiesthus on the basis of the current estimate for the parameters the conditional probability for a given observation x being generated from state s is determined for each  n being the sample size
the parameters are then updated such that the new component weights correspond to the average conditional probability and each component mean and covariance is the component specific weighted average of the mean and covariance of the entire sample
dempster also showed that each successive em iteration will not decrease the likelihood a property not shared by other gradient based maximization techniques
moreover em naturally embeds within it constraints on the probability vector and for sufficiently large sample sizes positive definiteness of the covariance iterates
this is a key advantage since explicitly constrained methods incur extra computational costs to check and maintain appropriate values
theoretically em is a firstorder algorithm and as such converges slowly to a fixedpoint solution
redner and walker  make this point arguing in favour of superlinear and second order newton and quasinewton methods and reporting slow convergence in em on the basis of their empirical tests
they do concede that convergence in likelihood was rapid even if convergence in the parameter values themselves was not
the relative merits of em and other algorithms visvis convergence have been discussed in other literature
other common objections to the use of em are that it has a propensity to spuriously identify local maxima as well as displaying sensitivity to initial values
one may address these problems by evaluating em at several initial points in the parameter space but this is computationally costly and other approaches such as the annealing em method of udea and nakano  in which the initial components are essentially forced to overlap providing a less heterogeneous basis for initial guesses may be preferable
figueiredo and jain note that convergence to meaningless parameter values obtained at the boundary where regularity conditions breakdown e
ghosh and sen  is frequently observed when the number of model components exceeds the optimaltrue one
on this basis they suggest a unified approach to estimation and identification in which the initial n is chosen to greatly exceed the expected optimal value
their optimization routine is constructed via a minimum message length mml criterion that effectively eliminates a candidate component if there is insufficient information to support it
in this way it is possible to systematize reductions in n and consider estimation and identification jointly
the expectationmaximization algorithm can be used to compute the parameters of a parametric mixture model distribution the a and
it is an iterative algorithm with two steps an expectation step and a maximization step
practical examples of em and mixture modeling are included in the socr demonstrations
with initial guesses for the parameters of our mixture model partial membership of each data point in each constituent distribution is computed by calculating expectation values for the membership variables of each data point
that is for each data point x and distribution y the membership value y iswith expectation values in hand for group membership plugin estimates are recomputed for the distribution parameters
the mixing coefficients a are the means of the membership values over the n data points
the component model parameters  are also calculated by expectation maximization using data points x that have been weighted using the membership values
for example if  is a mean with new estimates for a and the s the expectation step is repeated to recompute new membership values
the entire procedure is repeated until model parameters converge
as an alternative to the em algorithm the mixture model parameters can be deduced using posterior sampling as indicated by bayes theorem
this is still regarded as an incomplete data problem whereby membership of data points is the missing data
a twostep iterative procedure known as gibbs sampling can be used
the previous example of a mixture of two gaussian distributions can demonstrate how the method works
as before initial guesses of the parameters for the mixture model are made
instead of computing partial memberships for each elemental distribution a membership value for each data point is drawn from a bernoulli distribution that is it will be assigned to either the first or the second gaussian
the bernoulli parameter  is determined for each data point on the basis of one of the constituent distributions
draws from the distribution generate membership associations for each data point
plugin estimators can then be used as in the m step of em to generate a new set of mixture model parameters and the binomial draw step repeated
the method of moment matching is one of the oldest techniques for determining the mixture parameters dating back to karl pearsons seminal work of
in this approach the parameters of the mixture are determined such that the composite distribution has moments matching some given value
in many instances extraction of solutions to the moment equations may present nontrivial algebraic or computational problems
moreover numerical analysis by day has indicated that such methods may be inefficient compared to em
nonetheless there has been renewed interest in this method e
craigmile and titterington  and wang
mcwilliam and loh  consider the characterisation of a hypercuboid normal mixture copula in large dimensional systems for which em would be computationally prohibitive
here a pattern analysis routine is used to generate multivariate taildependencies consistent with a set of univariate and in some sense bivariate moments
the performance of this method is then evaluated using equity logreturn data with kolmogorovsmirnov test statistics suggesting a good descriptive fit
some problems in mixture model estimation can be solved using spectral methods
in particular it becomes useful if data points x are points in highdimensional real space and the hidden distributions are known to be logconcave such as gaussian distribution or exponential distribution
spectral methods of learning mixture models are based on the use of singular value decomposition of a matrix which contains data points
the idea is to consider the top k singular vectors where k is the number of distributions to be learned
the projectionof each data point to a linear subspace spanned by those vectors groups points originating from the same distributionvery close together while points from different distributions stay far apart
one distinctive feature of the spectral method is that it allows us to prove that ifdistributions satisfy certain separation condition e
not too close then the estimated mixture will be very close to the true one with high probability
tarter and lock describe a graphical approach to mixture identification in which a kernel function is applied to an empirical frequency plot so to reduce intracomponent variance
in this way one may more readily identify components having differing means
while this method does not require prior knowledge of the number or functional form of the components its success does rely on the choice of the kernel parameters which to some extent implicitly embeds assumptions about the component structure
some of them can even probably learn mixtures of heavytailed distributions including those withinfinite variance see links to papers below
in this setting em based methods would not work since the expectation step would diverge due to presence ofoutliers
to simulate a sample of size n that is from a mixture of distributions f i to n with probabilities p sumpin a bayesian setting additional levels can be added to the graphical model defining the mixture model
for example in the common latent dirichlet allocation topic model the observations are sets of words drawn from d different documents and the k mixture components represent topics that are shared across documents
each document has a different set of mixture weights which specify the topics prevalent in that document
all sets of mixture weights share common hyperparameters
a very common extension is to connect the latent variables defining the mixture component identities into a markov chain instead of assuming that they are independent identically distributed random variables
the resulting model is termed a hidden markov model and is one of the most common sequential hierarchical models
numerous extensions of hidden markov models have been developed see the resulting article for more information
mixture distributions and the problem of mixture decomposition that is the identification of its constituent components and the parameters thereof has been cited in the literature as far back as  quetelet in mclachlan  although common reference is made to the work of karl pearson  as the first author to explicitly address the decomposition problem in characterising nonnormal attributes of forehead to body length ratios in female shore crab populations
the motivation for this work was provided by the zoologist walter frank raphael weldon who had speculated in  in tarter and lock that asymmetry in the histogram of these ratios could signal evolutionary divergence
pearsons approach was to fit a univariate mixture of two normals to the data by choosing the five parameters of the mixture such that the empirical moments matched that of the model
while his work was successful in identifying two potentially distinct subpopulations and in demonstrating the flexibility of mixtures as a moment matching tool the formulation required the solution of a th degree nonic polynomial which at the time posed a significant computational challenge
subsequent works focused on addressing these problems but it was not until the advent of the modern computer and the popularisation of maximum likelihood mle parameterisation techniques that research really took off
since that time there has been a vast body of research on the subject spanning areas such as fisheries research agriculture botany economics medicine genetics psychology palaeontology electrophoresis finance sedimentologygeology and zoology
dataiku is a computer software company headquartered in new york city
the company develops collaborative data science software marketed for big data
the company was founded in paris in  by  cofounders
two of them met while working at french search engine company exalead including chief executive florian douetteau and clment stnac
for its first two years the company relied on its own capital
in january  dataiku raised
million from serena capital and alven capital two french technology venture capital funds
this was followed by  million raised with firstmark capital a new york citybased venture capital firm in october
in september  the company raised a  million series b investment from battery ventures as well as historic investors
dataiku opened an office in new york city in  which became the company headquarters
they opened an office in london in the summer of
the software dataiku data science studio dss was announced in  supporting predictive modelling to build business applications
later versions of dss added other features
dataiku offers a free edition and enterprise versions with additional features such as multiuser collaboration or realtime scoring
in  dataiku entered the gartner magic quadrant for data science platforms as a visionary
in computer science uncertain data is data that contains noise that makes it deviate from the correct intended or original values
in the age of big data uncertainty or data veracity is one of the defining characteristics of data
data is constantly growing in volume variety velocity and uncertainty veracity
uncertain data is found in abundance today on the web in sensor networks within enterprises both in their structured and unstructured sources
for example there may be uncertainty regarding the address of a customer in an enterprise dataset or the temperature readings captured by a sensor due to aging of the sensor
in  ibm called out managing uncertain data at scale in its global technology outlook report that presents a comprehensive analysis looking three to ten years into the future seeking to identify significant disruptive technologies that will change the world
in order to make confident business decisions based on realworld data analyses must necessarily account for many different kinds of uncertainty present in very large amounts of data
analyses based on uncertain data will have an effect on the quality of subsequent decisions so the degree and types of inaccuracies in this uncertain data cannot be ignored
uncertain data is found in the area of sensor networks text where noisy text is found in abundance on social media web and within enterprises where the structured and unstructured data may be old outdated or plain incorrect in modeling where the mathematical model may only be an approximation of the actual process
when representing such data in a database some indication of the probability of the correctness of the various values also needs to be estimated
there are three main models of uncertain data in databases
in attribute uncertainty each uncertain attribute in a tuple is subject to its own independent probability distribution
for example if readings are taken of temperature and wind speed each would be described by its own probability distribution as knowing the reading for one measurement would not provide any information about the other
in correlated uncertainty multiple attributes may be described by a joint probability distribution
for example if readings are taken of the position of an object and the x and ycoordinates stored the probability of different values may depend on the distance from the recorded coordinates
as distance depends on both coordinates it may be appropriate to use a joint distribution for these coordinates as they are not independent
in tuple uncertainty all the attributes of a tuple are subject to a joint probability distribution
this covers the case of correlated uncertainty but also includes the case where there is a probability of a tuple not belonging in the relevant relation which is indicated by all the probabilities not summing to one
for example assume we have the following tuple from a probabilistic databasethen the tuple has  chance of not existing in the database
multitask learning mtl is a subfield of machine learning in which multiple learning tasks are solved at the same time while exploiting commonalities and differences across tasks
this can result in improved learning efficiency and prediction accuracy for the taskspecific models when compared to training the models separately
early versions of mtl were called hintsin a widely cited  paper rich caruana gave the following characterizationmultitask learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias
it does this by learning tasks in parallel while using a shared representation what is learned for each task can help other tasks be learned better
in the classification context mtl aims to improve the performance of multiple classification tasks by learning them jointly
one example is a spamfilter which can be treated as distinct but related classification tasks across different users
to make this more concrete consider that different people have different distributions of features which distinguish spam emails from legitimate ones for example an english speaker may find that all emails in russian are spam not so for russian speakers
yet there is a definite commonality in this classification task across users for example one common feature might be text related to money transfer
solving each users spam classification problem jointly via mtl can let the solutions inform each other and improve performance
further examples of settings for mtl include multiclass classification and multilabel classification
multitask learning works because regularization induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalizing all complexity uniformly
one situation where mtl may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled
however as discussed below mtl has also been shown to be beneficial for learning unrelated tasks
within the mtl paradigm information can be shared across some or all of the tasks
depending on the structure of task relatedness one may want to share information selectively across the tasks
for example tasks may be grouped or exist in a hierarchy or be related according to some general metric
suppose as developed more formally below that the parameter vector modeling each task is a linear combination of some underlying basis
similarity in terms of this basis can indicate the relatedness of the tasks
for example with sparsity overlap of nonzero coefficients across tasks indicates commonality
a task grouping then corresponds to those tasks lying in a subspace generated by some subset of basis elements where tasks in different groups may be disjoint or overlap arbitrarily in terms of their bases
task relatedness can be imposed a priori or learned from the data
hierarchical task relatedness can also be exploited implicitly without assuming a priori knowledge or learning relations explicitly
one can attempt learning a group of principal tasks using a group of auxiliary tasks unrelated to the principal ones
in many applications joint learning of unrelated tasks which use the same input data can be beneficial
the reason is that prior knowledge about task relatedness can lead to sparser and more informative representations for each task grouping essentially by screening out idiosyncrasies of the data distribution
novel methods which builds on a prior multitask methodology by favoring a shared lowdimensional representation within each task grouping have been proposed
the programmer can impose a penalty on tasks from different groups which encourages the two representations to be orthogonal
experiments on synthetic and real data have indicated that incorporating unrelated tasks can result in significant improvements over standard multitask learning methods
related to multitask learning is the concept of knowledge transfer
whereas traditional multitask learning implies that a shared representation is developed concurrently across tasks transfer of knowledge implies a sequentially shared representation
large scale machine learning projects such as the deep convolutional neural network googlenet an imagebased object classifier can develop robust representations which may be useful to further algorithms learning related tasks
for example the pretrained model can be used as a feature extractor to perform preprocessing for another learning algorithm
or the pretrained model can be used to initialize a model with similar architecture which is then finetuned to learn a different classification task
traditionally multitask learning and transfer of knowledge are applied to stationary learning settings
their extension to nonstationary environments is termed group online adaptive learning goal
sharing information could be particularly useful if learners operate in continuously changing environments because a learner could benefit from previous experience of another learner to quickly adapt to their new environment
such groupadaptive learning has numerous applications from predicting financial timeseries through content recommendation systems to visual understanding for adaptive autonomous agents
the mtl problem can be cast within the context of rkhsvv a complete inner product space of vectorvalued functions equipped with a reproducing kernel
in particular recent focus has been on cases where task structure can be identified via a separable kernel described below
the presentation here derives from ciliberto et al
suppose the training data set is formula with formula formula where formula indexes task and formula
let formula
in this setting there is a consistent input and output space and the same loss function formula for each task
this results in the regularized machine learning problem where formula is a vector valued reproducing kernel hilbert space with functions formula having components formula
the reproducing kernel for the space formula of functions formula is a symmetric matrixvalued function formula  such that formula and the following reproducing property holds the form of the kernel formula induces both the representation of the feature space and structures the output across tasks
a natural simplification is to choose a separable kernel which factors into separate kernels on the input space formula and on the tasks formula
in this case the kernel relating scalar components formula and formula is given by formula
for vector valued functions formula we can write formula where formula is a scalar reproducing kernel and formula is a symmetric positive semidefinite formula matrix
henceforth denote formula
this factorization property separability implies the input feature space representation does not vary by task
that is there is no interaction between the input kernel and the task kernel
the structure on tasks is represented solely by formula
methods for nonseparable kernels formula is an current field of research
for the separable case the representation theorem is reduced to formula
the model output on the training data is then formula  where formula is the formula empirical kernel matrix with entries formula and formula is the formula matrix of rows formula
with the separable kernel equation can be rewritten aswhere formula is a weighted average of formula applied entrywise to y and kca
the weight is zero if formula is a missing observation
note the second term in can be derived as followsformulaformula bilinearityformula reproducing propertyformulathere are three largely equivalent ways to represent task structure through a regularizer through an output metric and through an output mapping
regularizer  with the separable kernel it can be shown below that formula where formula is the formula element of the pseudoinverse of formula and formula is the rkhs based on the scalar kernel formula and formula
this formulation shows that formula controls the weight of the penalty associated with formula
note that formula arises from formula
proofformulaformulaformulaformulaformulaformulaformulaformulaformulaoutput metric  an alternative output metric on formula can be induced by the inner product formula
with the squared loss there is an equivalence between the separable kernels formula under the alternative metric and formula under the canonical metric
output mapping  outputs can be mapped as formula to a higher dimensional space to encode complex structures such as trees graphs and strings
for linear maps formula with appropriate choice of separable kernel it can be shown that formula
via the regularizer formulation one can represent a variety of task structures easily
learning problem can be generalized to admit learning task matrix a as followschoice of formula must be designed to learn matrices a of a given type
see special cases below
restricting to the case of convex losses and coercive penalties ciliberto et al
have shown that although is not convex jointly in c and a a related problem is jointly convex
specifically on the convex set formula the equivalent problemis convex with the same minimum value
and if formula is a minimizer for then formula is a minimizer for
the perturbation via the barrier formula forces the objective functions to be equal to formula on the boundary of formula
spectral penalties  dinnuzo et al suggested setting f as the frobenius norm formula
they optimized directly using block coordinate descent not accounting for difficulties at the boundary of formula
clustered tasks learning  jacob et al suggested to learn a in the setting where t tasks are organized in r disjoint clusters
in this case let formula be the matrix with formula
setting formula and formula the task matrix formula can be parameterized as a function of formula formula  with terms that penalize the average between clusters variance and within clusters variance respectively of the task predictions
m is not convex but there is a convex relaxation formula
in this formulation formula
nonconvex penalties  penalties can be constructed such that a is constrained to be a graph laplacian or that a has low rank factorization
however these penalties are not convex and the analysis of the barrier method proposed by ciliberto et al
does not go through in these cases
nonseparable kernels  separable kernels are limited in particular they do not account for structures in the interaction space between the input and output domains jointly
future work is needed to develop models for these kernels
using the principles of mtl techniques for collaborative spam filtering that facilitates personalization have been proposed
in large scale open membership email systems most users do not label enough messages for an individual local classifier to be effective while the data is too noisy to be used for a global filter across all users
a hybrid globalindividual classifier can be effective at absorbing the influence of users who label emails very diligently from the general public
this can be accomplished while still providing sufficient quality to users with few labeled instances
using boosted decision trees one can enable implicit data sharing and regularization
this learning method can be used on websearch ranking data sets
one example is to use ranking data sets from several countries
here multitask learning is particularly helpful as data sets from different countries vary largely in size because of the cost of editorial judgments
it has been demonstrated that learning various tasks jointly can lead to significant improvements in performance with surprising reliability
in order to facilitate transfer of knowledge it infrastructure is being developed
one such project roboearth aims to set up an open source internet database that can be accessed and continually updated from around the world
the goal is to facilitate a cloudbased interactive knowledge base accessible to technology companies and academic institutions which can enhance the sensing acting and learning capabilities of robots and other artificial intelligence agents
the multitask learning via structural regularization malsar matlab package implements the following multitask learning algorithms
machine learning a subfield of computer science involving the development of algorithms that learn how to make predictions based on data has a number of emerging applications in the field of bioinformatics
bioinformatics deals with computational and mathematical approaches for understanding and processing biological data
prior to the emergence of machine learning algorithms bioinformatics algorithms had to be explicitly programmed by hand which for problems such as protein structure prediction proves extremely difficult
machine learning techniques such as deep learning enable the algorithm to make use of automatic feature learning which means that based on the dataset alone the algorithm can learn how to combine multiple features of the input data into a more abstract set of features from which to conduct further learning
this multilayered approach to learning patterns in the input data allows such systems to make quite complex predictions when trained on large datasets
in recent years the size and number of available biological datasets have skyrocketed enabling bioinformatics researchers to make use of these machine learning systems
machine learning has been applied to six main subfields of bioinformatics genomics proteomics microarrays systems biology evolution and text mining
genomics involves the study of the genome the complete dna sequence of organisms
while genomic sequence data has historically been sparse due to the technical difficulty in sequencing a piece of dna the number of available sequences is growing exponentially
however while raw data is becoming increasingly available and accessible the biological interpretation of this data is occurring at a much slower pace
therefore there is an increasing need for the development of machine learning systems that can automatically determine the location of proteinencoding genes within a given dna sequence
this is a problem in computational biology known as gene prediction
gene prediction is commonly performed through a combination of what are known as extrinsic and intrinsic searches
for the extrinsic search the input dna sequence is run through a large database of sequences whose genes have been previously discovered and their locations annotated
a number of the sequences genes can be identified by determining which strings of bases within the sequence are homologous to known gene sequences
however given the limitation in size of the database of known and annotated gene sequences not all the genes in a given input sequence can be identified through homology alone
therefore an intrinsic search is needed where a gene prediction program attempts to identify the remaining genes from the dna sequence alone
machine learning is also been used for the problem of multiple sequence alignment which involves aligning many dna or amino acid sequences in order to determine regions of similarity that could indicate a shared evolutionary history
it can also be used to detect and visualize genome rearrangements
proteins strings of amino acids gain much of their function from protein folding in which they conform into a threedimensional structure
this structure is composed of a number of layers of folding including the primary structure i
the flat string of amino acids the secondary structure alpha helices and beta sheets the tertiary structure and the quartenary structure
protein secondary structure prediction is a main focus of this subfield as the further protein foldings tertiary and quartenary structures are determined based on the secondary structure
solving the true structure of a protein is an incredibly expensive and timeintensive process furthering the need for systems that can accurately predict the structure of a protein by analyzing the amino acid sequence directly
prior to machine learning researchers needed to conduct this prediction manually
this trend began in  when pauling and corey released their work on predicting the hydrogen bond configurations of a protein from a polypeptide chain
today through the use of automatic feature learning the best machine learning techniques are able to achieve an accuracy of
the current stateoftheart in secondary structure prediction uses a system called deepcnf deep convolutional neural fields which relies on the machine learning model of artificial neural networks to achieve an accuracy of approximately  when tasked to classify the amino acids of a protein sequence into one of three structural classes helix sheet or coil
the theoretical limit for threestate protein secondary structure is
machine learning has also been applied to proteomics problems such as protein sidechain prediction protein loop modeling and protein contact map prediction
microarrays a type of labonachip are used for automatically collecting data about large amounts of biological material
machine learning can aid in the analysis of this data and it has been applied to expression pattern identification classification and genetic network induction
this technology is especially useful for monitoring the expression of genes within a genome aiding in diagnosing different types of cancer based on which genes are expressed
one of the main problems in this field is identifying which genes are expressed based on the collected data
in addition due to the huge number of genes on which data is collected by the microarray there is a large amount of irrelevant data to the task of expressed gene identification further complicating this problem
machine learning presents a potential solution to this problem as various classification methods can be used to perform this identification
the most commonly used methods are radial basis function networks deep learning bayesian classification decision trees and random forest
systems biology focuses on the study of the emergent behaviors from complex interactions of simple biological components in a system
such components can include molecules such as dna rna proteins and metabolites
machine learning has been used to aid in the modelling of these complex interactions in biological systems in domains such as genetic networks signal transduction networks and metabolic pathways
probabilistic graphical models a machine learning technique for determining the structure between different variables are one of the most commonly used methods for modeling genetic networks
in addition machine learning has been applied to systems biology problems such as identifying transcription factor binding sites using a technique known as markov chain optimization
genetic algorithms machine learning techniques which are based on the natural process of evolution have been used to model genetic networks and regulatory structures
other systems biology applications of machine learning include the task of enzyme function prediction high throughput microarray data analysis analysis of genomewide association studies to better understand markers of multiple sclerosis protein function prediction and identification of ncrsensitivity of genes in yeast
the increase in available biological publications led to the issue of the increase in difficulty in searching through and compiling all the relevant available information on a given topic across all sources
this task is known as knowledge extraction
this is necessary for biological data collection which can then in turn be fed into machine learning algorithms to generate new biological knowledge
machine learning can be used for this knowledge extraction task using techniques such as natural language processing to extract the useful information from humangenerated reports in a database
text nailing an alternative approach to machine learning capable of extracting features from clinical narrative notes was introduced in
this technique has been applied to the search for novel drug targets as this task requires the examination of information stored in biological databases and journals
annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein so additional information must be extracted from biomedical literature
machine learning has been applied to automatic annotation of the function of genes and proteins determination of the subcellular localization of a protein analysis of dnaexpression arrays largescale protein interaction analysis and molecule interaction analysis
another application of text mining is the detection and visualization of distinct dna regions given sufficient reference data
catastrophic interference also known as catastrophic forgetting is the tendency of an artificial neural network to completely and abruptly forget previously learned information upon learning new information
neural networks are an important part of the network approach and connectionist approach to cognitive science
these networks use computer simulations to try to model human behaviours such as memory and learning
catastrophic interference is an important issue to consider when creating connectionist models of memory
it was originally brought to the attention of the scientific community by research from mccloskey and cohen  and ractcliff
it is a radical manifestation of the sensitivitystability dilemma or the stabilityplasticity dilemma
specifically these problems refer to the issue of being able to make an artificial neural network that is sensitive to but not disrupted by new information
lookup tables and connectionist networks lie on the opposite sides of the stability plasticity spectrum
the former remains completely stable in the presence of new information but lacks the ability to generalize i
infer general principles from new inputs
on the other hand connectionist networks like the standard backpropagation network are very sensitive to new information and can generalize on new inputs
backpropagation models can be considered good models of human memory insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory
notably these backpropagation networks are susceptible to catastrophic interference
this is considered an issue when attempting to model human memory because unlike these networks humans typically do not show catastrophic forgetting
thus the issue of catastrophic interference must be eradicated from these backpropagation models in order to enhance the plausibility as models of human memory
the term catastrophic interference was originally coined by mccloskey and cohen  but was also brought to the attention of the scientific community by research from ratcliff
mccloskey and cohe n noted the problem of catastrophic interference during two different experiments with backpropagation neural network modelling
in their first experiment they trained a standard backpropagation neural network on a single training set consisting of  singledigit ones problems i
through    and    through    until the network could represent and respond properly to all of them
the error between the actual output and the desired output steadily declined across training sessions which reflected that the network learned to represent the target outputs better across trials
next they trained the network on a single training set consisting of  singledigit twos problems i
through    and    through    until the network could represent respond properly to all of them
they noted that their procedure was similar to how a child would learn their addition facts
following each learning trial on the twos facts the network was tested for its knowledge on both the ones and twos addition facts
like the ones facts the twos facts were readily learned by the network
however mccloskey and cohen noted the network was no longer able to properly answer the ones addition problems even after one learning trial of the twos addition problems
the output pattern produced in response to the ones facts often resembled an output pattern for an incorrect number more closely than the output pattern for an incorrect number
this is considered to be a drastic amount of error
furthermore the problems  and  which were included in both training sets even showed dramatic disruption during the first learning trials of the twos facts
in their second connectionist model mccloskey and cohen attempted to replicate the study on retroactive interference in humans by barnes and underwood
they trained the model on ab and ac lists and used a context pattern in the input vector input pattern to differentiate between the lists
specifically the network was trained to responds with the right b response when shown the a stimulus and ab context pattern and to respond with the correct c response when shown the a stimulus and the ac context pattern
when the model was trained concurrently on the ab and ac items then the network readily learned all of the associations correctly
in sequential training the ab list was trained first followed by the ac list
after each presentation of the ac list performance was measured for both the ab and ac lists
they found that the amount of training on the ac list in barnes and underwood study that lead to  correct responses lead to nearly  correct responses by the backpropagation network
furthermore they found that the network tended to show responses that looked like the c response pattern when the network was prompted to give the b response pattern
this indicated that the ac list apparently had overwritten the ab list
this could be likened to learning the word dog followed by learning the word stool and then finding that you cannot recognize the word cat well but instead think of the word stool when presented with the word dog
mccloskey and cohen tried to reduce interference through a number of manipulations including changing the number of hidden units changing the value of the learning rate parameter overtraining on the ab list freezing certain connection weights changing target values  and  instead
and
however none of these manipulations satisfactorily reduced the catastrophic interference exhibited by the networks
overall mccloskey and cohen  concluded that ratcliff  used multiple sets of backpropagation models applied to standard recognition memory procedures in which the items were sequentially learned
after inspecting the recognition performance models he found two major problemseven one learning trial with new information resulted in a significant loss of the old information paralleling the findings of mccloskey and cohen
ratcliff also found that the resulting outputs were often a blend of the previous input and the new input
in larger networks items learned in groups e
ab then cd were more resistant to forgetting than were items learned singly e
a then b then c
however the forgetting for items learned in groups was still large
adding new hidden units to the network did not reduce interference
this finding contradicts with studies on human memory which indicated that discrimination increases with learning
ratcliff attempted to alleviate this problem by adding response nodes that would selectively respond to old and new inputs
however this method did not work as these response nodes would become active for all inputs
a model which used a context pattern also failed to increase discrimination between new and old items
many researchers have suggested that the main cause of catastrophic interference is overlap in the representations at the hidden layer of distributed neural networks
in a distributed representation any given input will tend to create changes in the weights to many of the nodes
catastrophic forgetting occurs because when many of the weights where knowledge is stored are changed it is impossible for prior knowledge to be kept intact
during sequential learning the inputs become mixed with the new input being superimposed over top of the old input
another way to conceptualize this is through visualizing learning as movement through a weight space
this weight space can be likened to a spatial representation of all of the possible combinations of weights that the network can possess
when a network first learns to represent a set of patterns it has found a point in weight space which allows it to recognize all of the patterns that it has seen
however when the network learns a new set of patterns sequentially it will move to a place in the weight space that allows it to only recognize the new pattern
to recognize both sets of patterns the network must find a place in weight space that can represent both the new and the old output
one way to do this is by connecting a hidden unit to only a subset of the input units
this reduces the likelihood that two different inputs will be encoded by the same hidden units and weights and so will decrease the chance of interference
indeed a number of the proposed solutions to catastrophic interference involve reducing the amount of overlap that occurs when storing information in these weights
many of the early techniques in reducing representational overlap involved making either the input vectors or the hidden unit activation patterns orthogonal to one another
lewandowsky and li  noted that the interference between sequentially learned patterns is minimized if the input vectors are orthogonal to each other
input vectors are said to be orthogonal to each other if the pairwise product of their elements across the two vectors sum to zero
for example the patterns  and  are said to be orthogonal because
one of the techniques which can create orthogonal representations at the hidden layers involves bipolar feature coding i
coding using  and  rather than  and
orthogonal patterns tend to produce less interference with each other
however not all learning problems can be represented using these types of vectors and some studies report that the degree of interference is still problematic with orthogonal vectors
simple techniques such as varying the learning rate parameters in the backpropagation equation were not successful in reducing interference
varying the number of hidden nodes has also been used to try and reduce interference
however the findings have been mixed with some studies finding that more hidden units decrease interference and other studies finding it does not
below are a number of techniques which have empirical support in successfully reducing catastrophic interference in backpropagation neural networksfrench  proposed that catastrophic interference arises in feedforward backpropagation networks due to the interaction of node activations or activation overlap that occur in distributed representations at the hidden layer
specifically he defined this activation overlap as the average shared activation over all units in the hidden layer calculated by summing the lowest activation of the nodes at the hidden layer and averaging this sum
for example if the activations at the hidden layer from one input are
and the activations from the next input are
the activation overlap would be
when using binary numberbinary representation of input row vectorvectors activation values will be  through  where  indicates no activation overlap and  indicates full activation overlap
french noted that neural networks which employ very localized representations do not show catastrophic interference because of the lack of overlap at the hidden layer
that is to say each input pattern will create a hidden layer representation that involves the activation of only one node so differed inputs will have an activation overlap of
thus he suggested that reducing the value of activation overlap at the hidden layer would reduce catastrophic interference in distributed networks
specifically he proposed that this could be done through changing the distributed representations at the hidden layer to semidistributed representations
a semidistributed representation has fewer hidden nodes that are active andor a lower activation value for these nodes for each representation which will make the representations of the different inputs overlap less at the hidden layer
french recommended that this could be done through activation sharpening a technique which slightly increases the activation of a certain number of the most active nodes in the hidden layer slightly reduces the activation of all the other units and then changes the inputtohidden layer weights to reflect these activation changes similar to error backpropagation
overall the guidelines for the process of activation sharpening are as followsin his tests of an  inputhiddenoutput node backpropagation network where one node was sharpened french found that this sharpening paradigm did result in one node being much more active than the other seven
moreover when sharpened this network took one fourth the time to relearn the initial inputs than a standard backpropagation without node sharpening
relearning is a measure of memory savings and thus extent of forgetting where more time to relearn suggests more forgetting ebbinghaus savings method
a twonode sharpened network performed even slightly better however if more than two nodes were sharpened forgetting increased again
according to french the sharpened activations interfere less with weights in the network than unsharpened weights and this is due specifically to the way that backpropagation algorithm calculates weight changes
activations near  will change the weights of links less than activations near
consequently when there are many nodes with low activations due to sharpening the weights to and from these nodes will be modified much less than the weights on very active nodes
as a result when a new input is fed into the network sharpening will reduce activation overlap by limiting the number of highly active hidden units and will reduce the likelihood of representational overlap by reducing the number of weights that are to be changed
thus node sharpening will decrease the amount of disruption in the old weights which store prior input patterns thereby reducing the likelihood of catastrophic forgetting
kortge  proposed a learning rule for training neural networks called the novelty rule to help alleviate catastrophic interference
as its name suggests this rule helps the neural network to learn only the components of a new input that differ from an old input
consequently the novelty rule changes only the weights that were not previously dedicated to storing information thereby reducing the overlap in representations at the hidden units
thus even when inputs are somewhat similar to another dissimilar representations can be made at the hidden layer
in order to apply the novelty rule during learning the input pattern is replaced by a novelty vector that represents the components that differ
the novelty vector for the first layer input units to hidden units is determined by taking the target pattern away from the current output of the network the delta rule
for the second layer hidden units to output units the novelty vector is simply the activation of the hidden units that resulted from using the novelty vector as an input through the first layer
weight changes in the network are computed by using a modified delta rule with the novelty vector replacing the activation value sum of the inputswhen the novelty rule is used in a standard backpropagation network there is no or lessened forgetting of old items when new items are presented sequentially
however this rule can only apply to autoencoder or autoassociative networks in which the target response for the output layer is identical to the input pattern
this is because the novelty vector would be meaningless if the desired output was not identical to the input as it would be impossible to calculate how much a new input differed from the old input
mcrae and hetherington  argued that humans unlike most neural networks do not take on new learning tasks with a random set of weights
rather people tend to bring a wealth of prior knowledge to a task and this helps to avoid the problem of interference
they proposed that when a network is pretrained on a random sample of data prior to starting a sequential learning task that this prior knowledge will naturally constrain how the new information can be incorporated
this would occur because a random sample of data from a domain which has a high degree of internal structure such as the english language training would capture the regularities or recurring patterns found within that domain
since the domain is based on regularities a newly learned item will tend to be similar to the previously learned information which will allow the network to incorporate new data with little interference with existing data
specifically an input vector which follows the same pattern of regularities as the previously trained data should not cause a drastically different pattern of activation at the hidden layer or drastically alter weights
to test their hypothesis mcrae and hetherington  compared the performance of a nave and pretrained autoencoder backpropagation network on three simulations of verbal learning tasks
the pretrained network was trained using letter based representations of english monosyllabic words or english word pairs
all three tasks involved the learning of some consonantvowelconsonant cvc strings or cvc pairs list a followed by training on a second list of these items list b
afterwards the distributions of the hidden node activations were compared between the nave and pretrained network
in all three tasks the representations of a cvc in the nave network tended to be spread fairly evenly across all hidden nodes whereas most hidden nodes were inactive in the pretrained network
furthermore in the pretrained network the representational overlap between cvcs was reduced compared to the nave network
the pretrained network also retained some similarity information as the representational overlap between similar cvcs like jep and zep was greater than for dissimilar cvcs such as jep and yug
this suggests that the pretrained network had a better ability to generalize i
notice the patterns than the nave network
most importantly this reduction in hidden unit activation and representational overlap resulted in significantly less forgetting in the pretrained network than the nave network essentially eliminating catastrophic interference
essentially the pretraining acted to create internal orthogonalization of the activations at the hidden layer which reduced interference
thus pretraining is a simple way to reduce catastrophic forgetting in standard backpropagation networks
french  proposed the idea of a pseudorecurrent backpropagation network in order to help reduce catastrophic interference see figure
in this model the network is separated into two functionally distinct but interacting subnetworks
this model is biologically inspired and is based on research from mcclelland mcnaughton and oreilly
in this research mcclelland et al
suggested that the hippocampus and neocortex act as separable but complementary memory systems
specifically the hippocampus short term memory storage and acts gradually over time to transfer memories into the neocortex for long term memory storage
they suggest that the information that is stored can be brought back to the hippocampus during active rehearsal reminiscence and sleep and renewed activation is what acts to transfer the information to the neocortex over time
in the pseudorecurrent network one of the subnetworks acts as an early processing area akin to the hippocampus and functions to learn new input patters
the other subnetwork acts as a finalstorage area akin to the neocortex
however unlike in mcclelland et al
model the finalstorage area sends internally generated representation back to the early processing area
this creates a recurrent network
french proposed that this interleaving of old representations with new representations is the only way to reduce radical forgetting
since the brain would most likely not have access to the original input patterns the patterns that would be fed back to the neocortex would be internally generated representations called pseudopatterns
these pseudopatterns are approximations of previous inputs and they can be interleaved with the learning of new inputs
the use of these pseudopatterns could be biologically plausible as parallels between the consolidation of learning that occurs during sleep and the use of interleaved pseudopatterns
specifically they both serve to integrate new information with old information without disruption of the old information
when given an input and a teacher value is fed into the pseudorecurrent network would act as followswhen tested on sequential learning of real world patterns categorization of edible and poisonous mushrooms the pseudorecurrent network was shown less interference than a standard backpropagation network
this improvement was with both memory savings and exact recognition of old patterns
when the activation patterns of the pseudorecurrent network were investigated it was shown that this network automatically formed semidistributed representations
since these types of representations involve fewer nodes being activated for each pattern it is likely what helped to reduce interference
not only did the pseudorecurrent model show reduced interference but also it models listlength and liststrength effects seen in humans
the listlength effect means that adding new items to a list harms the memory of earlier items
like humans the pseudo recurrent network showed a more gradual forgetting when to be trained list is lengthened
the liststrength effect means that when the strength of recognition for one item is increased there is no effect on the recognition of the other list items
this is an important finding as other models often exhibit a decrease in the recognition of other list items when one list item is strengthened
since the direct copying of weights from the early processing area to the final storage area does not seem highly biologically plausible the transfer of information to the final storage area can be done through training the final storage area with pseudopatterns created by the early processing area
however a disadvantage of the pseudorecurrent model is that the number of hidden units in the early processing and final storage subnetworks must be identical
following the same basic idea contributed by robins ans and rousset  have also proposed a twonetwork artificial neural architecture with memory selfrefreshing that overcomes catastrophic interference when sequential learning tasks are carried out in distributed networks trained by backpropagation
the principle is to interleave at the time when new external patterns are learned those tobelearned new external patterns with internally generated pseudopatterns or pseudomemories that reflect the previously learned information
what mainly distinguishes this model from those that use classical pseudorehearsal in feedforward multilayer networks is a reverberating process that is used for generating pseudopatterns
this process which after a number of activity reinjections from a single random seed tends to go up to nonlinear network attractors is more suitable for optimally capturing the deep structure of previously learned knowledge than a single feedforward pass of random activation
ans and rousset  have shown that the learning mechanism they proposed avoiding catastrophic forgetting provides a more appropriate way to deal with knowledge transfer as measured by learning speed ability to generalize and vulnerability to network damages
musca rousset and ans  have also shown that pseudopatterns originating from an artificial reverberating neural network could induce familiarity in humans with never seen items in the way predicted by simulations conducted with a twonetwork artificial neural architecture
furthermore ans  has implemented a version of the selfrefreshing mechanism using only one network trained by the contrastive hebbian learning rule a training rule considered as more realistic than the largely used backpropagation algorithm but fortunately equivalent to the latter
so far the different solutions to catastrophic interference that have been presented concern tasks of sequential learning involving only nontemporally ordered lists of items
but to be credible the selfrefreshing mechanism for static learning has to encompass our human ability to learn serially many temporal sequences of patterns without catastrophic interference e
learning one song followed by learning a second song without forgetting the first one
this was done by ans rousset french and musca  who have presented in addition to simulation work an experiment that evidences a close similarity between the behaviour of humans and the behaviour of the proposed neuromimetic architecture
latent learning is a technique used by gutstein  stump  both to mitigate catastrophic interference and to take advantage of transfer learning
rather than manipulating the representations for new classes used by the hidden nodes this approach tries to train optimal representations for new classes into the output nodes
it chooses output encodings that are least likely to catastrophically interfere with existing responses
given a net that has learned to discriminate among one set of classes using error correcting output codes ecoc as opposed to  hot codes optimal encodings for new classes are chosen by observing the nets average responses to them
since these average responses arose while learning the original set of classes without any exposure to the new classes they are referred to as latently learned encodings
this terminology borrows from the concept of latent learning as introduced by tolman in
in effect this technique uses transfer learning to avoid catastrophic interference by making a nets responses to new classes as consistent as possible with existing responses to classes already learned
kirkpatrick et al
demonstrated a method to train a single artificial neural network on multiple tasks using a technique called elastic weight consolidation
practopoietic theory proposes that biological systems solve the problem of catastrophic interference by storing longterm memories only in a general form not applicable to a given situation but instead loosely applicable to a class of different situations
in order to adjust the loosely applicable knowledge to the given current situation the process of anapoiesis is applied
anapoiesis stands for reconstruction of knowledgetransforming knowledge from a general form to a specific one
practopoietic theory is founded in the theorems of cybernetics and is concerned with the question of how cybernetic systems obtain their capabilities to control and act
in computer science a predictive state representation psr is a way to model a state of controlled dynamical system from a history of actions taken and resulting observations
psr captures the state of a system as a vector of predictions for future tests experiments that can be done on the system
a test is a sequence of actionobservation pairs and its prediction is the probability of the tests observationsequence happening if the tests actionsequence were to be executed on the system
one of the advantage of using psr is that the predictions are directly related to observable quantities
this is in contrast to other models of dynamical systems such as partially observable markov decision processes pomdps where the state of the system is represented as a probability distribution over unobserved nominal states
the lifetime of correlation measures the timespan over which there is appreciable autocorrelation or cross correlation in stochastic processes
the correlation coefficient  expressed as an autocorrelation function or crosscorrelation function depends on the lagtime between the times being considered
typically such functions t decay to zero with increasing lagtime but they can assume values across all levels of correlations strong and weak and positive and negative as in the table
the lifetime of a correlation is defined as the length of time when the correlation coefficient is at the strong level
the durability of correlation is determined by signal the strong level of correlation is separated from weak and negative levels
the mean lifetime of correlation could measure how the durability of correlation depends on the window width size the window is the length of time series used to calculate correlation
in statistics and in machine learning a linear predictor function is a linear function linear combination of a set of coefficients and explanatory variables independent variables whose value is used to predict the outcome of a dependent variable
this sort of function usually comes in linear regression where the coefficients are called regression coefficients
however they also occur in various types of linear classifiers e
logistic regression perceptrons support vector machines and linear discriminant analysis as well as in various other models such as principal component analysis and factor analysis
in many of these models the coefficients are referred to as weights
the basic form of a linear predictor function formula for data point i consisting of p explanatory variables for i
n iswhere formula are the coefficients regression coefficients weights etc
indicating the relative effect of a particular explanatory variable on the outcome
it is common to write the predictor function in a more compact form as followsthis makes it possible to write the linear predictor function as followsusing the notation for a dot product between two vectors
an equivalent form using matrix notation is as followswhere formula and formula are assumed to be a pby column vectors formula is the matrix transpose of formula so formula is a byp row vector and formula indicates matrix multiplication between the byp row vector and the pby column vector producing a by matrix that is taken to be a scalar
an example of the usage of a linear predictor function is in linear regression where each data point is associated with a continuous outcome y and the relationship writtenwhere formula is a disturbance term or error variable  an unobserved random variable that adds noise to the linear relationship between the dependent variable and predictor function
in some models standard linear regression in particular the equations for each of the data points i
n are stacked together and written in vector form aswherethe matrix x is known as the design matrix and encodes all known information about the independent variables
the variables formula are random variables which in standard linear regression are distributed according to a standard normal distribution they express the influence of any unknown factors on the outcome
this makes it possible to find optimal coefficients through the method of least squares using simple matrix operations
in particular the optimal coefficients formula as estimated by least squares can be written as followsthe matrix formula is known as the moorepenrose pseudoinverse of x
the use of the matrix inverse in this formula requires that x is of full rank i
there is not perfect multicollinearity among different explanatory variables i
no explanatory variable can be perfectly predicted from the others
in such cases the singular value decomposition can be used to compute the pseudoinverse
although the outcomes dependent variables to be predicted are assumed to be random variables the explanatory variables themselves are usually not assumed to be random
instead they are assumed to be fixed values and any random variables e
the outcomes are assumed to be conditional on them
as a result the data analyst is free to transform the explanatory variables in arbitrary ways including creating multiple copies of a given explanatory variable each transformed using a different function
other common techniques are to create new explanatory variables in the form of interaction variables by taking products of two or sometimes more existing explanatory variables
when a fixed set of nonlinear functions are used to transform the values of a data point these functions are known as basis functions
an example is polynomial regression which uses a linear predictor function to fit an arbitrary degree polynomial relationship up to a given order between two sets of data points i
a single realvalued explanatory variable and a related realvalued dependent variable by adding multiple explanatory variables corresponding to various powers of the existing explanatory variable
mathematically the form looks like thisin this case for each data point i a set of explanatory variables is created as followsand then standard linear regression is run
the basis functions in this example would bethis example shows that a linear predictor function can actually be much more powerful than it first appears it only really needs to be linear in the coefficients
all sorts of nonlinear functions of the explanatory variables can be fit by the model
there is no particular need for the inputs to basis functions to be univariate or singledimensional or their outputs for that matter although in such a case a kdimensional output value is likely to be treated as k separate scalaroutput basis functions
an example of this is radial basis functions rbfs which compute some transformed version of the distance to some fixed pointan example is the gaussian rbf which has the same functional form as the normal distributionwhich drops off rapidly as the distance from c increases
a possible usage of rbfs is to create one for every observed data point
this means that the result of an rbf applied to a new data point will be close to  unless the new point is near to the point around which the rbf was applied
that is the application of the radial basis functions will pick out the nearest point and its regression coefficient will dominate
the result will be a form of nearest neighbor interpolation where predictions are made by simply using the prediction of the nearest observed data point possibly interpolating between multiple nearby data points when they are all similar distances away
this type of nearest neighbor method for prediction is often considered diametrically opposed to the type of prediction used in standard linear regression but in fact the transformations that can be applied to the explanatory variables in a linear predictor function are so powerful that even the nearest neighbor method can be implemented as a type of linear regression
it is even possible to fit some functions that appear nonlinear in the coefficients by transforming the coefficients into new coefficients that do appear linear
for example a function of the form formula for coefficients formula could be transformed into the appropriate linear function by applying the substitutions formula leading to formula which is linear
linear regression and similar techniques could be applied and will often still find the optimal coefficients but their error estimates and such will be wrong
the explanatory variables may be of any type realvalued binary categorical etc
the main distinction is between continuous variables e
income age blood pressure etc
and discrete variables e
sex race political party etc
discrete variables referring to more than two possible choices are typically coded using dummy variables or indicator variables i
separate explanatory variables taking the value  or  are created for each possible value of the discrete variable with a  meaning variable does have the given value and a  meaning variable does not have the given value
for example a fourway discrete variable of blood type with the possible values a b ab o would be converted to separate twoway dummy variables isa isb isab iso where only one of them has the value  and all the rest have the value
this allows for separate regression coefficients to be matched for each possible value of the discrete variable
note that for k categories not all k dummy variables are independent of each other
for example in the above blood type example only three of the four dummy variables are independent in the sense that once the values of three of the variables are known the fourth is automatically determined
thus its really only necessary to encode three of the four possibilities as dummy variables and in fact if all four possibilities are encoded the overall model becomes nonidentifiable
this causes problems for a number of methods such as the simple closedform solution used in linear regression
the solution is either to avoid such cases by eliminating one of the dummy variables andor introduce a regularization constraint which necessitates a more powerful typically iterative method for finding the optimal coefficients
inferential theory of learning itl is an area of machine learning which describes inferential processes performed by learning agents
itl has been developed by ryszard s
michalski in s
in itl learning process is viewed as a search inference through hypotheses space guided by a specific goal
results of learning need to be stored in order to be used in the future

In statistical learning theory, a learnable function class is a set of functions for which an algorithm can be devised to asymptotically minimize the expected risk, uniformly over all probability distributions. The concept of learnable classes are closely related to regularization in machine learning, and provides large sample justifications for certain learning algorithms.

Let formula_1 be the sample space, where formula_2 are the labels and formula_3 are the covariates (predictors). formula_4 is a collection of mappings (functions) under consideration to link formula_3 to formula_2. formula_7 is a pre-given loss function (usually non-negative). Given a probability distribution formula_8 on formula_9, define the expected risk formula_10 to be:
The general goal in statistical learning is to find the function in formula_12 that minimizes the expected risk. That is, to find solutions to the following problem:
But in practice the distribution formula_14 is unknown, and any learning task can only be based on finite samples. Thus we seek instead to find an algorithm that asymptotically minimizes the empirical risk, i.e., to find a sequence of functions formula_15 that satisfies
One usual algorithm to find such a sequence is through empirical risk minimization.

We can make the condition given in the above equation stronger by requiring that the convergence is uniform for all probability distributions. That is:

The intuition behind the more strict requirement is as such: the rate at which sequence formula_17 converges to the minimizer of the expected risk can be very different for different formula_8. Because in real world the true distribution formula_14 is always unknown, we would want to select a sequence that performs well under all cases.

However, by the no free lunch theorem, such a sequence that satisfies () does not exist if formula_12 is too complex. This means we need to be careful and not allow too "many" functions in formula_12 if we want () to be a meaningful requirement. Specifically, function classes that ensure the existence of a sequence formula_17 that satisfies () are known as learnable classes.

It is worth noting that at least for supervised classification and regression problems, if a function class is learnable, then the empirical risk minimization automatically satisfies (). Thus in these settings not only do we know that the problem posed by () is solvable, we also immediately have an algorithm that gives the solution.

If the true relationship between formula_2 and formula_3 is formula_25, then by selecting the appropriate loss function, formula_26 can always be expressed as the minimizer of the expected loss across all possible functions. That is,

Here we let formula_28 be the collection of all possible functions mapping formula_29 onto formula_30. formula_26 can be interpreted as the actual data generating mechanism. However, the no free lunch theorem tells us that in practice, with finite samples we cannot hope to search for the expected risk minimizer over formula_28. Thus we often consider a subset of formula_28, formula_12, to carry out searches on. By doing so, we risk that formula_26 might not be an element of formula_12. This tradeoff can be mathematically expressed as

In the above decomposition, part formula_37 does not depend on the data and is non-stochastic. It describes how far away our assumptions (formula_12) are from the truth (formula_28). formula_37 will be strictly greater than 0 if we make assumptions that are too strong (formula_12 too small). On the other hand, failing to put enough restrictions on formula_12 will cause it to be not learnable, and part formula_43 will not stochastically converge to 0. This is the well-known overfitting problem in statistics and machine learning literature.

A good example where learnable classes are used is the so-called Tikhonov regularization in reproducing kernel Hilbert space (RKHS). Specifically, let formula_44 be an RKHS, and formula_45 be the norm on formula_44 given by its inner product. It is shown in that formula_47 is a learnable class for any finite, positive formula_48. The empirical minimization algorithm to the dual form of this problem is

This was first introduced by Tikhonov to solve ill-posed problems. Many statistical learning algorithms can be expressed in such a form (for example, the well-known ridge regression).

The tradeoff between formula_43 and formula_37 in () is geometrically more intuitive with Tikhonov regularization in RKHS. We can consider a sequence of formula_52, which are essentially balls in formula_44 with centers at 0. As formula_48 gets larger, formula_55 gets closer to the entire space, and formula_37 is likely to become smaller. However we will also suffer smaller convergence rates in formula_43. The way to choose an optimal formula_48 in finite sample settings is usually through cross-validation.

Part formula_43 in () is closely linked to empirical process theory in statistics, where the empirical risk formula_60 are known as empirical processes. In this field, the function class formula_12 that satisfies the stochastic convergence

are known as uniform Glivenkoâ€“Cantelli classes. It has been shown that under certain regularity conditions, learnable classes and uniformly Glivenko-Cantelli classes are equivalent. Interplay between formula_43 and formula_37 in statistics literature is often known as the bias-variance tradeoff.

However, note that in the authors gave an example of stochastic convex optimization for General Setting of Learning where learnability is not equivalent with uniform convergence.

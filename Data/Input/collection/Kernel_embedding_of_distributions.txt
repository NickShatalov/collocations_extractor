In machine learning, the kernel embedding of distributions (also called the kernel mean or mean map) comprises a class of nonparametric methods in which a probability distribution is represented as an element of a reproducing kernel Hilbert space (RKHS). A generalization of the individual data-point feature mapping done in classical kernel methods, the embedding of distributions into infinite-dimensional feature spaces can preserve all of the statistical features of arbitrary distributions, while allowing one to compare and manipulate distributions using Hilbert space operations such as inner products, distances, projections, linear transformations, and spectral analysis. This learning framework is very general and can be applied to distributions over any space formula_1 on which a sensible kernel function (measuring similarity between elements of formula_1) may be defined. For example, various kernels have been proposed for learning from data which are: vectors in formula_3, discrete classes/categories, strings, graphs/networks, images, time series, manifolds, dynamical systems, and other structured objects. The theory behind kernel embeddings of distributions has been primarily developed by Alex Smola, Le Song , Arthur Gretton, and Bernhard Schölkopf. A review of recent works on kernel embedding of distributions can be found in .

The analysis of distributions is fundamental in machine learning and statistics, and many algorithms in these fields rely on information theoretic approaches such as entropy, mutual information, or Kullback–Leibler divergence. However, to estimate these quantities, one must first either perform density estimation, or employ sophisticated space-partitioning/bias-correction strategies which are typically infeasible for high-dimensional data. Commonly, methods for modeling complex distributions rely on parametric assumptions that may be unfounded or computationally challenging (e.g. Gaussian mixture models), while nonparametric methods like kernel density estimation (Note: the smoothing kernels in this context have a different interpretation than the kernels discussed here) or characteristic function representation (via the Fourier transform of the distribution) break down in high-dimensional settings.

Methods based on the kernel embedding of distributions sidestep these problems and also possess the following advantages: 
Thus, learning via the kernel embedding of distributions offers a principled drop-in replacement for information theoretic approaches and is a framework which not only subsumes many popular methods in machine learning and statistics as special cases, but also can lead to entirely new learning algorithms.

Let formula_4 denote a random variable with codomain formula_5 and distribution formula_6. Given a kernel formula_7 on formula_8, the Moore-Aronszajn Theorem asserts the existence of a RKHS formula_9 (a Hilbert space of functions formula_10 equipped with inner products formula_11 and norms formula_12) in which the element formula_13 satisfies the reproducing property formula_14. One may alternatively consider formula_15 an implicit feature mapping formula_16 from formula_5 to formula_9 (which is therefore also called the feature space), so that formula_19 can be viewed as a measure of similarity between points formula_20. While the similarity measure is linear in the feature space, it may be highly nonlinear in the original space depending on the choice of kernel.

The kernel embedding of the distribution formula_6 in formula_9 (also called the kernel mean or mean map) is given by:

If formula_24 allows a square integrable density formula_25, then formula_26, where formula_27 is the Hilbert–Schmidt integral operator. A kernel is "characteristic" if the mean embedding formula_28 is injective. Each distribution can thus be uniquely represented in the RKHS and all statistical features of distributions are preserved by the kernel embedding if a characteristic kernel is used.

Given formula_29 training examples formula_30 drawn independently and identically distributed (i.i.d.) from formula_31, the kernel embedding of formula_31 can be empirically estimated as

If formula_34 denotes another random variable (for simplicity, assume the co-domain of formula_34 is also formula_5 with the same kernel formula_7 which satisfies formula_38), then the joint distribution formula_39 can be mapped into a tensor product feature space formula_40 via 

By the equivalence between a tensor and a linear map, this joint embedding may be interpreted as an uncentered cross-covariance operator formula_42 from which the cross-covariance of mean-zero functions formula_43 can be computed as 

Given formula_29 pairs of training examples formula_46 drawn i.i.d. from formula_31, we can also empirically estimate the joint distribution kernel embedding via

Given a conditional distribution formula_49, one can define the corresponding RKHS embedding as 
Note that the embedding of formula_49 thus defines a family of points in the RKHS indexed by the values formula_52 taken by conditioning variable formula_53. By fixing formula_54 to a particular value, we obtain a single element in formula_55, and thus it is natural to define the operator
which given the feature mapping of formula_58 outputs the conditional embedding of formula_59 given formula_60. Assuming that for all formula_61, it can be shown that 
This assumption is always true for finite domains with characteristic kernels, but may not necessarily hold for continuous domains. Nevertheless, even in cases where the assumption fails, formula_63 may still be used to approximate the conditional kernel embedding formula_64, and in practice, the inversion operator is replaced with a regularized version of itself formula_65 (where formula_66 denotes the identity matrix).

Given training examples formula_67, the empirical kernel conditional embedding operator may be estimated as 
where formula_69 are implicitly formed feature matrices, formula_70 is the Gram matrix for samples of formula_54, and formula_72 is a regularization parameter needed to avoid overfitting.

Thus, the empirical estimate of the kernel conditional embedding is given by a weighted sum of samples of formula_59 in the feature space:




on compact subsets of formula_108 is universal.

and support of formula_112 is an entire space, then formula_104 is universal. For example, Gaussian RBF is universal, sinc kernel is not universal.



This section illustrates how basic probabilistic rules may be reformulated as (multi)linear algebraic operations in the kernel embedding framework and is primarily based on the work of Song et al. The following notation is adopted: 

In practice, all embeddings are empirically estimated from data formula_133 and it assumed that a set of samples formula_134 may be used to estimate the kernel embedding of the prior distribution formula_135.

In probability theory, the marginal distribution of formula_4 can be computed by integrating out formula_34 from the joint density (including the prior distribution on formula_34)
The analog of this rule in the kernel embedding framework states that formula_140, the RKHS embedding of formula_141, can be computed via
In practical implementations, the kernel sum rule takes the following form
where formula_146 is the empirical kernel embedding of the prior distribution, formula_147, formula_148, and formula_149 are Gram matrices with entries formula_150 respectively.

In probability theory, a joint distribution can be factorized into a product between conditional and marginal distributions 
The analog of this rule in the kernel embedding framework states that formula_152, the joint embedding of formula_153, can be factorized as a composition of conditional embedding operator with the auto-covariance operator associated with formula_135
In practical implementations, the kernel chain rule takes the following form

In probability theory, a posterior distribution can be expressed in terms of a prior distribution and a likelihood function as 
The analog of this rule in the kernel embedding framework expresses the kernel embedding of the conditional distribution in terms of conditional embedding operators which are modified by the prior distribution
In practical implementations, the kernel Bayes' rule takes the following form
where formula_164.
Two regularization parameters are used in this framework: formula_165 for the estimation of formula_166 and formula_167 for the estimation of the final conditional embedding operator formula_168. The latter regularization is done on square of formula_169 because formula_170 may not be positive definite.

The maximum mean discrepancy (MMD) is a distance-measure between distributions formula_171 and formula_172 which is defined as the squared distance between their embeddings in the RKHS 
While most distance-measures between distributions such as the widely used Kullback–Leibler divergence either require density estimation (either parametrically or nonparametrically) or space partitioning/bias correction strategies, the MMD is easily estimated as an empirical mean which is concentrated around the true value of the MMD. The characterization of this distance as the "maximum mean discrepancy" refers to the fact that computing the MMD is equivalent to finding the RKHS function that maximizes the difference in expectations between the two probability distributions 

Given "n" training examples from formula_171 and "m" samples from formula_172, one can formulate a test statistic based on the empirical estimate of the MMD
to obtain a two-sample test of the null hypothesis that both samples stem from the same distribution (i.e. formula_178) against the broad alternative formula_179.

Although learning algorithms in the kernel embedding framework circumvent the need for intermediate density estimation, one may nonetheless use the empirical embedding to perform density estimation based on "n" samples drawn from an underlying distribution formula_180. This can be done by solving the following optimization problem 
where the maximization is done over the entire space of distributions on formula_5. Here, formula_184 is the kernel embedding of the proposed density formula_185 and formula_186 is an entropy-like quantity (e.g. Entropy, KL divergence, Bregman divergence). The distribution which solves this optimization may be interpreted as a compromise between fitting the empirical kernel means of the samples well, while still allocating a substantial portion of the probability mass to all regions of the probability space (much of which may not be represented in the training examples). In practice, a good approximate solution of the difficult optimization may be found by restricting the space of candidate densities to a mixture of "M" candidate distributions with regularized mixing proportions. Connections between the ideas underlying Gaussian processes and conditional random fields may be drawn with the estimation of conditional probability distributions in this fashion, if one views the feature mappings associated with the kernel as sufficient statistics in generalized (possibly infinite-dimensional) exponential families.

A measure of the statistical dependence between random variables formula_4 and formula_34 (from any domains on which sensible kernels can be defined) can be formulated based on the Hilbert–Schmidt Independence Criterion 
and can be used as a principled replacement for mutual information, Pearson correlation or any other dependence measure used in learning algorithms. Most notably, HSIC can detect arbitrary dependencies (when a characteristic kernel is used in the embeddings, HSIC is zero if and only if the variables are independent), and can be used to measure dependence between different types of data (e.g. images and text captions). Given "n" i.i.d. samples of each random variable, a simple parameter-free unbiased estimator of HSIC which exhibits concentration about the true value can be computed in formula_190 time, where the Gram matrices of the two datasets are approximated using formula_191 with formula_192. The desirable properties of HSIC have led to the formulation of numerous algorithms which utilize this dependence measure for a variety of common machine learning tasks such as: feature selection (BAHSIC ), clustering (CLUHSIC ), and dimensionality reduction (MUHSIC ).

HSIC can be extended to measure the dependence of multiple random variables. The question of when HSIC captures independence in this case has recently been studied : for 
more than two variables

Belief propagation is a fundamental algorithm for inference in graphical models in which nodes repeatedly pass and receive messages corresponding to the evaluation of conditional expectations. In the kernel embedding framework, the messages may be represented as RKHS functions and the conditional distribution embeddings can be applied to efficiently compute message updates. Given "n" samples of random variables represented by nodes in a Markov Random Field, the incoming message to node "t" from node "u" can be expressed as formula_194 if it assumed to lie in the RKHS. The kernel belief propagation update message from "t" to node "s" is then given by 
where formula_196 denotes the element-wise vector product, formula_197 is the set of nodes connected to "t" excluding node "s", formula_198, formula_199 are the Gram matrices of the samples from variables formula_200, respectively, and formula_201 is the feature matrix for the samples from formula_202.

Thus, if the incoming messages to node "t" are linear combinations of feature mapped samples from formula_203, then the outgoing message from this node is also a linear combination of feature mapped samples from formula_204. This RKHS function representation of message-passing updates therefore produces an efficient belief propagation algorithm in which the potentials are nonparametric functions inferred from the data so that arbitrary statistical relationships may be modeled.

In the hidden Markov model (HMM), two key quantities of interest are the transition probabilities between hidden states formula_205 and the emission probabilities formula_206 for observations. Using the kernel conditional distribution embedding framework, these quantities may be expressed in terms of samples from the HMM. A serious limitation of the embedding methods in this domain is the need for training samples containing hidden states, as otherwise inference with arbitrary distributions in the HMM is not possible.

One common use of HMMs is filtering in which the goal is to estimate posterior distribution over the hidden state formula_207 at time step "t" given a history of previous observations formula_208 from the system. In filtering, a belief state formula_209 is recursively maintained via a prediction step (where updates formula_210 are computed by marginalizing out the previous hidden state) followed by a conditioning step (where updates formula_211 are computed by applying Bayes' rule to condition on a new observation). The RKHS embedding of the belief state at time "t+1" can be recursively expressed as 
by computing the embeddings of the prediction step via the kernel sum rule and the embedding of the conditioning step via kernel Bayes' rule. Assuming a training sample formula_213 is given, one can in practice estimate formula_214 and filtering with kernel embeddings is thus implemented recursively using the following updates for the weights formula_215 
where formula_218 denote the Gram matrices of formula_219 and formula_220 respectively, formula_221 is a transfer Gram matrix defined as formula_222, and formula_223.

The support measure machine (SMM) is a generalization of the support vector machine (SVM) in which the training examples are probability distributions paired with labels formula_224. 
SMMs solve the standard SVM dual optimization problem using the following expected kernel
which is computable in closed form for many common specific distributions formula_226 (such as the Gaussian distribution) combined with popular embedding kernels formula_104 (e.g. the Gaussian kernel or polynomial kernel), or can be accurately empirically estimated from i.i.d. samples formula_228 via
Under certain choices of the embedding kernel formula_104, the SMM applied to training examples formula_231 is equivalent to a SVM trained on samples formula_232, and thus the SMM can be viewed as a "flexible" SVM in which a different data-dependent kernel (specified by the assumed form of the distribution formula_226) may be placed on each training point.

The goal of domain adaptation is the formulation of learning algorithms which generalize well when the training and test data have different distributions. Given training examples formula_234 and a test set formula_235 where the formula_236 are unknown, three types of differences are commonly assumed between the distribution of the training examples formula_237 and the test distribution formula_238:

By utilizing the kernel embedding of marginal and conditional distributions, practical approaches to deal with the presence of these types of differences between training and test domains can be formulated. Covariate shift may be accounted for by reweighting examples via estimates of the ratio formula_245 obtained directly from the kernel embeddings of the marginal distributions of formula_54 in each domain without any need for explicit estimation of the distributions. Target shift, which cannot be similarly dealt with since no samples from formula_59 are available in the test domain, is accounted for by weighting training examples using the vector formula_248 which solves the following optimization problem (where in practice, empirical approximations must be used) 

To deal with location scale conditional shift, one can perform a LS transformation of the training points to obtain new transformed training data formula_251 (where formula_252 denotes the element-wise vector product). To ensure similar distributions between the new transformed training samples and the test data, formula_253 are estimated by minimizing the following empirical kernel embedding distance 
In general, the kernel embedding methods for dealing with LS conditional shift and target shift may be combined to find a reweighted transformation of the training data which mimics the test distribution, and these methods may perform well even in the presence of conditional shifts other than location-scale changes.

Given "N" sets of training examples sampled i.i.d. from distributions formula_255, the goal of domain generalization is to formulate learning algorithms which perform well on test examples sampled from a previously unseen domain formula_256 where no data from the test domain is available at training time. If conditional distributions formula_257 are assumed to be relatively similar across all domains, then a learner capable of domain generalization must estimate a functional relationship between the variables which is robust to changes in the marginals formula_171. Based on kernel embeddings of these distributions, Domain Invariant Component Analysis (DICA) is a method which determines the transformation of the training data that minimizes the difference between marginal distributions while preserving a common conditional distribution shared between all training domains. DICA thus extracts "invariants", features that transfer across domains, and may be viewed as a generalization of many popular dimension-reduction methods such as kernel principal component analysis, transfer component analysis, and covariance operator inverse regression. 
Defining a probability distribution formula_259 on the RKHS formula_55 with formula_261, DICA measures dissimilarity between domains via distributional variance which is computed as 
so formula_264 is a formula_265 Gram matrix over the distributions from which the training data are sampled. Finding an orthogonal transform onto a low-dimensional subspace "B" (in the feature space) which minimizes the distributional variance, DICA simultaneously ensures that "B" aligns with the bases of a central subspace "C" for which formula_59 becomes independent of formula_54 given formula_268 across all domains. In the absence of target values formula_59, an unsupervised version of DICA may be formulated which finds a low-dimensional subspace that minimizes distributional variance while simultaneously maximizing the variance of formula_54 (in the feature space) across all domains (rather than preserving a central subspace).

In distribution regression, the goal is to regress from probability distributions to reals (or vectors). Many important machine learning and statistical tasks fit into this framework, including multi-instance learning, and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). In practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between "sets of points". Distribution regression has been successfully applied for example in supervised entropy learning, and aerosol prediction using multispectral satellite images.

Given formula_271 training data, where the formula_272 bag contains samples from a probability distribution formula_273 and the formula_274 output label is formula_275, one can tackle the distribution regression task by taking the embeddings of the distributions, and learning the regressor from the embeddings to the outputs. In other words, one can consider the following kernel ridge regression problem formula_276
where formula_278 with a formula_104 kernel on the domain of formula_273-s formula_281, formula_282 is a kernel on the embedded distributions, and formula_283 is the RKHS determined by formula_282. Examples for formula_282 include the linear kernel formula_286, the Gaussian kernel formula_287, the exponential kernel formula_288, the Cauchy kernel formula_289, the generalized t-student kernel formula_290, or the inverse multiquadrics kernel formula_291.

The prediction on a new distribution formula_292 takes the simple, analytical form
where formula_294, formula_295, formula_296, formula_297. Under mild regularity conditions this estimator can be shown to be consistent and it can achieve the one-stage sampled (as if one had access to the true formula_273-s) minimax optimal rate. In the formula_299 objective function formula_300-s are real numbers; the results can also be extended to the case when formula_300-s are formula_302-dimensional vectors, or more generally elements of a separable Hilbert space using operator-valued formula_282 kernels.

In this simple example, which is taken from Song et al., formula_304 are assumed to be discrete random variables which take values in the set formula_305 and the kernel is chosen to be the Kronecker delta function, so formula_306. The feature map corresponding to this kernel is the standard basis vector formula_307. The kernel embeddings of such a distributions are thus vectors of marginal probabilities while the embeddings of joint distributions in this setting are formula_308 matrices specifying joint probability tables, and the explicit form of these embeddings is

The conditional distribution embedding operator formula_311 is in this setting a conditional probability table
Thus, the embeddings of the conditional distribution under a fixed value of formula_54 may be computed as

In this discrete-valued setting with the Kronecker delta kernel, the kernel sum rule becomes

The kernel chain rule in this case is given by


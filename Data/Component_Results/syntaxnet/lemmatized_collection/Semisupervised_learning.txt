semisupervised learning be a class of supervise learning task and technique that also make use of unlabeled data for train typically a small amount of label data with a large amount of unlabeled data . semisupervised learn fall between unsupervised learn without any label training data and supervise learning with completely label training data . many machinelearning researcher have find that unlabeled data when use in conjunction with a small amount of label data can produce considerable improvement in learn accuracy . the acquisition of label data for a learning problem often require a skilled human agent e . to transcribe an audio segment or a physical experiment e . determine the d structure of a protein or determine whether there be oil at a particular location . the cost associate with the labeling process thus may render a fully label training set infeasible whereas acquisition of unlabeled data be relatively inexpensive . in such situation semisupervised learn can be of great practical value . semisupervised learning be also of theoretical interest in machine learn and a a model for human learning . a in the supervised learn framework we are give a set of formula independently identically distribute example formula with correspond label formula . additionally we are give formula unlabeled example formula . semisupervised learn attempt to make use of this combined information to surpass the classification performance that could be obtain either by discard the unlabeled data and doing supervise learning or by discard the label and do unsupervised learn . semisupervised learning may refer to either transductive learn or inductive learn . the goal of transductive learn be to infer the correct label for the given unlabeled data formula only . the goal of inductive learn be to infer the correct map from formula to formula . intuitively we can think of the learn problem a an exam and label data a the few example problem that the teacher solve in class . the teacher also provide a set of unsolved problem . in the transductive setting these unsolved problem be a takehome exam and you want to do well on them in particular . in the inductive setting these be practice problem of the sort you will encounter on the inclass exam . it be unnecessary and according to vapniks principle imprudent to perform transductive learn by way of infer a classification rule over the entire input space however in practice algorithm formally design for transduction or induction are often use interchangeably . in order to make any use of unlabeled data we must assume some structure to the underlying distribution of data . semisupervised learn algorithm make use of at least one of the follow assumption . point which be close to each other be more likely to share a label . this be also generally assumed in supervise learn and yield a preference for geometrically simple decision boundary . in the case of semisupervised learn the smoothness assumption additionally yield a preference for decision boundary in lowdensity region so that there be few point close to each other but in different class . the data tend to form discrete cluster and point in the same cluster be more likely to share a label although data share a label may be spread across multiple cluster . this be a special case of the smoothness assumption and give rise to feature learn with cluster algorithm . the data lie approximately on a manifold of much low dimension than the input space . in this case we can attempt to learn the manifold use both the label and unlabeled data to avoid the curse of dimensionality . then learn can proceed use distance and density define on the manifold . the manifold assumption be practical when highdimensional data are being generate by some process that may be hard to model directly but which only have a few degree of freedom . for instance human voice is control by a few vocal fold and image of various facial expression are control by a few muscle . we would like in these case to use distance and smoothness in the natural space of the generating problem rather than in the space of all possible acoustic wave or image respectively . the heuristic approach of selftraining also known a selflearning or selflabeling be historically the oldest approach to semisupervised learn with example of application start in the s see for instance scudder . the transductive learn framework wa formally introduce by vladimir vapnik in the s . interest in inductive learning use generative model also begin in the s . a probably approximately correct learning bound for semisupervised learn of a gaussian mixture be demonstrate by ratsaby and venkatesh in . semisupervised learning ha recently become more popular and practically relevant due to the variety of problem for which vast quantity of unlabeled data be availablee . text on website protein sequence or image . for a review of recent work see a survey article by zhu . generative approach to statistical learn first seek to estimate formula the distribution of data point belong to each class . the probability formula that a given point formula have label formula be then proportional to formula by bayes rule . semisupervised learn with generative model can be view either a an extension of supervise learning classification plus information about formula or a an extension of unsupervised learning clustering plus some label . generative model assume that the distribution take some particular form formula parameterized by the vector formula . if these assumption are incorrect the unlabeled data may actually decrease the accuracy of the solution relative to what would have been obtain from label data alone . however if the assumption be correct then the unlabeled data necessarily improve performance . the unlabeled data are distribute according to a mixture of individualclass distribution . in order to learn the mixture distribution from the unlabeled data it must be identifiable that be different parameter must yield different sum distribution . gaussian mixture distribution be identifiable and commonly use for generative model . the parameterized joint distribution can be write a formula by use the chain rule . each parameter vector formula be associated with a decision function formula . the parameter be then chosen base on fit to both the label and unlabeled data weight by formula . another major class of method attempt to place boundary in region where there be few data point label or unlabeled . one of the most commonly use algorithm be the transductive support vector machine or tsvm which despite it name may be use for inductive learn as well . whereas support vector machine for supervise learn seek a decision boundary with maximal margin over the labeled data the goal of tsvm be a labeling of the unlabeled data such that the decision boundary have maximal margin over all of the data . in addition to the standard hinge loss formula for label data a loss function formula is introduce over the unlabeled data by let formula . tsvm then select formula from a reproduce kernel hilbert space formula by minimize the regularize empirical risk . an exact solution be intractable due to the nonconvex term formula so research ha focus on find useful approximation . other approach that implement lowdensity separation include gaussian process model information regularization and entropy minimization of which tsvm be a special case . graphbased method for semisupervised learn use a graph representation of the data with a node for each label and unlabeled example . the graph may be construct using domain knowledge or similarity of example two common method be to connect each data point to it formula nearest neighbor or to example within some distance formula . the weight formula of an edge between formula and formula be then set to formula . within the framework of manifold regularization . the graph serve a a proxy for the manifold . a term is add to the standard tikhonov regularization problem to enforce smoothness of the solution relative to the manifold in the intrinsic space of the problem as well a relative to the ambient input space . the minimization problem become . where formula be a reproduce kernel hilbert space and formula be the manifold on which the data lie . the regularization parameter formula and formula control smoothness in the ambient and intrinsic space respectively . the graph is use to approximate the intrinsic regularization term . define the graph laplacian formula where formula and formula the vector formula we have . the laplacian can also be use to extend the supervised learn algorithm regularize least square and support vector machine svm to semisupervised version laplacian regularized least square and laplacian svm . some method for semisupervised learning are not intrinsically gear to learn from both unlabeled and label data but instead make use of unlabeled data within a supervise learn framework . for instance the label and unlabeled example formula may inform a choice of representation distance metric or kernel for the data in an unsupervised first step . then supervise learning proceeds from only the labeled example . selftraining be a wrapper method for semisupervised learn . first a supervise learning algorithm is train base on the labeled data only . this classifier be then applied to the unlabeled data to generate more label example a input for the supervised learn algorithm . generally only the label the classifier be most confident of are add at each step . cotraining be an extension of selftraining in which multiple classifier are train on different ideally disjoint set of feature and generate label example for one another . human response to formal semisupervised learn problem have yield vary conclusion about the degree of influence of the unlabeled data for a summary see . more natural learning problem may also be view a instance of semisupervised learn . much of human concept learn involves a small amount of direct instruction e . parental labeling of object during childhood combine with large amount of unlabeled experience e . observation of object without name or count them or at least without feedback . human infant be sensitive to the structure of unlabeled natural category such a image of dog and cat or male and female face . more recent work ha show that infant and child take into account not only the unlabeled example available but the sampling process from which label example arise .
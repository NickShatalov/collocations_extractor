near neighbor search nns a a form of proximity search be the optimization problem of find the point in a given set that be close or most similar to a given point . closeness is typically express in term of a dissimilarity function the le similar the object the larger the function value . formally the nearestneighbor nn search problem is define a follows give a set s of point in a space m and a query point qm find the close point in s to q . donald knuth in vol . of the art of computer programming call it the postoffice problem refer to an application of assign to a residence the near post office . a direct generalization of this problem be a knn search where we need to find the k close point . most commonly m be a metric space and dissimilarity is express a a distance metric which be symmetric and satisfies the triangle inequality . even more common m is take to be the ddimensional vector space where dissimilarity is measure use the euclidean distance manhattan distance or other distance metric . however the dissimilarity function can be arbitrary . one example be asymmetric bregman divergence for which the triangle inequality doe not hold . the near neighbor search problem arises in numerous field of application include . various solution to the nns problem have been propose . the quality and usefulness of the algorithm are determine by the time complexity of query as well a the space complexity of any search data structure that must be maintained . the informal observation usually refer to a the curse of dimensionality state that there be no generalpurpose exact solution for nns in highdimensional euclidean space use polynomial preprocessing and polylogarithmic search time . the simple solution to the nns problem be to compute the distance from the query point to every other point in the database keep track of the best so far . this algorithm sometimes refer to a the naive approach have a run time of odn where n be the cardinality of s and d be the dimensionality of m . there be no search data structure to maintain so linear search have no space complexity beyond the storage of the database . naive search can on average outperform space partition approach on high dimensional space . since the s branch and bound methodology ha be apply to the problem . in the case of euclidean space this approach is know a spatial index or spatial access method . several spacepartitioning method have been develop for solve the nns problem . perhaps the simple be the kd tree which iteratively bisect the search space into two region contain half of the point of the parent region . query are perform via traversal of the tree from the root to a leaf by evaluate the query point at each split . depend on the distance specify in the query neighboring branch that might contain hit may also need to be evaluated . for constant dimension query time average complexity be ologn in the case of randomly distributed point worst case complexity be oknk . alternatively the rtree data structure wa design to support near neighbor search in dynamic context a it have efficient algorithm for insertion and deletion such a the r tree . rtrees can yield near neighbor not only for euclidean distance but can also be use with other distance . in case of general metric space branch and bound approach is know under the name of metric tree . particular example include vptree and bktree . use a set of point take from a dimensional space and put into a bsp tree and give a query point take from the same space a possible solution to the problem of find the near pointcloud point to the query point is give in the follow description of an algorithm . strictly speak no such point may exist because it may not be unique . but in practice usually we only care about find any one of the subset of all pointcloud point that exist at the short distance to a give query point . the idea be for each branching of the tree guess that the close point in the cloud resides in the halfspace contain the query point . this may not be the case but it be a good heuristic . after have recursively gone through all the trouble of solve the problem for the guess halfspace now compare the distance return by this result with the shortest distance from the query point to the partitioning plane . this latter distance be that between the query point and the closest possible point that could exist in the halfspace not search . if this distance be great than that return in the early result then clearly there be no need to search the other halfspace . if there be such a need then you must go through the trouble of solve the problem for the other half space and then compare it result to the former result and then return the proper result . the performance of this algorithm be nearer to logarithmic time than linear time when the query point be near the cloud because a the distance between the query point and the close pointcloud point nears zero the algorithm need only perform a lookup use the query point a a key to get the correct result . an approximation algorithm is allow to return a point whose distance from the query be at most formula time the distance from the query to it near point . the appeal of this approach be that in many case an approximate nearest neighbor be almost as good a the exact one . in particular if the distance measure accurately capture the notion of user quality then small difference in the distance should not matter . locality sensitive hashing lsh be a technique for group point in space into bucket base on some distance metric operate on the point . point that be close to each other under the chosen metric are map to the same bucket with high probability . the cover tree have a theoretical bound that is base on the datasets double constant . the bound on search time be oclogn where c be the expansion constant of the dataset . in the special case where the data be a dense d map of geometric point the projection geometry of the sensing technique can be use to dramatically simplify the search problem . this approach require that the d data is organize by a projection to a two dimensional grid and assume that the data be spatially smooth across neighboring grid cells with the exception of object boundary . these assumption be valid when deal with d sensor data in application such a survey robotics and stereo vision but may not hold for unorganized data in general . in practice this technique have an average search time of o or ok for the knearest neighbor problem when apply to real world stereo vision data . in high dimensional space tree index structure become useless because an increase percentage of the node need to be examined anyway . to speed up linear search a compress version of the feature vector store in ram is use to prefilter the datasets in a first run . the final candidate are determine in a second stage use the uncompress data from the disk for distance calculation . the vafile approach be a special case of a compression base search where each feature component is compress uniformly and independently . the optimal compression technique in multidimensional space be vector quantization vq implement through clustering . the database be clustered and the most promising cluster are retrieve . huge gain over vafile treebased index and sequential scan have been observe . also note the parallel between cluster and lsh . one possible way to solve nns be to construct a graph formula where every point formula be uniquely associated with vertex formula . the search of the point in the set s close to the query q take the form of the search of vertex in the graph formula . one of the basic vertex search algorithm in graph with metric object be the greedy search algorithm . it start from the random vertex formula . the algorithm compute a distance value from the query q to each vertex from the neighborhood formula of the current vertex formula and then select a vertex with the minimal distance value . if the distance value between the query and the select vertex be small than the one between the query and the current element then the algorithm move to the select vertex and it become new current vertex . the algorithm stop when it reach a local minimum a vertex whose neighborhood doe not contain a vertex that be close to the query than the vertex itself . this idea be exploited in multiple publication include the seminal paper by arya and mount in the voronet system for the plane in the raynet system for the formula in the metrized small world algorithm for the general metric space . this work wa precede by a pioneering paper by toussaint where he introduce a concept of a relative neighborhood graph . there be numerous variant of the nns problem and the two most wellknown be the knearest neighbor search and the approximate near neighbor search . knearest neighbor search identify the top k nearest neighbor to the query . this technique is commonly use in predictive analytics to estimate or classify a point base on the consensus of it neighbor . knearest neighbor graph be graph in which every point is connect to it k near neighbor . in some application it may be acceptable to retrieve a good guess of the near neighbor . in those case we can use an algorithm which doesnt guarantee to return the actual nearest neighbor in every case in return for improve speed or memory saving . often such an algorithm will find the near neighbor in a majority of case but this depend strongly on the dataset being query . algorithm that support the approximate near neighbor search include localitysensitive hash best bin first and balanced boxdecomposition tree base search . near neighbor distance ratio do not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depend on the distance to the previous neighbor . it is use in cbir to retrieve picture through a query by example use the similarity between local feature . more generally it be involved in several matching problem . fixedradius near neighbor be the problem where one want to efficiently find all point give in euclidean space within a give fix distance from a specified point . the data structure should work on a distance which is fix however the query point be arbitrary . for some application e . entropy estimation we may have n datapoints and wish to know which be the near neighbor for every one of those n point . this could of course be achieve by run a nearestneighbor search once for every point but an improved strategy would be an algorithm that exploit the information redundancy between these n query to produce a more efficient search . a a simple example when we find the distance from point x to point y that also tell u the distance from point y to point x so the same calculation can be reuse in two different query . give a fixed dimension a semidefinite positive norm thereby include every l norm and n point in this space the near neighbour of every point can be find in onlogn time and the m near neighbour of every point can be find in omnlogn time .
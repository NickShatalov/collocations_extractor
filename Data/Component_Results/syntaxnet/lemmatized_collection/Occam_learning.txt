in computational learning theory occam learning be a model of algorithmic learn where the objective of the learner be to output a succinct representation of receive training data . this be closely related to probably approximately correct pac learn where the learner is evaluate on it predictive power of a test set . occam learnability implies pac learn and for a wide variety of concept class the converse be also true pac learnability implies occam learnability . occam learn is name after occam razor which be a principle stating that give all other thing be equal a shorter explanation for observe data should be favor over a lengthy explanation . the theory of occam learning be a formal and mathematical justification for this principle . it wa first show by blumer et al . that occam learn implies pac learn which be the standard model of learn in computational learning theory . in other word parsimony of the output hypothesis implies predictive power . the succinctness of a concept formula in concept class formula can be express by the length formula of the short bit string that can represent formula in formula . occam learn connects the succinctness of a learning algorithm output to it predictive power on unseen data . let formula and formula be concept class contain target concept and hypothesis respectively . then for constant formula and formula a learning algorithm formula be an formulaoccam algorithm for formula using formula iff give a set formula of formula sample label accord to a concept formula formula output a hypothesis formula such that . where formula be the maximum length of any sample formula . an occam algorithm is call efficient if it run in time polynomial in formula formula and formula we say a concept class formula be occam learnable with respect to a hypothesis class formula if there exist an efficient occam algorithm for formula using formula . occam learnability implies pac learnability a the follow theorem of blumer et al . show . let formula be an efficient formulaoccam algorithm for formula using formula . then there exist a constant formula such that for any formula for any distribution formula give formula sample draw from formula and label according to a concept formula of length formula bits each the algorithm formula will output a hypothesis formula such that formula with probability at least formula . here formula be with respect to the concept formula and distribution formula . this imply that the algorithm formula be also a pac learner for the concept class formula using hypothesis class formula . a slightly more general formulation be a follows . let formula . let formula be an algorithm such that give formula sample draw from a fix but unknown distribution formula and label according to a concept formula of length formula bit each output a hypothesis formula that be consistent with the labeled sample . then there exist a constant formula such that if formula then formula be guaranteed to output a hypothesis formula such that formula with probability at least formula . while the above theorem show that occam learn be sufficient for pac learn it doesnt say anything about necessity . board and pitt show that for a wide variety of concept class occam learn be in fact necessary for pac learn . they prove that for any concept class that is polynomially close under exception list pac learnability imply the existence of an occam algorithm for that concept class . concept class that are polynomially close under exception list include boolean formulas circuit deterministic finite automaton decisionlists decisiontrees and other geometricallydefined concept class . a concept class formula be polynomially close under exception list if there exist a polynomialtime algorithm formula such that when give the representation of a concept formula and a finite list formula of exception output a representation of a concept formula such that the concept formula and formula agree except on the set formula . we first prove the cardinality version . call a hypothesis formula bad if formula where again formula be with respect to the true concept formula and the underlying distribution formula . the probability that a set of sample formula be consistent with formula be at most formula by the independence of the sample . by the union bound the probability that there exist a bad hypothesis in formula be at most formula which be less than formula if formula . this conclude the proof of the second theorem above . use the second theorem we can prove the first theorem . since we have a formulaoccam algorithm this mean that any hypothesis output by formula can be represent by at most formula bit and thus formula . this be less than formula if we set formula for some constant formula . thus by the cardinality version theorem formula will output a consistent hypothesis formula with probability at least formula . this conclude the proof of the first theorem above . though occam and pac learnability be equivalent the occam framework can be use to produce tight bound on the sample complexity of classical problem include conjunction conjunction with few relevant variable and decision list . occam algorithm have also been show to be successful for pac learn in the presence of error probabilistic concepts function learn and markovian nonindependent example .
the sample complexity of a machine learn algorithm represents the number of trainingsamples that it need in order to successfully learn a target function . more precisely the sample complexity be the number of trainingsamples that we need to supply to the algorithm so that the function return by the algorithm be within an arbitrarily small error of the best possible function with probability arbitrarily close to . there be two variant of sample complexity . the no free lunch theorem discuss below prof that in general the strong sample complexity be infinite i . that there be no algorithm that can learn the globallyoptimal target function use a finite number of train sample . however if we be only interested in a particular class of target function e . g only linear function then the sample complexity be finite and it depend linearly on the vc dimension on the class of target function . let formula be a space which we call the input space and formula be a space which we call the output space and let formula denote the product formula . for example in the setting of binary classification formula be typically a finitedimensional vector space and formula be the set formula . fix a hypothesis space formula of function formula . a learning algorithm over formula be a computable map from formula to formula . in other word it be an algorithm that take a input a finite sequence of train sample and output a function from formula to formula . typical learn algorithm include empirical risk minimization without or with tikhonov regularization . fix a loss function formula for example the square loss formula . for a given distribution formula on formula the expect risk of a hypothesis a function formula be . in our setting we have formula where formula be a learning algorithm and formula be a sequence of vector which be all drawn independently from formula . define the optimal riskformulaset formula for each formula . note that . formula be a random variable and depends on the random variable formula which be drawn from the distribution formula . the algorithm formula is call consistent if formula probabilistically converge to formula in other word for all there exist a positive integer n such that for all n n we haveformulathe sample complexity of formula be then the minimum n for which this hold a a function of and . we write the sample complexity a formula to emphasize that this value of n depends on and . if formula be not consistent then we set formula . if there exist an algorithm for which formula be finite then we say that the hypothesis space formula be learnable . in word the sample complexity formula define the rate of consistency of the algorithm give a desire accuracy and confidence one need to sample formula data point to guarantee that the risk of the output function be within of the best possible with probability at least . in probabilistically approximately correct pac learn one is concern with whether the sample complexity be polynomial that be whether formula is bound by a polynomial in and . if formula be polynomial for some learning algorithm then one say that the hypothesis space . formula be paclearnable . note that this be a strong notion than be learnable . one can ask whether there exist a learning algorithm so that the sample complexity be finite in the strong sense that be there be a bound on the number of sample need so that the algorithm can learn any distribution over the inputoutput space with a specified target error . more formally one asks whether there exist a learning algorithm formula such that for all there exist a positive integer n such that for all n n we haveformulawhere formula with formula as above . the no free lunch theorem say that without restriction on the hypothesis space formula this be not the case i . there always exist bad distribution for which the sample complexity be arbitrarily large . thus in order to make statement about the rate of convergence of the quantity . formula . one must either . the latter approach lead to concept such a vc dimension and rademacher complexity which control the complexity of the space formula . a small hypothesis space introduces more bias into the inference process meaning that formula may be great than the best possible risk in a large space . however by restrict the complexity of the hypothesis space it become possible for an algorithm to produce more uniformly consistent function . this tradeoff lead to the concept of regularization . it be a theorem from vc theory that the follow three statement be equivalent for a hypothesis space formula . this give a way to prove that certain hypothesis space be pac learnable and by extension learnable . let x r y and let formula be the space of affine function on x that be function of the form formula for some formula . this be the linear classification with offset learn problem . now note that four coplanar point in a square cannot be shatter by any affine function since no affine function can be positive on two diagonally opposite vertex and negative on the remaining two . thus the vc dimension of formula be formula in particular finite . it follow by the above characterization of paclearnable class that formula be paclearnable and by extension learnable . suppose formula be a class of binary function function to . then formula be formulapaclearnable with a sample of size . formula . where formula be the vc dimension of formula . moreover any formulapaclearning algorithm for formula must have samplecomplexity . formula . thus the samplecomplexity be a linear function of the vc dimension of the hypothesis space . suppose formula be a class of realvalued function with range in t . then formula be formulapaclearnable with a sample of size . formula . where formula be pollard pseudodimension of formula . in addition to the supervised learning setting sample complexity be relevant to semisupervised learn problem include active learn where the algorithm can ask for label to specifically choose input in order to reduce the cost of obtain many label . the concept of sample complexity also show up in reinforcement learn online learn and unsupervised algorithm e . for dictionary learn .
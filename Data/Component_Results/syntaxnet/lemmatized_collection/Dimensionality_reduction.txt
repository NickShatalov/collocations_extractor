in statistic machine learn and information theory dimensionality reduction or dimension reduction be the process of reduce the number of random variable under consideration by obtain a set of principal variable . it can be divide into feature selection and feature extraction . feature selection approach try to find a subset of the original variable also call feature or attribute . there be three strategy the filter strategy e . information gain the wrapper strategy e . search guide by accuracy and the embedded strategy feature are select to add or be remove while build the model base on the prediction error . see also combinatorial optimization problem . in some case data analysis such a regression or classification can be do in the reduced space more accurately than in the original space . feature extraction transform the data in the highdimensional space to a space of few dimension . the data transformation may be linear a in principal component analysis pca but many nonlinear dimensionality reduction technique also exist . for multidimensional data tensor representation can be use in dimensionality reduction through multilinear subspace learn . the main linear technique for dimensionality reduction principal component analysis performs a linear mapping of the data to a lowerdimensional space in such a way that the variance of the data in the lowdimensional representation is maximize . in practice the covariance and sometimes the correlation matrix of the data be constructed and the eigenvectors on this matrix are compute . the eigenvectors that correspond to the large eigenvalue the principal component can now be use to reconstruct a large fraction of the variance of the original data . moreover the first few eigenvectors can often be interpret in term of the largescale physical behavior of the system . the original space with dimension of the number of point ha been reduce with data loss but hopefully retain the most important variance to the space span by a few eigenvectors . nmf decompose a nonnegative matrix to the product of two nonnegative one which ha be a promise tool in field where only nonnegative signal exist . such a astronomy . nmf is well know since the multiplicative update rule by lee seung which ha be continuously develop the inclusion of uncertainty the consideration of missing data and parallel computation sequential construction which lead to the stability and linearity of nmf as well a other update . with a stable component basis during construction and a linear modeling process sequential nmf be able to preserve the flux in direct imaging of circumstellar structure in astromony a one of the method of detect exoplanets especially for the direct imaging of circumstellar disk . in comparison with pca nmf doe not remove the mean of the matrix which lead to unphysical nonnegative flux therefore nmf be able to preserve more information than pca a demonstrate by ren et al . principal component analysis can be employ in a nonlinear way by mean of the kernel trick . the resulting technique be capable of construct nonlinear mapping that maximize the variance in the data . the resulting technique be entitled kernel pca . other prominent nonlinear technique include manifold learning technique such a isomap locally linear embedding lle hessian lle laplacian eigenmaps and local tangent space alignment ltsa . these technique construct a lowdimensional data representation use a cost function that retain local property of the data and can be view a define a graphbased kernel for kernel pca . more recently technique have been propose that instead of define a fix kernel try to learn the kernel using semidefinite programming . the most prominent example of such a technique be maximum variance unfolding mvu . the central idea of mvu be to exactly preserve all pairwise distance between nearest neighbor in the inner product space while maximize the distance between point that be not near neighbor . an alternative approach to neighborhood preservation be through the minimization of a cost function that measure difference between distance in the input and output space . important example of such technique include classical multidimensional scaling which be identical to pca isomap which use geodesic distance in the data space diffusion map which use diffusion distance in the data space tdistributed stochastic neighbor embedding tsne which minimize the divergence between distribution over pair of point and curvilinear component analysis . a different approach to nonlinear dimensionality reduction be through the use of autoencoders a special kind of feedforward neural network with a bottleneck hidden layer . the training of deep encoders is typically perform use a greedy layerwise pretraining e . use a stack of restrict boltzmann machine that is follow by a finetuning stage base on backpropagation . linear discriminant analysis lda be a generalization of fisher linear discriminant a method use in statistic pattern recognition and machine learn to find a linear combination of feature that characterizes or separate two or more class of object or event . gda deal with nonlinear discriminant analysis using kernel function operator . the underlying theory be close to the support vector machine svm insofar a the gda method provide a mapping of the input vectors into highdimensional feature space . similar to lda the objective of gda be to find a projection for the feature into a low dimensional space by maximize the ratio of betweenclass scatter to withinclass scatter . autoencoders can be use to learn nonlinear dimension reduction function and cod together with an inverse function from the coding to the original representation . for highdimensional datasets i . with number of dimension more than dimension reduction is usually perform prior to apply a knearest neighbors algorithm knn in order to avoid the effect of the curse of dimensionality . feature extraction and dimension reduction can be combine in one step use principal component analysis pca linear discriminant analysis lda canonical correlation analysis cca or nonnegative matrix factorization nmf technique a a preprocessing step follow by cluster by knn on feature vector in reduceddimension space . in machine learn this process is also call lowdimensional embed . for veryhighdimensional datasets e . when perform similarity search on live video stream dna data or highdimensional time series run a fast approximate knn search using locality sensitive hashing random projection sketch or other highdimensional similarity search technique from the vldb toolbox might be the only feasible option . a dimensionality reduction technique that be sometimes use in neuroscience be maximally informative dimension which find a lowerdimensional representation of a dataset such that a much information a possible about the original data be preserved .
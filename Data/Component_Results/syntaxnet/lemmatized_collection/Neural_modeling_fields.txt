neural modeling field nmf be a mathematical framework for machine learning which combine idea from neural network fuzzy logic and model base recognition . it ha also been refer to a model field modeling field theory mft maximum likelihood artificial neural network mlans . this framework ha been develop by leonid perlovsky at the afrl . nmf be interpreted a a mathematical description of mind mechanism including concept emotion instincts imagination think and understand . nmf be a multilevel heterohierarchical system . at each level in nmf there be conceptmodels encapsulating the knowledge they generate socalled topdown signal interact with input bottomup signal . these interaction are govern by dynamic equation which drive conceptmodel learning adaptation and formation of new conceptmodels for good correspondence to the input bottomup signal . in the general case nmf system consists of multiple process level . at each level output signal be the concept recognize in or form from input bottomup signal . input signal are associate with or recognize or group into concept accord to the model and at this level . in the process of learn the conceptmodels are adapt for good representation of the input signal so that similarity between the conceptmodels and signal increase . this increase in similarity can be interpret a satisfaction of an instinct for knowledge and be felt a aesthetic emotion . each hierarchical level consists of n neuron enumerate by index n . these neuron receive input bottomup signal xn from low level in the processing hierarchy . xn be a field of bottomup neuronal synaptic activation come from neuron at a lower level . each neuron have a number of synapsis for generality each neuron activation be describe a a set of number . where d be the number or dimension necessary to describe individual neuron activation . topdown or priming signal to these neuron are send by conceptmodels msn . where m be the number of model . each model is characterize by it parameter s in the neuron structure of the brain they are encode by strength of synaptic connection mathematically they are give by a set of number . where a be the number of dimension necessary to describe invividual model . model represent signal in the follow way . suppose that signal xn is come from sensory neuron n activate by object m which is characterize by parameter s . these parameter may include position orientation or lighting of an object m . model msn predicts a value xn of a signal at neuron n . for example during visual perception a neuron n in the visual cortex receive a signal xn from retina and a priming signal msn from an objectconceptmodel m . neuron n is activate if both the bottomup signal from lowerlevelinput and the topdown prim signal are strong . various model compete for evidence in the bottomup signal while adapt their parameter for well match a describe below . this be a simplified description of perception . the most benign everyday visual perception use many level from retina to object perception . the nmf premise be that the same law describe the basic interaction dynamic at each level . perception of minute feature or everyday object or cognition of complex abstract concept be due to the same mechanism described below . perception and cognition involve conceptmodels and learn . in perception conceptmodels correspond to object in cognition model correspond to relationship and situation . learn be an essential part of perception and cognition and in nmf theory it is drive by the dynamic that increase a similarity measure between the set of model and signal lxm . the similarity measure be a function of model parameter and association between the input bottomup signal and topdown conceptmodel signal . in construct a mathematical description of the similarity measure it be important to acknowledge two principle . therefore the similarity measure is construct so that it account for all bottomup signal xn . this expression contain a product of partial similarity lxn over all bottomup signal therefore it force the nmf system to account for every signal even if one term in the product be zero the product be zero the similarity be low and the knowledge instinct be not satisfy this be a reflection of the first principle . second before perception occurs the mind doe not know which object give rise to a signal from a particular retinal neuron . therefore a partial similarity measure is construct so that it treat each model a an alternative a sum over conceptmodels for each input neuron signal . it constituent element be conditional partial similarity between signal xn and model m lxnm . this measure be conditional on object m be present therefore when combine these quantity into the overall similarity measure l they are multiply by rm which represent a probabilistic measure of object m actually be present . combine these element with the two principle note above a similarity measure be constructed a follows . the structure of the expression above follow standard principle of the probability theory a summation is take over alternative m and various piece of evidence n be multiplied . this expression be not necessarily a probability but it have a probabilistic structure . if learn be successful it approximate probabilistic description and lead to nearoptimal bayesian decision . the name conditional partial similarity for lxnm or simply lnm follow the probabilistic terminology . if learn be successful lnm becomes a conditional probability density function a probabilistic measure that signal in neuron n originate from object m . then l be a total likelihood of observe signal xn come from objects describe by conceptmodel m . coefficient rm call prior in probability theory contain preliminary bias or expectation expect object m have relatively high rm value their true value be usually unknown and should be learn like other parameter s . note that in probability theory a product of probability usually assume that evidence be independent . expression for l contains a product over n but it doe not assume independence among various signal xn . there be a dependence among signal due to conceptmodels each model msn predicts expect signal value in many neuron n . during the learning process conceptmodels are constantly modify . usually the functional form of model msn are all fix and learningadaptation involves only model parameter s . from time to time a system form a new concept while retain an old one a well alternatively old concept be sometimes merged or eliminated . this require a modification of the similarity measure l the reason be that more model always result in a good fit between the model and data . this be a well known problem it is address by reduce similarity l use a skeptic penalty function penalty method pnm that grow with the number of model m and this growth be steep for a small amount of data n . for example an asymptotically unbiased maximum likelihood estimation lead to multiplicative pnm expn where n be a total number of adaptive parameter in all model this penalty function is know a akaike information criterion see perlovsky for further discussion and reference . the learning process consists of estimating model parameter s and associate signal with concept by maximize the similarity l . note that all possible combination of signal and model are account for in expression for l . this can be see by expand a sum and multiply all the term resulting in m items a huge number . this be the number of combination between all signal n and all model m . this be the source of combinatorial complexity which be solve in nmf by utilize the idea of dynamic logic . an important aspect of dynamic logic be matching vagueness or fuzziness of similarity measure to the uncertainty of model . initially parameter value are not know and uncertainty of model be high so be the fuzziness of the similarity measure . in the process of learn model become more accurate and the similarity measure more crisp the value of the similarity increase . the maximization of similarity l is do a follows . first the unknown parameter s be randomly initialized . then the association variable fmn are compute . equation for fmn look like the bayes formula for a posteriori probability if lnm in the result of learn become conditional likelihood fmn become bayesian probability for signal n originate from object m . the dynamic logic of the nmf is define a follows . the follow theorem ha been prove perlovsky . theorem . equation and define a convergent dynamic nmf system with stationary state define by maxsl . it follow that the stationary state of an mf system be the maximum similarity state . when partial similarity are specify a probability density function pdf or likelihoods the stationary value of parameter s are asymptotically unbiased and efficient estimate of these parameter . the computational complexity of dynamic logic be linear in n . practically when solve the equation through successive iteration fmn can be recomputed at every iteration using a oppose to incremental formula . the proof of the above theorem contain a proof that similarity l increase at each iteration . this have a psychological interpretation that the instinct for increase knowledge is satisfy at each step result in the positive emotion nmfdynamic logic system emotionally enjoys learn . find pattern below noise can be an exceedingly complex problem . if an exact pattern shape is not know and depend on unknown parameter these parameter should be find by fit the pattern model to the data . however when the location and orientation of pattern are not know it be not clear which subset of the data point should be select for fit . a standard approach for solve this kind of problem be multiple hypothesis testing singer et al . since all combination of subset and model are exhaustively search this method face the problem of combinatorial complexity . in the current example noisy smile and frown pattern be sought . they are show in fig . a without noise and in fig . b with the noise a actually measure . the true number of pattern be which is not know . therefore at least pattern should be fit to the data to decide that pattern fit best . the image size in this example be x point . if one attempt to fit model to all subset of data point computation of complexity m . an alternative computation by search through the parameter space yield low complexity each pattern is characterize by a parameter parabolic shape . fit x parameter to x grid by a bruteforce testing would take about to operation still a prohibitive computational complexity . to apply nmf and dynamic logic to this problem one need to develop parametric adaptive model of expect pattern . the model and conditional partial similarity for this case are describe in detail in a uniform model for noise gaussian blob for highlyfuzzy poorly resolve pattern and parabolic model for smile and frown . the number of computer operation in this example be about . thus a problem that be not solvable due to combinatorial complexity become solvable using dynamic logic . during an adaptation process initially fuzzy and uncertain model are associate with structure in the input signal and fuzzy model become more definite and crisp with successive iteration . the type shape and number of model are select so that the internal representation within the system be similar to input signal the nmf conceptmodels represent structureobjects in the signal . the figure below illustrates operation of dynamic logic . in fig . a true smile and frown pattern are show without noise b actual image available for recognition signal be below noise signaltonoise ratio be between db and . db c an initial fuzzy model a large fuzziness corresponds to uncertainty of knowledge d through m show improve model at various iteration stage total of iteration . every five iteration the algorithm try to increase or decrease the number of model . between iteration d and e the algorithm decide that it need three gaussian model for the best fit . there be several type of model one uniform model describing noise it is not show and a variable number of blob model and parabolic model their number location and curvature are estimate from the data . until about stage g the algorithm use simple blob model at g and beyond the algorithm decide that it need more complex parabolic model to describe the data . iteration stop at h when similarity stop increase . above a single processing level in a hierarchical nmf system wa describe . at each level of hierarchy there be input signal from lower level model similarity measure l emotion which are define a change in similarity and action action include adaptation behavior satisfy the knowledge instinct maximization of similarity . an input to each level be a set of signal xn or in neural terminology an input field of neuronal activation . the result of signal processing at a given level are activate model or concept m recognize in the input signal n these model along with the correspond instinctual signal and emotion may activate behavioral model and generate behavior at this level . the activated model initiate other action . they serve a input signal to the next process level where more general conceptmodels are recognize or create . output signal from a give level serve a input to the next level be the model activation signal a define a . a fmn . the hierarchical nmf system is illustrate in fig . within the hierarchy of the mind each conceptmodel find it mental meaning and purpose at a high level in addition to other purpose . for example consider a conceptmodel chair . it have a behavioral purpose of initiate sitting behavior if sit is require by the body this be the bodily purpose at the same hierarchical level . in addition it have a purely mental purpose at a higher level in the hierarchy a purpose of help to recognize a more general concept say of a concert hall a model of which contain row of chair . from time to time a system form a new concept or eliminate an old one . at every level the nmf system always keep a reserve of vague fuzzy inactive conceptmodels . they be inactive in that their parameter are not adapt to the data therefore their similarity to signal be low . yet because of a large vagueness covariance the similarity are not exactly zero . when a new signal do not fit well into any of the active model it similarity to inactive model automatically increase because first every piece of data is account for and second inactive model be vaguefuzzy and potentially can grab every signal that doe not fit into more specific le fuzzy active model . when the activation signal a for an inactive model m exceed a certain threshold the model is activate . similarly when an activation signal for a particular model fall below a threshold the model be deactivated . threshold for activation and deactivation are set usually base on information existing at a high hierarchical level prior information system resource number of activate model of various type etc . activation signal for active model at a particular level a form a neuronal field which serve a input signal to the next level where more abstract and more general concept are form .
paraphrase or paraphrasing in computational linguistics be the natural language process task of detecting and generate paraphrase . application of paraphrase are vary include information retrieval question answer text summarization and plagiarism detection . paraphrasing be also useful in the evaluation of machine translation as well a generation of new sample to expand exist corpora . barzilay and lee propose a method to generate paraphrase through the usage of monolingual parallel corpus namely news article cover the same event on the same day . train consists of use multisequence alignment to generate sentencelevel paraphrase from an unannotated corpus . this is do by . this is achieve by first cluster similar sentence together use ngram overlap . recur pattern are find within cluster by using multisequence alignment . then the position of argument word are determine by find area of high variability within each cluster aka between word share by more than of a cluster sentence . pairing between pattern are then find by compare similar variable word between different corpus . finally new paraphrase can be generate by choose a matching cluster for a source sentence then substitute the source sentence argument into any number of pattern in the cluster . paraphrase can also be generate through the use of phrasebased translation a propose by bannard and callisonburch . the chief concept consists of align phrase in a pivot language to produce potential paraphrase in the original language . for example the phrase under control in an english sentence is align with the phrase unter kontrolle in it german counterpart . the phrase unter kontrolle is then find in another german sentence with the align english phrase be in check a paraphrase of under control . the probability distribution can be model a formula the probability phrase formula be a paraphrase of formula which be equivalent to formula sum over all formula a potential phrase translation in the pivot language . additionally the sentence formula be add a a prior to add context to the paraphrase . thus the optimal paraphrase formula can be model a . formula and formula can be approximate by simply take their frequency . add formula a a prior is model by calculate the probability of form the formula when formula is substitute with . there have been success in use long shortterm memory lstm model to generate paraphrase . in short the model consists of an encoder and decoder component both implemented using variation of a stack residual lstm . first the encode lstm take a onehot encoding of all the word in a sentence a input and produce a final hidden vector which can be view a a representation of the input sentence . the decoding lstm then take the hidden vector a input and generate new sentence terminating in an endofsentence token . the encoder and decoder are train to take a phrase and reproduce the onehot distribution of a correspond paraphrase by minimize perplexity use simple stochastic gradient descent . new paraphrase are generate by inputting a new phrase to the encoder and pass the output to the decoder . paraphrase recognition ha been attempt by socher et al through the use of recursive autoencoders . the main concept be to produce a vector representation of a sentence along with it component through recursively use an autoencoder . the vector representation of paraphrase should have similar vector representation they are process then feed a input into a neural network for classification . give a sentence formula with formula word the autoencoder is design to take formuladimensional word embeddings a input and produce an formuladimensional vector a output . the same autoencoder is apply to every pair of word in formula to produce formula vector . the autoencoder is then apply recursively with the new vector a input until a single vector is produce . give an odd number of input the first vector is forward a be to the next level of recursion . the autoencoder is then train to reproduce every vector in the full recursion tree include the initial word embeddings . give two sentence formula and formula of length and respectively the autoencoders would produce and vector representation include the initial word embeddings . the euclidean distance is then take between every combination of vector in formula and formula to produce a similarity matrix formula . formula be then subject to a dynamic minpooling layer to produce a fixed size formula matrix . since formula are not uniform in size among all potential sentence formula be split into formula roughly even section . the output be then normalized to have mean and standard deviation and is feed into a fully connect layer with a softmax output . the dynamic pooling to softmax model is train using pair of known paraphrase . skipthought vector be an attempt to create a vector representation of the semantic meaning of a sentence in a similar fashion a the skip gram model . skipthought vector are produce through the use of a skipthought model which consist of three key component an encoder and two decoder . give a corpus of document the skipthought model is train to take a sentence a input and encode it into a skipthought vector . the skipthought vector is use a input for both decoder one of which attempt to reproduce the previous sentence and the other the follow sentence in it entirety . the encoder and decoder can be implement through the use of a recursive neural network rnn or an lstm . since paraphrase carry the same semantic meaning between one another they should have similar skipthought vector . thus a simple logistic regression can be train to a good performance with the absolute difference and componentwise product of two skipthought vector a input . there be multiple method that can be use to evaluate paraphrase . since paraphrase recognition can be pose a a classification problem most standard evaluation metrics such a accuracy f score or an roc curve do relatively well . however there be difficulty calculating fscores due to trouble produce a complete list of paraphrase for a give phrase along with the fact that good paraphrase be dependent upon context . a metric design to counter these problem be parametric . parametric aim to calculate the precision and recall of an automatic paraphrase system by compare the automatic alignment of paraphrase to a manual alignment of similar phrase . since parametric is simply rat the quality of phrase alignment it can be use to rate paraphrase generation system a well assume it us phrase alignment a part of it generation process . a noted drawback to parametric be the large and exhaustive set of manual alignment that must be initially create before a rating can be produce . the evaluation of paraphrase generation have similar difficulty a the evaluation of machine translation . often the quality of a paraphrase be dependent upon it context whether it is being use a a summary and how it is generate among other factor . additionally a good paraphrase usually be lexically dissimilar from it source phrase . the simple method use to evaluate paraphrase generation would be through the use of human judge . unfortunately evaluation through human judge tend to be time consume . automate approach to evaluation prove to be challenge a it be essentially a problem a difficult a paraphrase recognition . while originally use to evaluate machine translation bilingual evaluation understudy bleu ha be use successfully to evaluate paraphrase generation model as well . however paraphrase often have several lexically different but equally valid solution which hurt bleu and other similar evaluation metric . metric specifically design to evaluate paraphrase generation include paraphrase in ngram change pinc and paraphrase evaluation metric pem along with the aforementioned parametric . pinc is design to be use in conjunction with bleu and help cover it inadequacy . since bleu have difficulty measuring lexical dissimilarity pinc be a measurement of the lack of ngram overlap between a source sentence and a candidate paraphrase . it be essentially the jaccard distance between the sentence excluding ngrams that appear in the source sentence to maintain some semantic equivalence . pem on the other hand attempt to evaluate the adequacy fluency and lexical dissimilarity of paraphrase by return a single value heuristic calculate use ngrams overlap in a pivot language . however a large drawback to pem be that must be train use a large indomain parallel corpus as well a human judge . in other word it be tantamount to train a paraphrase recognition system in order to evaluate a paraphrase generation system .
in machine learn kernel method arise from the assumption of an inner product space or similarity structure on input . for some such method such a support vector machine svms the original formulation and it regularization be not bayesian in nature . it be helpful to understand them from a bayesian perspective . because the kernel be not necessarily positive semidefinite the underlying structure may not be inner product space but instead more general reproducing kernel hilbert space . in bayesian probability kernel method be a key component of gaussian process where the kernel function is know a the covariance function . kernel method have traditionally be use in supervise learn problem where the input space be usually a space of vector while the output space be a space of scalar . more recently these method have been extend to problem that deal with multiple output such a in multitask learn . in this article we analyze the connection between the regularization and the bayesian point of view for kernel method in the case of scalar output . a mathematical equivalence between the regularization and the bayesian point of view is easily prove in case where the reproduce kernel hilbert space be finitedimensional . the infinitedimensional case raise subtle mathematical issue we will consider here the finitedimensional case . we start with a brief review of the main idea underlying kernel method for scalar learning and briefly introduce the concept of regularization and gaussian process . we then show how both point of view arrive at essentially equivalent estimator and show the connection that tie them together . the classical supervised learning problem requires estimate the output for some new input point formula by learn a scalarvalued estimator formula on the basis of a training set formula consisting of formula inputoutput pair formula . give a symmetric and positive bivariate function formula call a kernel one of the most popular estimator in machine learn is give by . where formula be the kernel matrix with entry formula formula and formula . we will see how this estimator can be derive both from a regularization and a bayesian perspective . the main assumption in the regularization perspective be that the set of function formula is assume to belong to a reproduce kernel hilbert space formula . a reproduce kernel hilbert space rkhs formula be a hilbert space of function define by a symmetric positivedefinite function formula call the reproducing kernel such that the function formula belongs to formula for all formula . there be three main property make an rkhs appeal . the reproduce property which give name to the space . where formula be the inner product in formula . function in an rkhs be in the closure of the linear combination of the kernel at given point . this allow the construction in a unified framework of both linear and generalize linear model . the squared norm in an rkhs can be write a . and could be view a measure the complexity of the function . the estimator is derive a the minimizer of the regularized functional . where formula and formula be the norm in formula . the first term in this functional which measure the average of the square of the error between the formula and the formula is call the empirical risk and represent the cost we pay by predict formula for the true value formula . the second term in the functional be the squared norm in a rkhs multiply by a weight formula and serve the purpose of stabilize the problem as well a of add a tradeoff between fit and complexity of the estimator . the weight formula call the regularizer determines the degree to which instability and complexity of the estimator should be penalize high penalty for increase value of formula . the explicit form of the estimator in equation is derive in two step . first the representer theorem state that the minimizer of the functional can always be write a a linear combination of the kernel center at the trainingset point . for some formula . the explicit form of the coefficient formula can be find by substitute for formula in the functional . for a function of the form in equation we have that . we can rewrite the functional a . this functional be convex in formula and therefore we can find it minimum by set the gradient with respect to formula to zero . substitute this expression for the coefficient in equation we obtain the estimator state previously in equation . the notion of a kernel play a crucial role in bayesian probability a the covariance function of a stochastic process call the gaussian process . a part of the bayesian framework the gaussian process specify the prior distribution that describe the prior belief about the property of the function being model . these belief are update after take into account observational data by mean of a likelihood function that relate the prior belief to the observation . take together the prior and likelihood lead to an updated distribution call the posterior distribution that is customarily use for predict test case . a gaussian process gp be a stochastic process in which any finite number of random variable that are sample follow a joint normal distribution . the mean vector and covariance matrix of the gaussian distribution completely specify the gp . gps are usually use a a priori distribution for function and a such the mean vector and covariance matrix can be view a function where the covariance function is also call the kernel of the gp . let a function formula follow a gaussian process with mean function formula and kernel function formula . in term of the underlie gaussian distribution we have that for any finite set formula if we let formula then . where formula be the mean vector and formula be the covariance matrix of the multivariate gaussian distribution . in a regression context the likelihood function be usually assumed to be a gaussian distribution and the observation to be independent and identically distribute iid . this assumption correspond to the observation being corrupt with zeromean gaussian noise with variance formula . the iid assumption make it possible to factorize the likelihood function over the data point give the set of input formula and the variance of the noise formula and thus the posterior distribution can be compute analytically . for a test input vector formula give the training data formula the posterior distribution is give by . where formula denote the set of parameter which include the variance of the noise formula and any parameter from the covariance function formula and where . a connection between regularization theory and bayesian theory can only be achieve in the case of finite dimensional rkhs . under this assumption regularization theory and bayesian theory are connect through gaussian process prediction . in the finite dimensional case every rkhs can be describe in term of a feature map formula such that . function in the rkhs with kernel formula can be then be write as . and we also have that . we can now build a gaussian process by assume formula to be distribute according to a multivariate gaussian distribution with zero mean and identity covariance matrix . if we assume a gaussian likelihood we have . where formula . the resulting posterior distribution be the give by . we can see that a maximum posterior map estimate be equivalent to the minimization problem defining tikhonov regularization where in the bayesian case the regularization parameter be related to the noise variance . from a philosophical perspective the loss function in a regularization setting play a different role than the likelihood function in the bayesian setting . whereas the loss function measure the error that is incur when predict formula in place of formula the likelihood function measure how likely the observation be from the model that wa assume to be true in the generative process . from a mathematical perspective however the formulation of the regularization and bayesian framework make the loss function and the likelihood function to have the same mathematical role of promote the inference of function formula that approximate the label formula as much a possible .
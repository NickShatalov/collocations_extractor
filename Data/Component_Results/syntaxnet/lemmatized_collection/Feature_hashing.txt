in machine learn feature hash also known a the hashing trick by analogy to the kernel trick be a fast and spaceefficient way of vectorizing feature i . turn arbitrary feature into index in a vector or matrix . it work by apply a hash function to the feature and use their hash value a index directly rather than look the index up in an associative array . in a typical document classification task the input to the machine learn algorithm both during learn and classification be free text . from this a bag of word bow representation is construct the individual token are extract and count and each distinct token in the training set define a feature independent variable of each of the document in both the training and test set . machine learn algorithm however are typically define in term of numerical vector . therefore the bag of word for a set of document is regard a a termdocument matrix where each row be a single document and each column be a single featureword the entry in such a matrix capture the frequency or weight of the th term of the vocabulary in document . an alternative convention swap the row and column of the matrix but this difference be immaterial . typically these vector be extremely sparseaccording to zipfs law . the common approach be to construct at learn time or prior to that a dictionary representation of the vocabulary of the training set and use that to map word to index . hash table and try be common candidate for dictionary implementation . the three document . can be convert use the dictionary . to the termdocument matrix . the problem with this process be that such dictionary take up a large amount of storage space and grow in size a the training set grows . on the contrary if the vocabulary is keep fix and not increase with a grow training set an adversary may try to invent new word or misspelling that be not in the stored vocabulary so a to circumvent a machine learned filter . this difficulty be why feature hashing ha been try for spam filtering at yahoo research . note that the hashing trick isnt limit to text classification and similar task at the document level but can be apply to any problem that involve large perhaps unbounded number of feature . instead of maintain a dictionary a feature vectorizer that use the hashing trick can build a vector of a predefined length by apply a hash function to the feature e . word then use the hash value directly a feature index and update the resulting vector at those index . here we assume that feature actually mean feature vector . thus if our feature vector be catdogcat and hash function be formula if formula be cat and formula if formula be dog . let u take the output feature vector dimension n to be . then output x will be . it ha been suggest that a second singlebit output hash function be use to determine the sign of the update value to counter the effect of hash collision . if such a hash function is use the algorithm become . the above pseudocode actually convert each sample into a vector . an optimized version would instead only generate a stream of pair and let the learn and prediction algorithm consume such stream a linear model can then be implement a a single hash table represent the coefficient vector . when a second hash function is use to determine the sign of a feature value the expect mean of each column in the output array become zero because cause some collision to cancel out . suppose an input contains two symbolic feature f and f that collide with each other but not with any other feature in the same input then there be four possibility which if we make no assumption about have equal probability a list in the table on the right . in this example there be a probability that the hash collision cancel out . multiple hash function can be use to further reduce the risk of collision . furthermore if be the transformation implement by a hashing trick with a sign hash i . x be the feature vector produce for a sample x then inner product in the hashed space are unbiased . where the expectation is take over the hashing function . it can be verify thatformula be a positive semidefinite kernel . recent work extend the hashing trick to supervise mapping from word to index . which are explicitly learn to avoid collision of important term . ganchev and dredze show that in text classification application with random hash function and several ten of thousand of column in the output vector feature hash need not have an adverse effect on classification performance even without the sign hash function . weinberger et al . apply their variant of hash to the problem of spam filtering formulate this a a multitask learn problem where the input feature be pair user feature so that a single parameter vector capture peruser spam filter as well a a global filter for several hundred thousand user and find that the accuracy of the filter go up . implementation of the hashing trick be present in .
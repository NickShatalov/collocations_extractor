in machine learn the vanish gradient problem be a difficulty found in train artificial neural network with gradientbased learning method and backpropagation . in such method each of the neural network weight receive an update proportional to the gradient of the error function with respect to the current weight in each iteration of training . the problem be that in some case the gradient will be vanishingly small effectively prevent the weight from change it value . in the bad case this may completely stop the neural network from further training . a one example of the problem cause traditional activation function such a the hyperbolic tangent function have gradient in the range and backpropagation computes gradient by the chain rule . this have the effect of multiply of these small number to compute gradient of the front layer in an layer network meaning that the gradient error signal decrease exponentially with while the front layer train very slowly . backpropagation allow researcher to train supervise deep artificial neural network from scratch initially with little success . hochreiters diploma thesis of formally identify the reason for this failure in the vanishing gradient problem which not only affect manylayered feedforward network but also recurrent network . the latter are train by unfold them into very deep feedforward network where a new layer is create for each time step of an input sequence process by the network . when activation function are use whose derivative can take on large value one risk encounter the related exploding gradient problem . to overcome this problem several method were propose . one be jrgen schmidhubers multilevel hierarchy of network pretrained one level at a time through unsupervised learning finetuned through backpropagation . here each level learn a compress representation of the observation that be fed to the next level . similar idea have been use in feedforward neural network for unsupervised pretraining to structure a neural network make it first learn generally useful feature detector . then the network is train far by supervise backpropagation to classify labeled data . the deep belief network model by hinton et al . involves learn the distribution of a high level representation use successive layer of binary or realvalued latent variable . it use a restrict boltzmann machine to model each new layer of high level feature . each new layer guarantee an increase on the lowerbound of the log likelihood of the data thus improve the model if train properly . once sufficiently many layer have been learn the deep architecture may be use a a generative model by reproduce the data when sample down the model an ancestral pas from the top level feature activation . hinton report that his model be effective feature extractor over highdimensional structured data . this work play a key role in reintroduce the interest in deep neural network research and consequently lead to the development of deep learn although deep belief network be no longer the main deep learn technique . another technique particularly use for recurrent neural network be the long shortterm memory lstm network of by hochreiter schmidhuber . in deep multidimensional lstm network demonstrate the power of deep learn with many nonlinear layer by win three icdar competition in connect handwriting recognition without any prior knowledge about the three different language to be learn . hardware advance have meant that from to computer power especially a deliver by gpus ha increase around a millionfold making standard backpropagation feasible for network several layer deeply than when the vanish gradient problem wa recognize . schmidhuber note that this be basically what is win many of the image recognition competition now but that it do not really . overcome the problem in a fundamental way since the original model tackling the vanish gradient problem by hinton et al . were train in a xeon processor not gpus . one of the newest and most effective way to resolve the vanish gradient problem be with residual neural network resnets not to be confuse with recurrent neural network . it wa note prior to resnets that a deep network would actually have higher training error than the shallow network . this intuitively can be understood a data disappear through too many layer of the network meaning output from a shallow layer wa diminish through the greater number of layer in the deep network yield a worse result . go with this intuitive hypothesis microsoft research found that split a deep network into three layer chunk and pass the input into each chunk straight through to the next chunk along with the residualoutput of the chunk minus the input to the chunk that is reintroduce help eliminate much of this disappearing signal problem . no extra parameter or change to the learn algorithm were need . resnets yield lower train error and test error than their shallower counterpart simply by reintroduce output from shallower layer in the network to compensate for the vanish data . note that resnets be an ensemble of relatively shallow net and do not resolve the vanish gradient problem by preserve gradient flow throughout the entire depth of the network rather they avoid the problem simply by construct ensemble of many short network together . ensemble by construction . rectifiers such a relu suffer le from the vanishing gradient problem because they only saturate in one direction . behnke rely only on the sign of the gradient rprop when train his neural abstraction pyramid to solve problem like image reconstruction and face localization . neural network can also be optimize by use a universal search algorithm on the space of neural network weight e . random guess or more systematically genetic algorithm . this approach is not base on gradient and avoid the vanish gradient problem .
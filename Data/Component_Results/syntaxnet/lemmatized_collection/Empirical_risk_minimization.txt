empirical risk minimization erm be a principle in statistical learning theory which define a family of learn algorithm and is use to give theoretical bound on their performance . consider the follow situation which be a general setting of many supervised learn problem . we have two space of object formula and formula and would like to learn a function formula often call hypothesis which output an object formula give formula . to do so we have at our disposal a training set of m example formula where formula be an input and formula be the corresponding response that we wish to get from formula . to put it more formally we assume that there be a joint probability distribution formula over formula and formula and that the training set consists of formula instance formula drawn i . from formula . note that the assumption of a joint probability distribution allow u to model uncertainty in prediction e . from noise in data because formula be not a deterministic function of formula but rather a random variable with conditional distribution formula for a fix formula . we also assume that we are give a nonnegative realvalued loss function formula which measure how different the prediction formula of a hypothesis be from the true outcome formula the risk associate with hypothesis formula be then defined a the expectation of the loss function . a loss function commonly use in theory be the loss function formula where formula be the indicator notation . the ultimate goal of a learning algorithm be to find a hypothesis formula among a fixed class of function formula for which the risk formula be minimal . in general the risk formula cannot be compute because the distribution formula be unknown to the learning algorithm this situation is refer to a agnostic learn . however we can compute an approximation call empirical risk by average the loss function on the training set . the empirical risk minimization principle state that the learning algorithm should choose a hypothesis formula which minimize the empirical risk . thus the learning algorithm define by the erm principle consists in solving the above optimization problem . empirical risk minimization for a classification problem with a loss function is know to be an nphard problem even for such a relatively simple class of function a linear classifier . though it can be solve efficiently when the minimal empirical risk be zero i . data be linearly separable . in practice machine learn algorithm cope with that either by employ a convex approximation to the loss function like hinge loss for svm which be easier to optimize or by pose assumption on the distribution formula and thus stop be agnostic learn algorithm to which the above result applies .
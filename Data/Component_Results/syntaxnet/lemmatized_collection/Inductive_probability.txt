inductive probability attempt to give the probability of future event base on past event . it be the basis for inductive reason and give the mathematical basis for learn and the perception of pattern . it be a source of knowledge about the world . there be three source of knowledge inference communication and deduction . communication relay information found use other method . deduction establish new fact base on exist fact . only inference establish new fact from data . the basis of inference be bayes theorem . but this theorem be sometimes hard to apply and understand . the simple method to understand inference be in term of quantity of information . information describe the world is write in a language . for example a simple mathematical language of proposition may be choose . sentence may be write down in this language a string of character . but in the computer it be possible to encode these sentence a string of bit s and s . then the language may be encode so that the most commonly use sentence be the short . this internal language implicitly represent probability of statement . occams razor say the simplest theory consistent with the data be most likely to be correct . the simple theory be interpret a the representation of the theory write in this internal language . the theory with the short encoding in this internal language be most likely to be correct . probability and statistic wa focus on probability distribution and test of significance . probability be formal well define but limit in scope . in particular it application wa limit to situation that could be define a an experiment or trial with a well define population . bayes theorem is name after rev . thomas bayes . bayesian inference broaden the application of probability to many situation where a population wa not well define . but bayes theorem always depend on prior probability to generate new probability . it be unclear where these prior probability should come from . ray solomonoff developed algorithmic probability which give an explanation for what randomness be and how pattern in the data may be represent by computer program that give shorter representation of the data circa . chris wallace and d . boulton developed minimum message length circa . later jorma rissanen develop the minimum description length circa . these method allow information theory to be related to probability in a way that can be compare to the application of bayes theorem but which give a source and explanation for the role of prior probability . marcus hutter combine decision theory with the work of ray solomonoff and andrey kolmogorov to give a theory for the pareto optimal behavior for an intelligent agent circa . the program with the short length that match the data be the most likely to predict future data . this be the thesis behind the minimum message length and minimum description length method . at first sight bayes theorem appears different from the minimimum messagedescription length principle . at closer inspection it turn out to be the same . bayes theorem be about conditional probability . what be the probability that event b happen if firstly event a happens . become in term of message length l . what this mean be that in describe an event if all the information is give describe the event then the length of the information may be use to give the raw probability of the event . so if the information describing the occurrence of a is give along with the information describing b give a then all the information describe a and b ha be give . overfitting be where the model match the random noise and not the pattern in the data . for example take the situation where a curve is fit to a set of point . if polynomial with many term is fit then it can more closely represent the data . then the fit will be good and the information need to describe the deviance from the fit curve will be small . small information length mean more probable . however the information need to describe the curve must also be consider . the total information for a curve with many term may be great than for a curve with fewer term that have not a good a fit but need less information to describe the polynomial . solomonoffs theory of inductive inference be also inductive inference . a bit string x is observe . then consider all program that generate string start with x . cast in the form of inductive inference the program be theory that imply the observation of the bit string x . the method use here to give probability for inductive inference is base on solomonoffs theory of inductive inference . if all the bit be then people infer that there be a bias in the coin and that it be more likely also that the next bit be also . this is describe a learn from or detect a pattern in the data . such a pattern may be represent by a computer program . a short computer program may be write that produce a series of bit which be all . if the length of the program k be formula bit then it prior probability be . the length of the short program that represent the string of bit is call the kolmogorov complexity . kolmogorov complexity be not computable . this be related to the halting problem . when search for the short program some program may go into an infinite loop . the greek philosopher epicurus is quote a say if more than one theory be consistent with the observation keep all theory . a in a crime novel all theory must be consider in determine the likely murderer so with inductive probability all program must be consider in determine the likely future bit arise from the stream of bit . program that be already long than n have no predictive power . the raw or prior probability that the pattern of bit be random have no pattern be formula . each program that produce the sequence of bit but be short than the n be a theorypattern about the bit with a probability of formula where k be the length of the program . the probability of receive a sequence of bit y after receive a series of bit x be then the conditional probability of receive y give x which be the probability of x with y append divided by the probability of x . the programming language affect the prediction of the next bit in the string . the language act a a prior probability . this be particularly a problem where the programming language code for number and other data type . intuitively we think that and be simple number and that prime number be somehow more complex than number that may be composite . use the kolmogorov complexity give an unbiased estimate a universal prior of the prior probability of a number . a a thought experiment an intelligent agent may be fit with a data input device give a series of number after apply some transformation function to the raw number . another agent might have the same input device with a different transformation function . the agent do not see or know about these transformation function . then there appear no rational basis for prefer one function over another . a universal prior insures that although two agent may have different initial probability distribution for the data input the difference will be bound by a constant . so universal prior do not eliminate an initial bias but they reduce and limit it . whenever we describe an event in a language either use a natural language or other the language ha encode in it our prior expectation . so some reliance on prior probability be inevitable . a problem arise where an intelligent agent prior expectation interact with the environment to form a self reinforcing fee back loop . this be the problem of bias or prejudice . universal prior reduce but do not eliminate this problem . the theory of universal artificial intelligence applies decision theory to inductive probability . the theory show how the best action to optimize a reward function may be choose . the result be a theoretical model of intelligence . it be a fundamental theory of intelligence which optimize the agent behavior in . in general no agent will always provide the best action in all situation . a particular choice make by an agent may be wrong and the environment may provide no way for the agent to recover from an initial bad choice . however the agent be pareto optimal in the sense that no other agent will do good than this agent in this environment without do worse in another environment . no other agent may in this sense be say to be good . at present the theory is limit by incomputability the halt problem . approximation may be use to avoid this . process speed and combinatorial explosion remain the primary limiting factor for artificial intelligence . probability be the representation of uncertain or partial knowledge about the truth of statement . probability be subjective and personal estimate of likely outcome base on past experience and inference make from the data . this description of probability may seem strange at first . in natural language we refer to the probability that the sun will rise tomorrow . we do not refer to your probability that the sun will rise . but in order for inference to be correctly model probability must be personal and the act of inference generates new posterior probability from prior probability . probability be personal because they be conditional on the knowledge of the individual . probability be subjective because they always depend to some extent on prior probability assign by the individual . subjective should not be take here to mean vague or undefined . the term intelligent agent is use to refer to the holder of the probability . the intelligent agent may be a human or a machine . if the intelligent agent doe not interact with the environment then the probability will converge over time to the frequency of the event . if however the agent use the probability to interact with the environment there may be a feedback so that two agent in the identical environment start with only slightly different prior end up with completely different probability . in this case optimal decision theory a in marcus hutters universal artificial intelligence will give pareto optimal performance for the agent . this mean that no other intelligent agent could do good in one environment without do worse in another environment . in deductive probability theory probability be absolute independent of the individual make the assessment . but deductive probability are base on . for example in a trial the participant be aware the outcome of all the previous history of trial . they also assume that each outcome be equally probable . together this allow a single unconditional value of probability to be define . but in reality each individual doe not have the same information . and in general the probability of each outcome be not equal . the dice may be load and this loading need to be infer from the data . the principle of indifference ha play a key role in probability theory . it say that if n statement be symmetric so that one condition cannot be prefer over another then all statement be equally probable . take seriously in evaluate probability this principle lead to contradiction . suppose there be bag of gold in the distance and one is ask to select one . then because of the distance one cannot see the bag size . you estimate use the principle of indifference that each bag have equal amount of gold and each bag have one third of the gold . now while one of u is not look the other take one of the bag and divide it into bag . now there be bag of gold . the principle of indifference now say each bag have one fifth of the gold . a bag that wa estimate to have one third of the gold is now estimate to have one fifth of the gold . take a a value associate with the bag the value be different therefore contradictory . but take a an estimate give under a particular scenario both value be separate estimate give under different circumstance and there be no reason to believe they be equal . estimate of prior probability be particularly suspect . estimate will be construct that do not follow any consistent frequency distribution . for this reason prior probability are consider a estimate of probability rather than probability . a full theoretical treatment would associate with each probability . inductive probability combine two different approach to probability . each approach give a slightly different viewpoint . information theory is use in relate probability to quantity of information . this approach is often use in giving estimate of prior probability . frequentist probability define probability a objective statement about how often an event occurs . this approach may be stretch by define the trial to be over possible world . statement about possible world define event . whereas logic represents only two value true and false a the value of statement probability associate a number in to each statement . if the probability of a statement be the statement be false . if the probability of a statement be the statement be true . in consider some data a a string of bit the prior probability for a sequence of s and s the probability of and be equal . therefore each extra bit halve the probability of a sequence of bit . this lead to the conclusion that . where formula be the probability of the string of bit formula and formula be it length . the prior probability of any statement is calculate from the number of bit need to state it . see also information theory . two statement formula and formula may be represent by two separate encoding . then the length of the encoding be . or in term of probability . but this law be not always true because there may be a short method of encoding formula if we assume formula . so the above probability law apply only if formula and formula be independent . the primary use of the information approach to probability be to provide estimate of the complexity of statement . recall that occam razor state that all thing be equal the simple theory be the most likely to be correct . in order to apply this rule first there need to be a definition of what simple mean . information theory define simple to mean have the short encoding . knowledge is represent a statement . each statement be a boolean expression . expression are encode by a function that take a description a against the value of the expression and encode it a a bit string . the length of the encoding of a statement give an estimate of the probability of a statement . this probability estimate will often be use a the prior probability of a statement . technically this estimate be not a probability because it is not construct from a frequency distribution . the probability estimate give by it do not always obey the law of total of probability . apply the law of total probability to various scenario will usually give a more accurate probability estimate of the prior probability than the estimate from the length of the statement . an expression be constructed from sub expression . a huffman code must distinguish the case . the length of each code is base on the frequency of each type of sub expression . initially constant are all assign the same lengthprobability . later constant may be assign a probability use the huffman code base on the number of us of the function id in all expression record so far . in use a huffman code the goal be to estimate probability not to compress the data . the length of a function application be the length of the function identifier constant plus the sum of the size of the expression for each parameter . the length of a quantifier be the length of the expression being quantify over . no explicit representation of natural number is give . however natural number may be construct by apply the successor function to and then apply other arithmetic function . a distribution of natural number is imply by this base on the complexity of construct each number . rational number are construct by the division of natural number . the simple representation have no common factor between the numerator and the denominator . this allow the probability distribution of natural number may be extend to rational number . the probability of an event may be interpret a the frequency of outcome where the statement be true divide by the total number of outcome . if the outcomes form a continuum the frequency may need to be replace with a measure . event be set of outcome . statement may be relate to event . a boolean statement b about outcome define a set of outcome b . each probability be always associated with the state of knowledge at a particular point in the argument . probability before an inference are know a prior probability and probability after are know a posterior probability . probability depend on the fact known . the truth of a fact limit the domain of outcome to the outcome consistent with the fact . prior probability be the probability before a fact is know . posterior probability be after a fact is know . the posterior probability are say to be conditional on the fact . the probability that formula be true give that formula be true is write a formula . all probability be in some sense conditional . the prior probability of formula be . in the frequentist approach probability are define a the ratio of the number of outcome within an event to the total number of outcome . in the possible world model each possible world be an outcome and statement about possible world define event . the probability of a statement be true be the number of possible world divide by the total number of world . the probability of a statement formula be true about possible world be then . for a conditional probability . then . use symmetry this equation may be write out a bayes law . this law describe the relationship between prior and posterior probability when new fact be learnt . write a quantity of information bayes theorem becomes . two statement a and b are say to be independent if know the truth of a doe not change the probability of b . mathematically this be . then bay theorem reduces to . for a set of mutually exclusive possibility formula the sum of the posterior probability must be . substitute using bayes theorem give the law of total probability . this result is use to give the extend form of bayes theorem . this be the usual form of bayes theorem use in practice because it guarantee the sum of all the posterior probability for formula be . for mutually exclusive possibility the probability add . using . then the alternative . are all mutually exclusive . also . so put it all together . as . then . implication is relate to conditional probability by the follow equation . derivation . bayes theorem may be use to estimate the probability of a hypothesis or theory h give some fact f . the posterior probability of h be then . or in term of information . by assume the hypothesis be true a simple representation of the statement f may be give . the length of the encoding of this simple representation be formula . formula represent the amount of information need to represent the fact f if h be true . formula be the amount of information need to represent f without the hypothesis h . the difference be how much the representation of the fact ha been compress by assume that h be true . this be the evidence that the hypothesis h be true . if formula is estimate from encode length then the probability obtain will not be between and . the value obtain be proportional to the probability without be a good probability estimate . the number obtain be sometimes referred to a a relative probability be how much more probable the theory be than not hold the theory . if a full set of mutually exclusive hypothesis that provide evidence is know a proper estimate may be give for the prior probability formula . probability may be calculate from the extend form of bayes theorem . give all mutually exclusive hypothesis formula which give evidence such that . and also the hypothesis r that none of the hypothesis be true then . in term of information . in most situation it be a good approximation to assume that formula be independent of formula which mean formula give . abductive inference start with a set of fact f which be a statement boolean expression . abductive reasoning be of the form . the theory t also call an explanation of the condition f be an answer to the ubiquitous factual why question . for example for the condition f be why do apple fall . the answer be a theory t that implies that apples fall . inductive inference be of the form . in term of abductive inference all object in a class c or set have a property p be a theory that imply the observed condition all observe object in a class c have a property p . so inductive inference be a special case of abductive inference . in common usage the term inductive inference is often use to refer to both abductive and inductive inference . inductive inference be related to generalization . generalization may be form from statement by replace a specific value with membership of a category or by replace membership of a category with membership of a broader category . in deductive logic generalization be a powerful method of generate new theory that may be true . in inductive inference generalization generates theory that have a probability of be true . the opposite of generalization be specialization . specialization is use in apply a general rule to a specific case . specialization are create from generalization by replace membership of a category by a specific value or by replace a category with a sub category . the linnaen classification of living thing and object form the basis for generalization and specification . the ability to identify recognize and classify be the basis for generalization . perceive the world a a collection of object appears to be a key aspect of human intelligence . it be the object oriented model in the non computer science sense . the object orient model is construct from our perception . in particularly vision is base on the ability to compare two image and calculate how much information is need to morph or map one image into another . computer vision use this mapping to construct d image from stereo image pair . inductive logic programming be a mean of construct theory that imply a condition . plotkins relative least general generalization rlgg approach construct the simple generalization consistent with the condition . isaac newton use inductive argument in construct his law of universal gravitation . start with the statement . generalize by replace apple for object and earth for object give in a two body system . the theory explain all object fall so there be strong evidence for it . the second observation . after some complicate mathematical calculus it can be see that if the acceleration follow the inverse square law then object will follow an ellipse . so induction give evidence for the inverse square law . use galileo observation that all object drop with the same speed . where formula and formula vector towards the center of the other object . then use newton third law formula . implication determines condition probability a . so . this result may be use in the probability give for bayesian hypothesis testing . for a single theory h t and . or in term of information the relative probability be . note that this estimate for ptf be not a true probability . if formula then the theory have evidence to support it . then for a set of theory formula such that formula . give . make a list of all the short program formula that each produce a distinct infinite string of bit and satisfy the relation . where formula be the result of run the program formula and formula truncates the string after n bit . the problem be to calculate the probability that the source is produce by program formula give that the truncated source after n bit be x . this is represent by the conditional probability . use the extend form of bayes theorem . the extend form relies on the law of total probability . this mean that the formula must be distinct possibility which is give by the condition that each formula produce a different infinite string . also one of the condition formula must be true . this must be true a in the limit a formula there be always at least one program that produce formula . a formula be chosen so that formula then . the apriori probability of the string being produce from the program give no information about the string is base on the size of the program . give . program that be the same or long than the length of x provide no predictive power . separate them out give . then identify the two probability as . but the prior probability that x be a random set of bit be formula . so . the probability that the source be random or unpredictable be . a model of how world are construct is use in determine the probability of theory . if w be the bit string then the world is create such that formula be true . an intelligent agent have some fact about the word represent by the bit string c which give the condition . the set of bit string identical with any condition x be formula . a theory be a simple condition that explains or implies c . the set of all such theory is call t . extend form of bayes theorem may be apply . where . to apply bayes theorem the follow must hold formula be a partition of the event space . for formula to be a partition no bit string n may belong to two theory . to prove this assume they can and derive a contradiction . secondly prove that t include all outcomes consistent with the condition . a all theory consistent with c are include then formula must be in this set . so bayes theorem may be apply a specified give . use the implication and condition probability law the definition of formula implies . the probability of each theory in t is give by . so . finally the probability of the event may be identify with the probability of the condition which the outcome in the event satisfy . give . this be the probability of the theory t after observe that the condition c hold . theory that be less probable than the condition c have no predictive power . separate them out give . the probability of the theory without predictive power on c be the same a the probability of c . so . so the probability . and the probability of no prediction for c write a formula . the probability of a condition wa give a . bit string for theory that be more complex than the bit string give to the agent a input have no predictive power . there probability be well included in the random case . to implement this a new definition is give a f in . use f an improve version of the abductive probability be .
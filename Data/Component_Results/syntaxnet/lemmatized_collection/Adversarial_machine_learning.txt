adversarial machine learning be a research field that lie at the intersection of machine learn and computer security . it aim to enable the safe adoption of machine learning technique in adversarial setting like spam filtering malware detection and biometric recognition . the problem arise from the fact that machine learning technique were originally design for stationary environment in which the training and test data are assume to be generate from the same although possibly unknown distribution . in the presence of intelligent and adaptive adversary however this working hypothesis be likely to be violate to at least some degree depending on the adversary . in fact a malicious adversary can carefully manipulate the input data exploit specific vulnerability of learn algorithm to compromise the whole system security . example include attack in spam filter where spam message are obfuscate through misspell of bad word or insertion of good word attack in computer security e . to obfuscate malware code within network packet or mislead signature detection attack in biometric recognition where fake biometric trait may be exploit to impersonate a legitimate user biometric spoofing or to compromise user template gallery that are adaptively update over time . in mit researcher d print a toy turtle with a texture engineer to make google object detection ai classify it a a rifle no matter the angle the turtle wa view from . create the turtle require only lowcost commercially available d print technology . in google brain publish a machinetweaked image of a dog that look like a cat both to computer and to human . to understand the security property of learn algorithm in adversarial setting one should address the follow main issue . this process amount to simulate a proactive arm race instead of a reactive one a depict in figure and where system designer try to anticipate the adversary in order to understand whether there be potential vulnerability that should be fix in advance for instance by mean of specific countermeasure such a additional feature or different learn algorithm . however proactive approach are not necessarily superior to reactive one . for instance in the author show that under some circumstance reactive approach be more suitable for improve system security . the first step of the abovesketched arm race be identifying potential attack against machine learn algorithm . a substantial amount of work ha been do in this direction . attack against supervise machine learn algorithm have been categorize along three primary ax their influence on the classifier the security violation they cause and their specificity . this taxonomy ha been extend into a more comprehensive threat model that allow one to make explicit assumption on the adversary goal knowledge of the attack system capability of manipulate the input data andor the system component and on the correspond potentially formallydefined attack strategy . detail can be find here . two of the main attack scenario identify accord to this threat model are sketch below . evasion attack be the most prevalent type of attack that may be encounter in adversarial setting during system operation . for instance spammer and hacker often attempt to evade detection by obfuscate the content of spam email and malware code . in the evasion setting malicious sample are modify at test time to evade detection that be to be misclassified as legitimate . no influence over the training data is assume . a clear example of evasion be imagebased spam in which the spam content is embed within an attach image to evade the textual analysis perform by antispam filter . another example of evasion be give by spoof attack against biometric verification system . machine learn algorithm are often retrain on data collect during operation to adapt to change in the underlying data distribution . for instance intrusion detection system id are often retrain on a set of sample collect during network operation . within this scenario an attacker may poison the training data by inject carefully design sample to eventually compromise the whole learning process . poison may thus be regard a an adversarial contamination of the training data . example of poison attack against machine learn algorithm including learn in the presence of worstcase adversarial label flip in the training data can be find in . cluster algorithm have been increasingly adopt in security application to find dangerous or illicit activity . for instance clustering of malware and computer virus aim to identify and categorize different existing malware family and to generate specific signature for their detection by antiviruses or signaturebased intrusion detection system like snort . however clustering algorithm have not be originally devise to deal with deliberate attack attempt that are design to subvert the clustering process itself . whether clustering can be safely adopt in such setting thus remain questionable . preliminary work report some vulnerability of clustering can be find in . a number of defense mechanism against evasion poisoning and privacy attack have be propose in the field of adversarial machine learn include . some software library be available mainly for testing purpose and research .
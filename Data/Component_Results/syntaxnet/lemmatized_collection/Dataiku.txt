in statistic a mixture model be a probabilistic model for represent the presence of subpopulation within an overall population without require that an observe data set should identify the subpopulation to which an individual observation belongs . formally a mixture model corresponds to the mixture distribution that represent the probability distribution of observation in the overall population . however while problem associate with mixture distribution relate to derive the property of the overall population from those of the subpopulation mixture model are use to make statistical inference about the property of the subpopulation give only observation on the pooled population without subpopulation identity information . some way of implement mixture model involve step that attribute postulate subpopulationidentities to individual observation or weight towards such subpopulation in which case these can be regard a type of unsupervised learning or cluster procedure . however not all inference procedure involve such step . mixture model should not be confuse with model for compositional data i . data whose component are constrain to sum to a constant value etc . however compositional model can be think of a mixture model where member of the population are sample at random . conversely mixture model can be think of a compositional model where the total size read population ha be normalize to . a typical finitedimensional mixture model be a hierarchical model consisting of the follow componentsin addition in a bayesian set the mixture weight and parameter will themselves be random variable and prior distribution will be place over the variable . in such a case the weight are typically view a a kdimensional random vector draw from a dirichlet distribution the conjugate prior of the categorical distribution and the parameter will be distribute according to their respective conjugate prior . mathematically a basic parametric mixture model can be describe a followsin a bayesian setting all parameter are associate with random variable a followsthis characterization us f and h to describe arbitrary distribution over observation and parameter respectively . typically h will be the conjugate prior of f . the two most common choice of f be gaussian aka normal for realvalued observation and categorical for discrete observation . other common possibility for the distribution of the mixture component area typical nonbayesian gaussian mixture model look like thisa bayesian version of a gaussian mixture model be a followsa bayesian gaussian mixture model be commonly extended to fit a vector of unknown parameter denote in bold or multivariate normal distribution . in a multivariate distribution i . one model a vector formula with n random variable one may model a vector of parameter such a several observation of a signal or patch within an image use a gaussian mixture model prior distribution on the vector of estimate give bywhere the i vector component is characterize by normal distribution with weight formula mean formula and covariance matrix formula . to incorporate this prior into a bayesian estimation the prior is multiply with the known distribution formula of the data formula conditioned on the parameter formula to be estimate . with this formulation the posterior distribution formula be also a gaussian mixture model of the form with new parameter formula and formula that are update use the em algorithm . such distribution be useful for assuming patchwise shape of image and cluster for example . in the case of image representation each gaussian may be tilt expanded and warped accord to the covariance matrix formula . one gaussian distribution of the set be fit to each patch usually of size x pixel in the image . notably any distribution of point around a cluster see kmeans may be accurately give enough gaussian component but scarcely over k component are need to accurately model a give image distribution or cluster of data . a typical nonbayesian mixture model with categorical observation look like thisthe random variablesa typical bayesian mixture model with categorical observation look like thisthe random variablesfinancial return often behave differently in normal situation and during crisis time . a mixture model for return data seem reasonable . sometimes the model use be a jumpdiffusion model or a a mixture of two normal distribution . see financial economicschallenges and criticism for further context . assume that we observe the price of n different house . different type of house in different neighborhood will have vastly different price but the price of a particular type of house in a particular neighborhood e . threebedroom house in moderately upscale neighborhood will tend to cluster fairly closely around the mean . one possible model of such price would be to assume that the price are accurately describe by a mixture model with k different component each distribute a a normal distribution with unknown mean and variance with each component specify a particular combination of house typeneighborhood . fit this model to observed price e . use the expectationmaximization algorithm would tend to cluster the price according to house typeneighborhood and reveal the spread of price in each typeneighborhood . note that for value such a price or income that are guarantee to be positive and which tend to grow exponentially a lognormal distribution might actually be a good model than a normal distribution . assume that a document is compose of n different word from a total vocabulary of size v where each word corresponds to one of k possible topic . the distribution of such word could be model a a mixture of k different vdimensional categorical distribution . a model of this sort be commonly term a topic model . note that expectation maximization apply to such a model will typically fail to produce realistic result due among other thing to the excessive number of parameter . some sort of additional assumption be typically necessary to get good result . typically two sort of additional component are add to the modelthe following example is base on an example in christopher m . bishop pattern recognition and machine learn . imagine that we are give an nn blackandwhite image that is know to be a scan of a handwritten digit between and but we dont know which digit is write . we can create a mixture model with formula different component where each component be a vector of size formula of bernoulli distribution one per pixel . such a model can be train with the expectationmaximization algorithm on an unlabeled set of handwritten digit and will effectively cluster the image accord to the digit be written . the same model could then be use to recognize the digit of another image simply by hold the parameter constant compute the probability of the new image for each possible digit a trivial calculation and return the digit that generate the high probability . mixture model apply in the problem of direct multiple projectile at a target a in air land or sea defense application where the physical andor statistical characteristic of the projectile differ within the multiple projectile . an example might be shot from multiple munition type or shot from multiple location direct at one target . the combination of projectile type may be characterize a a gaussian mixture model . far a wellknown measure of accuracy for a group of projectile be the circular error probable cep which be the number r such that on average half of the group of projectile fall within the circle of radius r about the target point . the mixture model can be use to determine or estimate the value r . the mixture model properly capture the different type of projectile . the financial example above be one direct application of the mixture model a situation in which we assume an underlying mechanism so that each observation belong to one of some number of different source or category . this underlying mechanism may or may not however be observable . in this form of mixture each of the source is describe by a component probability density function and it mixture weight be the probability that an observation come from this component . in an indirect application of the mixture model we do not assume such a mechanism . the mixture model is simply use for it mathematical flexibility . for example a mixture of two normal distribution with different mean may result in a density with two mode which is not model by standard parametric distribution . another example is give by the possibility of mixture distribution to model fatter tail than the basic gaussian ones so a to be a candidate for model more extreme event . when combine with dynamical consistency this approach ha been apply to financial derivative valuation in presence of the volatility smile in the context of local volatility model . this define our application . the mixture modelbased clustering is also predominantly use in identify the state of the machine in predictive maintenance . density plot are use to analyze the density of high dimensional feature . if multimodel density are observe then it is assume that a finite set of density are form by a finite set of normal mixture . a multivariate gaussian mixture model be use to cluster the feature data into k number of group where k represents each state of the machine . the machine state can be a normal state power off state or faulty state . each formed cluster can be diagnose using technique such a spectral analysis . in the recent year this ha also be widely use in other areas such a early fault detection . in image process and computer vision traditional image segmentation model often assign to one pixel only one exclusive pattern . in fuzzy or soft segmentation any pattern can have certain ownership over any single pixel . if the pattern be gaussian fuzzy segmentation naturally result in gaussian mixture . combine with other analytic or geometric tool e . phase transition over diffusive boundary such spatially regularize mixture model could lead to more realistic and computationally efficient segmentation method . identifiability refer to the existence of a unique characterization for any one of the model in the class family be considered . estimation procedure may not be welldefined and asymptotic theory may not hold if a model be not identifiable . let j be the class of all binomial distribution with . then a mixture of two member of j would haveand . clearly give p and p it be not possible to determine the above mixture model uniquely a there be three parameter to be determined . consider a mixture of parametric distribution of the same class . letbe the class of all component distribution . then the convex hull k of j define the class of all finite mixture of distribution in jk is say to be identifiable if all it member be unique that is give two member p and in k be mixture of k distribution and distribution respectively in j we have if and only if first of all and secondly we can reorder the summation such that and for all i . parametric mixture model are often use when we know the distribution y and we can sample from x but we would like to determine the a and value . such situation can arise in study in which we sample from a population that is compose of several distinct subpopulation . it be common to think of probability mixture model a a missing data problem . one way to understand this be to assume that the data point under consideration have membership in one of the distribution we are use to model the data . when we start this membership be unknown or miss . the job of estimation be to devise appropriate parameter for the model function we choose with the connection to the data point being represent a their membership in the individual model distribution . a variety of approach to the problem of mixture decomposition have been propose many of which focus on maximum likelihood method such a expectation maximization em or maximum a posteriori estimation map . generally these method consider separately the question of system identification and parameter estimation method to determine the number and functional form of component within a mixture are distinguish from method to estimate the correspond parameter value . some notable departure be the graphical method a outline in tarter and lock and more recently minimum message length mml technique such a figueiredo and jain and to some extent the moment match pattern analysis routine suggest by mcwilliam and loh . expectation maximization em be seemingly the most popular technique use to determine the parameter of a mixture with an a priori give number of component . this be a particular way of implement maximum likelihood estimation for this problem . em be of particular appeal for finite normal mixture where closedform expression be possible such a in the follow iterative algorithm by dempster et al . with the posterior probabilitiesthus on the basis of the current estimate for the parameter the conditional probability for a given observation x being generate from state s be determined for each n be the sample size . the parameter are then update such that the new component weight correspond to the average conditional probability and each component mean and covariance be the component specific weight average of the mean and covariance of the entire sample . dempster also show that each successive em iteration will not decrease the likelihood a property not share by other gradient base maximization technique . moreover em naturally embed within it constraint on the probability vector and for sufficiently large sample size positive definiteness of the covariance iterates . this be a key advantage since explicitly constrained method incur extra computational cost to check and maintain appropriate value . theoretically em be a firstorder algorithm and a such converge slowly to a fixedpoint solution . redner and walker make this point arguing in favour of superlinear and second order newton and quasinewton method and report slow convergence in em on the basis of their empirical test . they do concede that convergence in likelihood wa rapid even if convergence in the parameter value themselves be not . the relative merit of em and other algorithm visvis convergence have been discuss in other literature . other common objection to the use of em be that it have a propensity to spuriously identify local maximum as well a display sensitivity to initial value . one may address these problem by evaluate em at several initial point in the parameter space but this be computationally costly and other approach such a the annealing em method of udea and nakano in which the initial component are essentially force to overlap provide a le heterogeneous basis for initial guess may be preferable . figueiredo and jain note that convergence to meaningless parameter value obtain at the boundary where regularity condition breakdown e . ghosh and sen is frequently observe when the number of model component exceeds the optimaltrue one . on this basis they suggest a unified approach to estimation and identification in which the initial n be chosen to greatly exceed the expect optimal value . their optimization routine is construct via a minimum message length mml criterion that effectively eliminate a candidate component if there be insufficient information to support it . in this way it be possible to systematize reduction in n and consider estimation and identification jointly . the expectationmaximization algorithm can be use to compute the parameter of a parametric mixture model distribution the a and . it be an iterative algorithm with two step an expectation step and a maximization step . practical example of em and mixture modeling are include in the socr demonstration . with initial guess for the parameter of our mixture model partial membership of each data point in each constituent distribution is compute by calculate expectation value for the membership variable of each data point . that be for each data point x and distribution y the membership value y iswith expectation value in hand for group membership plugin estimate are recomputed for the distribution parameter . the mixing coefficient a be the mean of the membership value over the n data point . the component model parameter are also calculate by expectation maximization using data point x that have been weight use the membership value . for example if be a mean with new estimate for a and the s the expectation step is repeat to recompute new membership value . the entire procedure is repeat until model parameter converge . a an alternative to the em algorithm the mixture model parameter can be deduce using posterior sampling a indicate by bayes theorem . this be still regarded a an incomplete data problem whereby membership of data point be the missing data . a twostep iterative procedure know a gibbs sample can be use . the previous example of a mixture of two gaussian distribution can demonstrate how the method work . a before initial guess of the parameter for the mixture model are make . instead of compute partial membership for each elemental distribution a membership value for each data point is draw from a bernoulli distribution that be it will be assign to either the first or the second gaussian . the bernoulli parameter is determine for each data point on the basis of one of the constituent distribution . draw from the distribution generate membership association for each data point . plugin estimator can then be use a in the m step of em to generate a new set of mixture model parameter and the binomial draw step repeat . the method of moment matching be one of the oldest technique for determine the mixture parameter dating back to karl pearsons seminal work of . in this approach the parameter of the mixture are determine such that the composite distribution have moment match some given value . in many instances extraction of solution to the moment equation may present nontrivial algebraic or computational problem . moreover numerical analysis by day ha indicate that such method may be inefficient compare to em . nonetheless there ha been renew interest in this method e . craigmile and titterington and wang . mcwilliam and loh consider the characterisation of a hypercuboid normal mixture copula in large dimensional system for which em would be computationally prohibitive . here a pattern analysis routine is use to generate multivariate taildependencies consistent with a set of univariate and in some sense bivariate moment . the performance of this method be then evaluated using equity logreturn data with kolmogorovsmirnov test statistic suggesting a good descriptive fit . some problem in mixture model estimation can be solve use spectral method . in particular it become useful if data point x be point in highdimensional real space and the hidden distribution are know to be logconcave such a gaussian distribution or exponential distribution . spectral method of learn mixture model are base on the use of singular value decomposition of a matrix which contain data point . the idea be to consider the top k singular vector where k be the number of distribution to be learn . the projectionof each data point to a linear subspace span by those vector group point originate from the same distributionvery close together while point from different distribution stay far apart . one distinctive feature of the spectral method be that it allow u to prove that ifdistributions satisfy certain separation condition e . not too close then the estimate mixture will be very close to the true one with high probability . tarter and lock describe a graphical approach to mixture identification in which a kernel function is apply to an empirical frequency plot so to reduce intracomponent variance . in this way one may more readily identify component have differing mean . while this method doe not require prior knowledge of the number or functional form of the component it success do rely on the choice of the kernel parameter which to some extent implicitly embed assumption about the component structure . some of them can even probably learn mixture of heavytailed distribution include those withinfinite variance see link to paper below . in this set em base method would not work since the expectation step would diverge due to presence ofoutliers . to simulate a sample of size n that be from a mixture of distribution f i to n with probability p sumpin a bayesian setting additional level can be add to the graphical model defining the mixture model . for example in the common latent dirichlet allocation topic model the observation be set of word drawn from d different document and the k mixture component represent topic that are share across document . each document have a different set of mixture weight which specify the topic prevalent in that document . all set of mixture weight share common hyperparameters . a very common extension be to connect the latent variable define the mixture component identities into a markov chain instead of assume that they be independent identically distribute random variable . the resulting model is term a hide markov model and be one of the most common sequential hierarchical model . numerous extension of hide markov model have be developed see the resulting article for more information . mixture distribution and the problem of mixture decomposition that be the identification of it constituent component and the parameter thereof ha been cite in the literature a far back a quetelet in mclachlan although common reference is make to the work of karl pearson a the first author to explicitly address the decomposition problem in characterise nonnormal attribute of forehead to body length ratio in female shore crab population . the motivation for this work wa provide by the zoologist walter frank raphael weldon who had speculate in in tarter and lock that asymmetry in the histogram of these ratio could signal evolutionary divergence . pearsons approach be to fit a univariate mixture of two normal to the data by choose the five parameter of the mixture such that the empirical moment match that of the model . while his work be successful in identify two potentially distinct subpopulation and in demonstrate the flexibility of mixture a a moment matching tool the formulation require the solution of a th degree nonic polynomial which at the time pose a significant computational challenge . subsequent work focus on address these problem but it be not until the advent of the modern computer and the popularisation of maximum likelihood mle parameterisation technique that research really take off . since that time there ha be a vast body of research on the subject spanning areas such a fishery research agriculture botany economics medicine genetics psychology palaeontology electrophoresis finance sedimentologygeology and zoology .
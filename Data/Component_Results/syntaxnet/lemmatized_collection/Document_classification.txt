in machine learn early stopping be a form of regularization use to avoid overfitting when train a learner with an iterative method such a gradient descent . such method update the learner so a to make it well fit the training data with each iteration . up to a point this improve the learner performance on data outside of the training set . past that point however improve the learner fit to the training data come at the expense of increase generalization error . early stop rule provide guidance a to how many iteration can be run before the learner begin to overfit . early stop rule have been employ in many different machine learn method with vary amount of theoretical foundation . this section present some of the basic machinelearning concept require for a description of early stop method . machine learn algorithm train a model base on a finite set of train data . during this train the model is evaluate base on how well it predict the observation contain in the training set . in general however the goal of a machine learning scheme be to produce a model that generalizes that be that predicts previously unseen observation . overfitting occurs when a model fit the data in the training set well while incur larger generalization error . regularization in the context of machine learn refers to the process of modify a learning algorithm so a to prevent overfitting . this generally involves impose some sort of smoothness constraint on the learned model . this smoothness may be enforce explicitly by fix the number of parameter in the model or by augment the cost function a in tikhonov regularization . tikhonov regularization along with principal component regression and many other regularization scheme fall under the umbrella of spectral regularization regularization characterize by the application of a filter . early stop also belongs to this class of method . gradient descent method be firstorder iterative optimization method . each iteration update an approximate solution to the optimization problem by take a step in the direction of the negative of the gradient of the objective function . by choose the stepsize appropriately such a method can be make to converge to a local minimum of the objective function . gradient descent is use in machinelearning by define a loss function that reflect the error of the learner on the training set and then minimize that function . earlystopping can be use to regularize nonparametric regression problem encounter in machine learn . for a give input space formula output space formula and sample draw from an unknown probability measure formula on formula the goal of such problem be to approximate a regression function formula give bywhere formula be the conditional distribution at formula induce by formula . one common choice for approximate the regression function be to use function from a reproduce kernel hilbert space . these space can be infinite dimensional in which they can supply solution that overfit training set of arbitrary size . regularization be therefore especially important for these method . one way to regularize nonparametric regression problem be to apply an early stopping rule to an iterative procedure such a gradient descent . the early stopping rule propose for these problem are base on analysis of upper bound on the generalization error a a function of the iteration number . they yield prescription for the number of iteration to run that can be compute prior to start the solution process . let formula and formula . give a set of samplesdrawn independently from formula minimize the functionalwhere formula be a member of the reproduce kernel hilbert space formula . that be minimize the expect risk for a leastsquares loss function . since formula depends on the unknown probability measure formula it cannot be use for computation . instead consider the follow empirical risklet formula and formula be the tth iterates of gradient descent apply to the expect and empirical risk respectively where both iteration are initialize at the origin and both use the step size formula . the formula form the population iteration which converge to formula but cannot be use in computation while the formula form the sample iteration which usually converge to an overfitting solution . we want to control the difference between the expect risk of the sample iteration and the minimum expected risk that be the expect risk of the regression functionthis difference can be rewrite a the sum of two term the difference in expect risk between the sample and population iteration and that between the population iteration and the regression functionthis equation present a biasvariance tradeoff which is then solve to give an optimal stopping rule that may depend on the unknown probability distribution . that rule ha associate probabilistic bound on the generalization error . for the analysis leading to the early stop rule and bound the reader is refer to the original article . in practice datadriven method e . crossvalidation can be use to obtain an adaptive stopping rule . boost refer to a family of algorithm in which a set of weak learner learner that are only slightly correlated with the true process are combine to produce a strong learner . it ha been show for several boosting algorithm including adaboost that regularization via early stop can provide guarantee of consistency that be that the result of the algorithm approach the true solution a the number of sample go to infinity . boost method have close tie to the gradient descent method describe above can be regard a a boost method base on the formula loss lboost . these early stopping rule work by split the original training set into a new training set and a validation set . the error on the validation set is use a a proxy for the generalization error in determine when overfitting ha begin . these method be most commonly employ in the training of neural network . prechelt give the follow summary of a naive implementation of holdoutbased early stopping a followsmore sophisticate form use crossvalidation multiple partition of the data into training set and validation set instead of a single partition into a training set and validation set . even this simple procedure is complicate in practice by the fact that the validation error may fluctuate during training produce multiple local minimum . this complication ha lead to the creation of many adhoc rule for decide when overfitting ha truly begin .
the curse of dimensionality refer to various phenomenon that arise when analyzing and organizing data in highdimensional space often with hundred or thousand of dimension that do not occur in lowdimensional setting such a the threedimensional physical space of everyday experience . the expression wa coin by richard e . bellman when consider problem in dynamic optimization . there be multiple phenomenon refer to by this name in domain such a numerical analysis sampling combinatorics machine learn data mining and database . the common theme of these problem be that when the dimensionality increase the volume of the space increase so fast that the available data become sparse . this sparsity be problematic for any method that require statistical significance . in order to obtain a statistically sound and reliable result the amount of data need to support the result often grows exponentially with the dimensionality . also organizing and search data often relies on detect area where object form group with similar property in high dimensional data however all object appear to be sparse and dissimilar in many way which prevent common data organization strategy from be efficient . these effect are also use for simplification of machine learn algorithm in high dimension that is bless of dimensionality . the blessing of dimensionality and the curse of dimensionality are recognise a two complementary influential principle in highdimensional data analysis . in some problem each variable can take one of several discrete value or the range of possible value is divide to give a finite number of possibility . take the variable together a huge number of combination of value must be consider . this effect be also known a the combinatorial explosion . even in the simple case of formula binary variable the number of possible combination already be formula exponential in the dimensionality . naively each additional dimension double the effort need to try all combination . there be an exponential increase in volume associate with add extra dimension to a mathematical space . for example evenly space sample point suffice to sample a unit interval a dimensional cube with no more than . distance between point an equivalent sampling of a dimensional unit hypercube with a lattice that have a spacing of . between adjacent point would require sample point . in general with a spacing distance of the dimensional hypercube appears to be a factor of large than the dimensional hypercube which be the unit interval . in the above example n when use a sampling distance of . the dimensional hypercube appear to be large than the unit interval . this effect be a combination of the combinatorics problem above and the distance function problem explain below . when solving dynamic optimization problem by numerical backward induction the objective function must be compute for each combination of value . this be a significant obstacle when the dimension of the state variable be large . in machine learn problem that involve learn a stateofnature from a finite number of data sample in a highdimensional feature space with each feature have a range of possible value typically an enormous amount of training data is require to ensure that there be several sample with each combination of value . a typical rule of thumb be that there should be at least training example for each dimension in the representation . with a fix number of train sample the predictive power of a classifier or regressor first increase a number of dimensionsfeatures use be increase but then decrease which be know a hughes phenomenon or peak phenomenon . when a measure such a a euclidean distance is define use many coordinate there be little difference in the distance between different pair of sample . one way to illustrate the vastness of highdimensional euclidean space be to compare the proportion of an inscribe hypersphere with radius formula and dimension formula to that of a hypercube with edge of length formula . the volume of such a sphere be formula where formula be the gamma function while the volume of the cube be formula . a the dimension formula of the space increase the hypersphere become an insignificant volume relative to that of the hypercube . this can clearly be by compare the proportion a the dimension formula go to infinity . furthermore the distance between the center and the corner be formula which increase without bound for fix r . in this sense nearly all of the highdimensional space be far away from the centre . to put it another way the highdimensional unit hypercube can be say to consist almost entirely of the corner of the hypercube with almost no middle . this also help to understand the chisquared distribution . indeed the noncentral chisquared distribution associate to a random point in the interval be the same a the distribution of the lengthsquared of a random point in the dcube . by the law of large number this distribution concentrate itself in a narrow band around d time the standard deviation square of the original derivation . this illuminate the chisquared distribution and also illustrate that most of the volume of the dcube concentrate near the surface of a sphere of radius . a further development of this phenomenon be a follow . any fixed distribution on induces a product distribution on point in . for any fix n it turn out that the minimum and the maximum distance between a random reference point q and a list of n random data point p . p become indiscernible compare to the minimum distance . this be often cited a distance function lose their usefulness for the nearestneighbor criterion in featurecomparison algorithm for example in high dimension . however recent research ha show this to only hold in the artificial scenario when the onedimensional distribution be independent and identically distribute . when attribute are correlated data can become easier and provide high distance contrast and the signaltonoise ratio wa find to play an important role thus feature selection should be use . the effect complicates near neighbor search in high dimensional space . it be not possible to quickly reject candidate by use the difference in one coordinate a a lower bound for a distance base on all the dimension . however it ha recently been observe that the mere number of dimension do not necessarily result in difficulty since relevant additional dimension can also increase the contrast . in addition for the resulting rank it remain useful to discern close and far neighbor . irrelevant noise dimension however reduce the contrast in the manner described above . in time series analysis where the data be inherently highdimensional distance function also work reliably as long a the signaltonoise ratio be high enough . another effect of high dimensionality on distance function concern knearest neighbor knn graph construct from a data set use a distance function . a the dimension increase the indegree distribution of the knn digraph becomes skew with a peak on the right because of the emergence of a disproportionate number of hub that be datapoints that appear in many more knn list of other datapoints than the average . this phenomenon can have a considerable impact on various technique for classification include the knn classifier semisupervised learn and cluster and it also affect information retrieval . in a recent survey zimek et al . identify the follow problem when search for anomaly in highdimensional data . many of the analyze specialize method tackle one or another of these problem but there remain many open research question .
in artificial intelligence apprenticeship learn or learn from demonstration be the process of learn by observe an expert . it can be view a a form of supervise learn where the train dataset consists of task execution by a demonstration teacher . map method try to mimic the expert by form a direct map from the state to the action . for example in researcher use such an approach to teach an aibo robot basic soccer skill . system model try to mimic the expert by model world dynamic . inverse reinforcement learn irl be the process of derive a reward function from observe behavior . while ordinary reinforcement learning involves use reward and punishment to learn behavior in irl the direction is reverse and a robot observe a person behavior to figure out what goal that behavior seem to be try to achieve . the irl problem can be define as . give measurement of an agent behaviour over time in a variety of circumstance measurement of the sensory input to that agent a model of the physical environment include the agent body determine the reward function that the agent be optimizing . irl researcher stuart j . russell proposes that irl might be use to observe human and attempt to codify their complex ethical value in an effort to create ethical robot that might someday know not to cook your cat without need to be explicitly told . the scenario can be model a a cooperative inverse reinforcement learn game where a person player and a robot player cooperate to secure the person implicit goal despite these goal not be explicitly known by either the person nor the robot . in openai and deepmind apply deep learn to the cooperative inverse reinforcement learn in simple domain such a atari game and straightforward robot task such a backflips . the human role wa limit to answer query from the robot a to which of two different action were prefer . the researcher find evidence that the technique may be economically scalable to modern system . apprenticeship via inverse reinforcement learn airp wa develop by in pieter abbeel professor in berkeley eec department and andrew ng associate professor in stanford university computer science department . airp deal with markov decision process where we are not explicitly give a reward function but where instead we can observe an expert demonstrate the task that we want to learn to perform . airp ha been use to model reward function of highly dynamic scenario where there be no obvious reward function intuitively . take the task of drive for example there be many different objectives working simultaneously such a maintaining safe follow distance a good speed not change lane too often etc . this task may seem easy at first glance but a trivial reward function may not converge to the policy want . one domain where airp ha been use extensively be helicopter control . while simple trajectory can be intuitively derive complicate task like aerobatics for show ha be successful . these include aerobatic maneuver like inplace flip inplace roll loop hurricane and even autorotation landing . this work wa develop by pieter abbeel adam coates and andrew ng autonomous helicopter aerobatics through apprenticeship learn . the system learns rule to associate precondition and postconditions with each action . in one demonstration a humanoid learns a generalize plan from only two demonstration of a repetitive ball . collection task . learn from demonstration is often explain from a perspective that the working robotcontrolsystem be available and the humandemonstrator is use it . and indeed if the software work the human operator take the robotarm make a move with it and the robot will reproduce the action later . for example he teach the robotarm how to put a cup under a coffeemaker and press the startbutton . in the replay phase the robot be imitate this behavior . but that be not how the system work internally it be only what the audience can observe . in reality learn from demonstration be much more complex . for tell the story right we must go back in the year . in this time the robotics expert stefan schaal wa work on the sarcos robotarm . the goal be simple solve the pendulum swingup task . the robot itself can execute a movement and a a result the pendulum is move . the problem be that it be unclear what action will result into which movement . it be an optimal controlproblem which can be describe with mathematical formula but be hard to solve . the idea from schaal be not to use a bruteforce solver but record the movement of a humandemonstration . the angle of the pendulum is log over the timeperiod of second at the yaxis . this result into a diagram which produce a pattern . in computer animation the principle is call spline animation . that mean on the xaxis the time is give for example . second . second . second while on the yaxis be the variable given . in most case its the position of an object . in the inverted pendulum it be the angle . the overall task consists of two part record the angle over time and reproduce the record motion . the reproducing step be surprisingly simple . a an input we know in which time step which angle the pendulum must have . bring the system to a state is call tracking control or pid control . that mean we have a trajectory over time and must find control action to map the system to this trajectory . other author call the principle steer behavior because the aim be to bring a robot to a given line .
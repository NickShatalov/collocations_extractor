multiplicative weight update method be a metaalgorithm . it be an algorithmic technique which maintain a distribution on a certain set of interest and update it iteratively by multiply the probability mass of element by suitably chosen factor base on feedback obtain by run another algorithm on the distribution . it wa discover repeatedly in very diverse field such a machine learn adaboost winnow hedge optimization solving linear program theoretical computer science devising fast algorithm for lp and sdps and game theory . multiplicative weight imply the iterative rule use in algorithm derive from the multiplicative weight update method . it is give with different name in the different field where it wa discover or rediscover . the earliest know version of this technique be in an algorithm named fictitious play which wa propose in game theory in the early s . grigoriadis and khachiyan apply a randomize variant of fictitious play to solve twoplayer zerosum game efficiently use the multiplicative weight algorithm . in this case player allocate high weight to the action that have a good outcome and choose his strategy rely on these weight . in machine learn littlestone apply the early form of the multiplicative weight update rule in his famous winnow algorithm which be similar to minsky and paperts earlier perceptron learn algorithm . later he generalize the winnow algorithm to weight majority algorithm . freund and schapire follow his step and generalize the winnow algorithm in the form of hedge algorithm . the multiplicative weight algorithm be also widely apply in computational geometry such a clarksons algorithm for linear programming lp with a bound number of variable in linear time . later bronnimann and goodrich employ analogous method to find set cover for hypergraphs with small vc dimension . in operation research and online statistical decision making problem field the weight majority algorithm and it more complicated version have be find independently . in computer science field some researcher have previously observe the close relationship between multiplicative update algorithm use in different context . young discover the similarity between fast lp algorithm and raghavans method of pessimistic estimator for derandomization of randomize rounding algorithm klivans and servedio link boosting algorithm in learn theory to proof of yaos xor lemma garg and khandekar define a common framework for convex optimization problem that contain gargkonemann and plotkinshmoystardos a subcases . a binary decision need to be make base on n expert opinion to attain an associated payoff . in the first round all expert opinion have the same weight . the decision maker will make the first decision base on the majority of the expert prediction . then in each successive round the decision maker will repeatedly update the weight of each expert opinion depending on the correctness of his prior prediction . real life example include predicting if it be rainy tomorrow or if the stock market will go up or go down . give a sequential game play between an adversary and an aggregator who is advise by n expert the goal be for the aggregator to make a few mistake a possible . assume there be an expert among the n expert who always give the correct prediction . in the halve algorithm only the consistent expert are retain . expert who make mistake will all be dismiss . for every decision the aggregator decides by take a majority vote among the remaining expert . therefore every time the aggregator make a mistake at least half of the remaining expert are dismiss . the aggregator make at most mistake . unlike halving algorithm which dismiss expert who have make mistake weight majority algorithm discount their advice . give the same expert advice setup suppose we have n decision and we need to select one decision for each loop . in each loop every decision incur a cost . all cost will be reveal after make the choice . the cost be if the expert be correct and otherwise . this algorithms goal be to limit it cumulative loss to roughly the same a the best of expert . the very first algorithm that make choice base on majority vote every iteration doe not work since the majority of the expert can be wrong consistently every time . the weight majority algorithm corrects above trivial algorithm by keep a weight of expert instead of fix the cost at either or . this would make few mistake compare to halve algorithm . if formula the weight of the expert advice will remain the same . when formula increase the weight of the expert advice will decrease . note that some researcher fix formula in weight majority algorithm . after formula step let formula be the number of mistake of expert i and formula be the number of mistake our algorithm ha make . then we have the follow bound for every formula . in particular this hold for i which be the best expert . since the best expert will have the least formula it will give the best bound on the number of mistake make by the algorithm a a whole . give the same setup with n expert . consider the special situation where the proportion of expert predicting positive and negative count the weight be both close to . then there might be a tie . follow the weight update rule in weight majority algorithm the prediction make by the algorithm would be randomize . the algorithm calculate the probability of expert predicting positive or negative and then make a random decision base on the computed fraction . predict . where . the number of mistake make by the randomize weight majority algorithm be bounded a . where formula and formula . note that only the learning algorithm is randomize . the underlying assumption be that the example and expert prediction be not random . the only randomness be the randomness where the learner make his own prediction . in this randomize algorithm formula if formula . compare to weight algorithm this randomness halve the number of mistake the algorithm is go to make . however it be important to note that in some research people define formula in weight majority algorithm and allow formula in randomize weighted majority algorithm . the multiplicative weight method be usually used to solve a constrained optimization problem . let each expert be the constraint in the problem and the event represent the point in the area of interest . the punishment of the expert corresponds to how well it correspond constraint is satisfy on the point represent by an event . suppose we were give the distribution formula on expert . let formula payoff matrix of a finite twoplayer zerosum game with formula row . when the row player formula use plan formula and the column player formula us plan formula the payoff of player formula be formulaformula assuming formula . if player formula choose action formula from a distribution formula over the row then the expect result for player formula selecting action formula be formula . to maximize formula player formula be should choose plan formula . similarly the expect payoff for player formula be formula . choose plan formula would minimize this payoff . by john von neumann minmax theorem we obtain . where p and i change over the distribution over row q and j change over the column . then let formula denote the common value of above quantity also name a the value of the game . let formula be an error parameter . to solve the zerosum game bound by additive error of formula . so there be an algorithm solving zerosum game up to an additive factor of use oformula call to oracle with an additional processing time of on per call . in machine learn littlestone and warmuth generalize the winnow algorithm to the weight majority algorithm . later freund and schapire generalize it in the form of hedge algorithm . adaboost algorithm formulate by yoav freund and robert schapire also employ the multiplicative weight update method . base on current knowledge in algorithms multiplicative weight update method wa first use in littlestones winnow algorithm . it is use in machine learn to solve a linear program . give formula label example formula where formula be feature vector and formula be their label . the aim be to find nonnegative weight such that for all example the sign of the weighted combination of the feature match it label . that be require that formula for all formula . without loss of generality assume the total weight be so that they form a distribution . thus for notational convenience redefine formula to be formula the problem reduces to find a solution to the follow lp . this be general form of lp . the hedge algorithm be similar to the weight majority algorithm . however their exponential update rule be different . it be generally used to solve the problem of binary allocation in which we need to allocate different portion of resource into n different option . the loss with every option be available at the end of every iteration . the goal be to reduce the total loss suffer for a particular allocation . the allocation for the follow iteration is then revise base on the total loss suffer in the current iteration using multiplicative update . assume the learn rate formula and for formula formula is pick by hedge . then for all expert formula . initialization fix an formula . for each expert associate the weight formula . for tt . this algorithm maintain a set of weight formula over the training example . on every iteration formula a distribution formula is compute by normalize these weight . this distribution is feed to the weak learner weaklearn which generates a hypothesis formula that hopefully have small error with respect to the distribution . use the new hypothesis formula adaboost generates the next weight vector formula . the process repeat . after t such iteration the final hypothesis formula be the output . the hypothesis formula combine the output of the t weak hypothesis use a weighted majority vote . give a formula matrix formula and formula be there a formula such that formula . use the oracle algorithm in solving zerosum problem with an error parameter formula the output would either be a point formula such that formula or a proof that formula doe not exist i . there be no solution to this linear system of inequality . give vector formula solve the follow relaxed problem . if there exist a x satisfy then x satisfies for all formula . the contrapositive of this statement be also true . suppose if oracle return a feasible solution for a formula the solution formula it return ha bound width formula . so if there be a solution to then there be an algorithm that it output x satisfy the system up to an additive error of formula . the algorithm make at most formula call to a widthbounded oracle for the problem . the contrapositive stand true as well . the multiplicative update is apply in the algorithm in this case . in operation research and online statistical decision making problem field the weight majority algorithm and it more complicated version have be find independently . the multiplicative weight algorithm be also widely apply in computational geometry such a clarksons algorithm for linear programming lp with a bound number of variable in linear time . later bronnimann and goodrich employ analogous method to find set cover for hypergraphs with small vc dimension .
multitask learning mtl be a subfield of machine learn in which multiple learning task are solve at the same time while exploiting commonality and difference across task . this can result in improve learn efficiency and prediction accuracy for the taskspecific model when compare to train the model separately . early version of mtl were call hintsin a widely cite paper rich caruana give the follow characterizationmultitask learning be an approach to inductive transfer that improve generalization by use the domain information contain in the training signal of relate task a an inductive bias . it do this by learn task in parallel while use a shared representation what is learn for each task can help other task be learn better . in the classification context mtl aim to improve the performance of multiple classification task by learn them jointly . one example be a spamfilter which can be treat a distinct but relate classification task across different user . to make this more concrete consider that different people have different distribution of feature which distinguish spam email from legitimate one for example an english speaker may find that all email in russian are spam not so for russian speaker . yet there be a definite commonality in this classification task across user for example one common feature might be text related to money transfer . solve each user spam classification problem jointly via mtl can let the solution inform each other and improve performance . further example of setting for mtl include multiclass classification and multilabel classification . multitask learning work because regularization induce by require an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalize all complexity uniformly . one situation where mtl may be particularly helpful be if the task share significant commonality and be generally slightly under sampled . however a discuss below mtl ha also be show to be beneficial for learn unrelated task . within the mtl paradigm information can be share across some or all of the task . depend on the structure of task relatedness one may want to share information selectively across the task . for example task may be group or exist in a hierarchy or be relate accord to some general metric . suppose a develop more formally below that the parameter vector modeling each task be a linear combination of some underlying basis . similarity in term of this basis can indicate the relatedness of the task . for example with sparsity overlap of nonzero coefficient across task indicates commonality . a task group then corresponds to those task lie in a subspace generate by some subset of basis element where task in different group may be disjoint or overlap arbitrarily in term of their base . task relatedness can be impose a priori or learn from the data . hierarchical task relatedness can also be exploit implicitly without assume a priori knowledge or learn relation explicitly . one can attempt learn a group of principal task use a group of auxiliary task unrelated to the principal one . in many application joint learn of unrelated task which use the same input data can be beneficial . the reason be that prior knowledge about task relatedness can lead to sparser and more informative representation for each task grouping essentially by screen out idiosyncrasy of the data distribution . novel method which build on a prior multitask methodology by favor a share lowdimensional representation within each task grouping have been propose . the programmer can impose a penalty on task from different group which encourage the two representation to be orthogonal . experiment on synthetic and real data have indicate that incorporate unrelated task can result in significant improvement over standard multitask learn method . related to multitask learning be the concept of knowledge transfer . whereas traditional multitask learning implies that a shared representation is develop concurrently across task transfer of knowledge implies a sequentially share representation . large scale machine learn project such a the deep convolutional neural network googlenet an imagebased object classifier can develop robust representation which may be useful to further algorithm learning relate task . for example the pretrained model can be use a a feature extractor to perform preprocessing for another learn algorithm . or the pretrained model can be use to initialize a model with similar architecture which is then finetuned to learn a different classification task . traditionally multitask learn and transfer of knowledge are apply to stationary learn setting . their extension to nonstationary environment is term group online adaptive learn goal . sharing information could be particularly useful if learner operate in continuously change environment because a learner could benefit from previous experience of another learner to quickly adapt to their new environment . such groupadaptive learning have numerous application from predict financial timeseries through content recommendation system to visual understanding for adaptive autonomous agent . the mtl problem can be cast within the context of rkhsvv a complete inner product space of vectorvalued function equip with a reproducing kernel . in particular recent focus ha be on case where task structure can be identify via a separable kernel describe below . the presentation here derives from ciliberto et al . suppose the training data set be formula with formula formula where formula index task and formula . let formula . in this set there be a consistent input and output space and the same loss function formula for each task . this result in the regularize machine learn problem where formula be a vector value reproduce kernel hilbert space with function formula have component formula . the reproducing kernel for the space formula of function formula be a symmetric matrixvalued function formula such that formula and the follow reproducing property hold the form of the kernel formula induce both the representation of the feature space and structure the output across task . a natural simplification be to choose a separable kernel which factor into separate kernel on the input space formula and on the task formula . in this case the kernel relate scalar component formula and formula is give by formula . for vector valued function formula we can write formula where formula be a scalar reproducing kernel and formula be a symmetric positive semidefinite formula matrix . henceforth denote formula . this factorization property separability imply the input feature space representation doe not vary by task . that be there be no interaction between the input kernel and the task kernel . the structure on task is represent solely by formula . method for nonseparable kernel formula be an current field of research . for the separable case the representation theorem is reduce to formula . the model output on the training data be then formula where formula be the formula empirical kernel matrix with entry formula and formula be the formula matrix of row formula . with the separable kernel equation can be rewrite aswhere formula be a weighted average of formula applied entrywise to y and kca . the weight be zero if formula be a missing observation . note the second term in can be derive a followsformulaformula bilinearityformula reproducing propertyformulathere be three largely equivalent ways to represent task structure through a regularizer through an output metric and through an output mapping . regularizer with the separable kernel it can be show below that formula where formula be the formula element of the pseudoinverse of formula and formula be the rkhs base on the scalar kernel formula and formula . this formulation show that formula control the weight of the penalty associate with formula . note that formula arises from formula . proofformulaformulaformulaformulaformulaformulaformulaformulaformulaoutput metric an alternative output metric on formula can be induce by the inner product formula . with the squared loss there be an equivalence between the separable kernel formula under the alternative metric and formula under the canonical metric . output mapping output can be map a formula to a high dimensional space to encode complex structure such a tree graph and string . for linear map formula with appropriate choice of separable kernel it can be show that formula . via the regularizer formulation one can represent a variety of task structure easily . learn problem can be generalize to admit learning task matrix a a followschoice of formula must be design to learn matrix a of a give type . see special case below . restrict to the case of convex loss and coercive penalty ciliberto et al . have show that although is not convex jointly in c and a a related problem be jointly convex . specifically on the convex set formula the equivalent problemis convex with the same minimum value . and if formula be a minimizer for then formula be a minimizer for . the perturbation via the barrier formula force the objective function to be equal to formula on the boundary of formula . spectral penalty dinnuzo et al suggested setting f a the frobenius norm formula . they optimize directly use block coordinate descent not accounting for difficulty at the boundary of formula . clustered task learn jacob et al suggested to learn a in the setting where t task are organize in r disjoint cluster . in this case let formula be the matrix with formula . set formula and formula the task matrix formula can be parameterized a a function of formula formula with term that penalize the average between cluster variance and within cluster variance respectively of the task prediction . m be not convex but there be a convex relaxation formula . in this formulation formula . nonconvex penalty penalty can be construct such that a is constrain to be a graph laplacian or that a have low rank factorization . however these penalty are not convex and the analysis of the barrier method propose by ciliberto et al . doe not go through in these case . nonseparable kernel separable kernel are limit in particular they do not account for structure in the interaction space between the input and output domain jointly . future work is need to develop model for these kernel . use the principle of mtl technique for collaborative spam filter that facilitates personalization have be propose . in large scale open membership email system most user do not label enough message for an individual local classifier to be effective while the data be too noisy to be use for a global filter across all user . a hybrid globalindividual classifier can be effective at absorb the influence of user who label email very diligently from the general public . this can be accomplish while still provide sufficient quality to user with few labeled instance . use boosted decision tree one can enable implicit data share and regularization . this learning method can be use on websearch rank data set . one example be to use rank data set from several country . here multitask learning be particularly helpful a data set from different country vary largely in size because of the cost of editorial judgment . it ha been demonstrate that learn various task jointly can lead to significant improvement in performance with surprising reliability . in order to facilitate transfer of knowledge it infrastructure is be developed . one such project roboearth aim to set up an open source internet database that can be access and continually update from around the world . the goal be to facilitate a cloudbased interactive knowledge base accessible to technology company and academic institution which can enhance the sense acting and learn capability of robot and other artificial intelligence agent . the multitask learn via structural regularization malsar matlab package implement the follow multitask learn algorithm .
in machine learn feature learn or representation learning be a set of technique that allow a system to automatically discover the representation need for feature detection or classification from raw data . this replace manual feature engineering and allow a machine to both learn the feature and use them to perform a specific task . feature learning be motivate by the fact that machine learning task such a classification often require input that be mathematically and computationally convenient to process . however realworld data such a image video and sensor data ha not yield to attempt to algorithmically define specific feature . an alternative be to discover such feature or representation through examination without rely on explicit algorithm . feature learn can be either supervise or unsupervised . supervise feature learning is learn feature from labeled data . the data label allow the system to compute an error term the degree to which the system fails to produce the label which can then be use a feedback to correct the learning process reduceminimize the error . approach include . dictionary learning develop a set dictionary of representative element from the input data such that each data point can be represent a a weight sum of the representative element . the dictionary element and the weight may be find by minimize the average representation error over the input data together with l regularization on the weight to enable sparsity i . the representation of each data point have only a few nonzero weight . supervise dictionary learn exploit both the structure underlie the input data and the label for optimize the dictionary element . for example a supervise dictionary learning technique applied dictionary learn on classification problem by jointly optimize the dictionary element weight for representing data point and parameter of the classifier base on the input data . in particular a minimization problem is formulate where the objective function consists of the classification error the representation error an l regularization on the representing weight for each data point to enable sparse representation of data and an l regularization on the parameter of the classifier . neural network be a family of learn algorithm that use a network consisting of multiple layer of interconnected node . it is inspire by the animal nervous system where the node are view a neuron and edge are view a synapsis . each edge have an associated weight and the network define computational rule for pass input data from the network input layer to the output layer . a network function associate with a neural network characterize the relationship between input and output layer which is parameterized by the weight . with appropriately define network function various learning task can be perform by minimize a cost function over the network function weight . multilayer neural network can be use to perform feature learn since they learn a representation of their input at the hidden layer which is subsequently use for classification or regression at the output layer . unsupervised feature learning be learn feature from unlabeled data . the goal of unsupervised feature learning is often to discover lowdimensional feature that capture some structure underlying the highdimensional input data . when the feature learning is perform in an unsupervised way it enable a form of semisupervised learn where feature learn from an unlabeled dataset are then employ to improve performance in a supervise setting with labeled data . several approach are introduce in the follow . kmeans clustering be an approach for vector quantization . in particular give a set of n vector kmeans clustering group them into k cluster i . subsets in such a way that each vector belong to the cluster with the close mean . the problem is computationally nphard although suboptimal greedy algorithm have been develop . kmeans clustering can be use to group an unlabeled set of input into k cluster and then use the centroid of these cluster to produce feature . these feature can be produce in several way . the simple be to add k binary feature to each sample where each feature j have value one iff the jth centroid learn by kmeans be the close to the sample under consideration . it be also possible to use the distance to the cluster a feature perhaps after transform them through a radial basis function a technique that ha been use to train rbf network . coates and ng note that certain variant of kmeans behave similarly to sparse cod algorithm . in a comparative evaluation of unsupervised feature learn method coates lee and ng find that kmeans clustering with an appropriate transformation outperforms the more recently invent autoencoders and rbms on an image classification task . kmeans also improve performance in the domain of nlp specifically for namedentity recognition there it compete with brown clustering as well a with distribute word representation also know a neural word embeddings . principal component analysis pca is often use for dimension reduction . give an unlabeled set of n input data vector pca generates p which be much small than the dimension of the input data right singular vector corresponding to the p large singular value of the data matrix where the kth row of the data matrix be the kth input data vector shift by the sample mean of the input i . subtract the sample mean from the data vector . equivalently these singular vector be the eigenvectors correspond to the p large eigenvalue of the sample covariance matrix of the input vector . these p singular vector be the feature vector learn from the input data and they represent direction along which the data have the large variation . pca be a linear feature learn approach since the p singular vector be linear function of the data matrix . the singular vector can be generate via a simple algorithm with p iteration . in the ith iteration the projection of the data matrix on the ith eigenvector be subtracted and the ith singular vector be find a the right singular vector correspond to the large singular of the residual data matrix . pca have several limitation . first it assume that the direction with large variance be of most interest which may not be the case . pca only relies on orthogonal transformation of the original data and it exploit only the first and secondorder moment of the data which may not well characterize the data distribution . furthermore pca can effectively reduce dimension only when the input data vector are correlate which result in a few dominant eigenvalue . local linear embedding lle be a nonlinear learning approach for generate lowdimensional neighborpreserving representation from unlabeled highdimension input . the approach wa propose by roweis and saul . the general idea of lle be to reconstruct the original highdimensional data use lowerdimensional point while maintain some geometric property of the neighborhood in the original data set . lle consists of two major step . the first step be for neighborpreserving where each input data point xi is reconstruct a a weight sum of k near neighbor data point and the optimal weight are find by minimize the average squared reconstruction error i . difference between an input point and it reconstruction under the constraint that the weight associate with each point sum up to one . the second step be for dimension reduction by look for vector in a lowerdimensional space that minimize the representation error use the optimized weight in the first step . note that in the first step the weight are optimize with fix data which can be solve a a least square problem . in the second step lowerdimensional point are optimize with fixed weight which can be solve via sparse eigenvalue decomposition . the reconstruction weight obtain in the first step capture the intrinsic geometric property of a neighborhood in the input data . it is assume that original data lie on a smooth lowerdimensional manifold and the intrinsic geometric property capture by the weight of the original data are also expect to be on the manifold . this be why the same weight are use in the second step of lle . compare with pca lle be more powerful in exploit the underlying data structure . independent component analysis ica be a technique for form a data representation use a weight sum of independent nongaussian component . the assumption of nongaussian is impose since the weight cannot be uniquely determine when all the component follow gaussian distribution . unsupervised dictionary learn doe not utilize data label and exploit the structure underlie the data for optimize dictionary element . an example of unsupervised dictionary learning be sparse coding which aim to learn basis function dictionary element for data representation from unlabeled input data . sparse coding can be apply to learn overcomplete dictionary where the number of dictionary element be large than the dimension of the input data . aharon et al . propose algorithm ksvd for learn a dictionary of element that enable sparse representation . the hierarchical architecture of the biological neural system inspire deep learning architecture for feature learning by stack multiple layer of learn node . these architecture are often design base on the assumption of distribute representation observe data is generate by the interaction of many different factor on multiple level . in a deep learn architecture the output of each intermediate layer can be view a a representation of the original input data . each level use the representation produce by previous level a input and produce new representation a output which is then feed to high level . the input at the bottom layer be raw data and the output of the final layer be the final lowdimensional feature or representation . restrict boltzmann machine rbms are often use a a building block for multilayer learn architecture . an rbm can be represent by an undirected bipartite graph consist of a group of binary hidden variable a group of visible variable and edge connect the hidden and visible node . it be a special case of the more general boltzmann machine with the constraint of no intranode connection . each edge in an rbm is associate with a weight . the weight together with the connection define an energy function base on which a joint distribution of visible and hidden node can be devise . base on the topology of the rbm the hidden visible variable be independent conditioned on the visible hidden variable . such conditional independence facilitates computation . an rbm can be view a a single layer architecture for unsupervised feature learn . in particular the visible variable correspond to input data and the hidden variables correspond to feature detector . the weight can be train by maximize the probability of visible variable use hintons contrastive divergence cd algorithm . in general training rbm by solve the maximization problem tend to result in nonsparse representation . sparse rbm wa propose to enable sparse representation . the idea be to add a regularization term in the objective function of data likelihood which penalize the deviation of the expect hidden variable from a small constant formula . an autoencoder consisting of an encoder and a decoder be a paradigm for deep learn architecture . an example is provide by hinton and salakhutdinov where the encoder use raw data e . image a input and produce feature or representation a output and the decoder use the extracted feature from the encoder a input and reconstruct the original input raw data a output . the encoder and decoder are construct by stack multiple layer of rbms . the parameter involve in the architecture were originally train in a greedy layerbylayer manner after one layer of feature detector is learn they are feed up a visible variable for train the correspond rbm . current approach typically apply endtoend train with stochastic gradient descent method . train can be repeat until some stopping criterion are satisfy .
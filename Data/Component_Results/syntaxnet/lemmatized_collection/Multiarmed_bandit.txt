in probability theory the multiarmed bandit problem sometimes call the kref namedoi . aref or narmed bandit problem be a problem in which a fix limited set of resource must be allocate between compete alternative choice in a way that maximize their expect gain when each choice property are only partially know at the time of allocation and may become good understood a time pass or by allocate resource to the choice . the name come from imagine a gambler at a row of slot machine sometimes know a onearmed bandit who have to decide which machine to play how many time to play each machine and in which order to play them and whether to continue with the current machine or try a different machine . the multiarmed bandit problem also fall into the broad category of stochastic scheduling . in the problem each machine provide a random reward from a probability distribution specific to that machine . the objective of the gambler be to maximize the sum of reward earn through a sequence of lever pull . the crucial tradeoff the gambler face at each trial be between exploitation of the machine that have the highest expected payoff and exploration to get more information about the expected payoff of the other machine . the tradeoff between exploration and exploitation is also face in reinforcement learning . in practice multiarmed bandit have been use to model problem such a manage research project in a large organization like a science foundation or a pharmaceutical company . in early version of the problem the gambler begin with no initial knowledge about the machine . herbert robbins in realize the importance of the problem constructed convergent population selection strategy in some aspect of the sequential design of experiment . a theorem the gittins index first publish by john c . gittins give an optimal policy for maximize the expect discounted reward . the multiarmed bandit problem model an agent that simultaneously attempt to acquire new knowledge called exploration and optimize his or her decision base on existing knowledge call exploitation . the agent attempt to balance these competing task in order to maximize his total value over the period of time consider . there be many practical application of the bandit model for example . in these practical example the problem require balancing reward maximization base on the knowledge already acquire with attempt new action to further increase knowledge . this be known a the exploitation vs . exploration tradeoff in reinforcement learn . the model ha also been use to control dynamic allocation of resource to different project answer the question of which project to work on give uncertainty about the difficulty and payoff of each possibility . originally consider by ally scientist in world war ii it prove so intractable that accord to peter whittle the problem wa propose to be drop over germany so that german scientist could also waste their time on it . the version of the problem now commonly analyze wa formulate by herbert robbins in . the multiarmed bandit short bandit or mab can be see a a set of real distribution formula each distribution being associate with the reward deliver by one of the formula lever . let formula be the mean value associate with these reward distribution . the gambler iteratively play one lever per round and observe the associated reward . the objective be to maximize the sum of the collect reward . the horizon formula be the number of round that remain to be played . the bandit problem be formally equivalent to a onestate markov decision process . the regret formula after formula round is define a the expect difference between the reward sum associate with an optimal strategy and the sum of the collect reward formula where formula be the maximal reward mean formula and formula be the reward in round t . a zeroregret strategy be a strategy whose average regret per round formula tends to zero with probability when the number of played round tend to infinity . intuitively zeroregret strategy are guarantee to converge to a not necessarily unique optimal strategy if enough round be played . a common formulation be the binary multiarmed bandit or bernoulli multiarmed bandit which issue a reward of one with probability formula and otherwise a reward of zero . another formulation of the multiarmed bandit have each arm represent an independent markov machine . each time a particular arm is play the state of that machine advance to a new one chosen accord to the markov state evolution probability . there be a reward depending on the current state of the machine . in a generalisation call the restless bandit problem the state of nonplayed arm can also evolve over time . there ha also be discussion of system where the number of choice about which arm to play increase over time . computer science researcher have study multiarmed bandit under worstcase assumption obtain algorithm to minimize regret in both finite and infinite asymptotic time horizon for both stochastic and nonstochastic arm payoffs . a major breakthrough be the construction of optimal population selection strategy or policy that possess uniformly maximum convergence rate to the population with high mean in the work describe below . in the paper asymptotically efficient adaptive allocation rule lai and robbins follow paper of robbins and his coworkers go back to robbins in the year constructed convergent population selection policy that possess the fast rate of convergence to the population with high mean for the case that the population reward distribution be the oneparameter exponential family . then in katehakis and robbins simplification of the policy and the main proof were give for the case of normal population with known variance . the next notable progress wa obtain by burnetas and katehakis in the paper optimal adaptive policy for sequential allocation problem where index base policy with uniformly maximum convergence rate were construct under more general condition that include the case in which the distribution of outcome from each population depend on a vector of unknown parameter . burnetas and katehakis also provide an explicit solution for the important case in which the distribution of outcome follow arbitrary i . nonparametric discrete univariate distribution . later in optimal adaptive policy for markov decision process burnetas and katehakis study the much large model of markov decision process under partial information where the transition law andor the expect one period reward may depend on unknown parameter . in this work the explicit form for a class of adaptive policy that possess uniformly maximum convergence rate property for the total expect finite horizon reward were construct under sufficient assumption of finite stateaction space and irreducibility of the transition law . a main feature of these policy be that the choice of action at each state and time period is base on index that be inflation of the righthand side of the estimated average reward optimality equation . these inflation have recently been call the optimistic approach in the work of tewari and bartlett ortner filippi capp and garivier and honda and takemura . many strategy exist which provide an approximate solution to the bandit problem and can be put into the four broad category detail below . semiuniform strategy be the early and simple strategy discover to approximately solve the bandit problem . all those strategy have in common a greedy behavior where the best lever base on previous observation be always pulled except when a uniformly random action is take . probability matching strategy reflect the idea that the number of pull for a give lever should match it actual probability of be the optimal lever . probability matching strategy are also know a thompson sampling or bayesian bandit and be surprisingly easy to implement if you can sample from the posterior for the mean value of each alternative . probability matching strategy also admit solution to socalled contextual bandit problem . pricing strategy establish a price for each lever . for example as illustrate with the poker algorithm the price can be the sum of the expect reward plus an estimation of extra future reward that will gain through the additional knowledge . the lever of high price be always pulled . these strategy minimize the assignment of any patient to an inferior arm physician duty . in a typical case they minimize expect success lose esl that be the expect number of favorable outcome that were miss because of assignment to an arm later prove to be inferior . another version minimizes resource waste on any inferior more expensive treatment . a particularly useful version of the multiarmed bandit be the contextual multiarmed bandit problem . in this problem in each iteration an agent have to choose between arm . before make the choice the agent see a ddimensional feature vector context vector . associate with the current iteration . the learner use these context vector along with the reward of the arm play in the past to make the choice of the arm to play in . the current iteration . over time the learner aim be to collect enough information about how the context vector and reward relate to each other so that it can predict the next best arm to play by look at the feature vector . many strategy exist that provide an approximate solution to the contextual bandit problem and can be put into two broad category detail below . in practice there be usually a cost associate with the resource consume by each action and the total cost is limit by a budget in many application such a crowdsourcing and clinical trial . constrain contextual bandit ccb be such a model that consider both the time and budget constraint in a multiarmed bandit setting . badanidiyuru et al . first study contextual bandit with budget constraint also refer to a resourceful contextual bandit and show that a formula regret be achievable . however their work focus on a finite set of policy and the algorithm be computationally inefficient . a simple algorithm with logarithmic regret is propose in . another variant of the multiarmed bandit problem is call the adversarial bandit first introduce by auer and cesabianchi . in this variant at each iteration an agent choose an arm and an adversary simultaneously choose the payoff structure for each arm . this be one of the strong generalization of the bandit problem a it remove all assumption of the distribution and a solution to the adversarial bandit problem be a generalize solution to the more specific bandit problem . in the original specification and in the above variant the bandit problem is specify with a discrete and finite number of arm often indicate by the variable formula . in the infinite armed case introduce by agarwal the arm be a continuous variable in formula dimension . garivier and moulines derive some of the first result with respect to bandit problem where the underlying model can change during play . a number of algorithm were present to deal with this case include discount ucb and slidingwindow ucb . another work by burtini et al . introduce a weight least square thompson sample approach wlsts which prove beneficial in both the known and unknown nonstationary case . in the known nonstationary case the author in produce an alternative solution a variant of ucb name adjust upper confidence bound aucb which assume a stochastic model and provide upperbounds of the regret . many variant of the problem have be propose in recent year . the duel bandit variant wa introduce by yue et al . to model the explorationversusexploitation tradeoff for relative feedback . in this variant the gambler is allow to pull two lever at the same time but they only get a binary feedback telling which lever provide the best reward . the difficulty of this problem stem from the fact that the gambler have no way of directly observe the reward of their action . the early algorithm for this problem are interleavefiltering beatthemean . the relative feedback of dueling bandit can also lead to vote paradox . a solution be to take the condorcet winner a a reference . more recently researcher have generalize algorithm from traditional mab to duel bandit relative upper confidence bound rucb relative exponential weighing rex . copeland confidence bound ccb relative minimum empirical divergence rmed and double thompson sample dts . the collaborative filtering bandit i . cofiba wa introduce by li and karatzoglou and gentile sigir where the classical collaborative filtering and contentbased filtering method try to learn a static recommendation model given train data . these approach be far from ideal in highly dynamic recommendation domains such a news recommendation and computational advertisement where the set of item and user be very fluid . in this work they investigate an adaptive clustering technique for content recommendation base on explorationexploitation strategy in contextual multiarmed bandit setting . their algorithm cofiba pronounce a coffee bar take into account the collaborative effect that arise due to the interaction of the user with the item by dynamically group user base on the item under consideration and at the same time grouping item base on the similarity of the clustering induce over the user . the resulting algorithm thus take advantage of preference pattern in the data in a way akin to collaborative filtering method . they provide an empirical analysis on mediumsize realworld datasets showing scalability and increase prediction performance a measure by clickthrough rate over stateoftheart method for cluster bandit . they also provide a regret analysis within a standard linear stochastic noise setting . the combinatorial multiarmed bandit cmab problem arises when instead of a single discrete variable to choose from an agent need to choose value for a set of variable . assume each variable is discrete the number of possible choice per iteration be exponential in the number of variable . several cmab setting have been study in the literature from setting where the variable be binary to more general setting where each variable can take an arbitrary set of value .
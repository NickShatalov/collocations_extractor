in statistical learning theory a representer theorem be any of several related result state that a minimizer formula of a regularize empirical risk function define over a reproduce kernel hilbert space can be represent a a finite linear combination of kernel product evaluate on the input point in the training set data . the follow representer theorem and it proof be due to schlkopf herbrich and smola . theorem let formula be a nonempty set and formula a positivedefinite realvalued kernel on formula with correspond reproducing kernel hilbert space formula . give a training sample formula a strictly monotonically increasing realvalued function formula and an arbitrary empirical risk function formula then for any formula satisfying . formula admits a representation of the form . where formula for all formula . proof . define a mapping . so that formula be itself a map formula . since formula be a reproducing kernel then . where formula be the inner product on formula . give any formula one can use orthogonal projection to decompose any formula into a sum of two function one lie in formula and the other lying in the orthogonal complement . where formula for all formula . the above orthogonal decomposition and the reproduce property together show that apply formula to any training point formula produce . which we observe be independent of formula . consequently the value of the empirical risk formula in be likewise independent of formula . for the second term the regularization term since formula be orthogonal to formula and formula be strictly monotonic we have . therefore setting formula doe not affect the first term of while it strictly decrease the second term . consequently any minimizer formula in must have formula i . it must be of the form . which be the desired result . the theorem state above be a particular example of a family of result that are collectively refer to a representer theorem here we describe several such . the first statement of a representer theorem be due to kimeldorf and wahba for the special case in which . for formula . schlkopf herbrich and smola generalize this result by relax the assumption of the squaredloss cost and allow the regularizer to be any strictly monotonically increase function formula of the hilbert space norm . it be possible to generalize far by augment the regularize empirical risk function through the addition of unpenalized offset term . for example schlkopf herbrich and smola also consider the minimization . we consider function of the form formula where formula and formula be an unpenalized function lie in the span of a finite set of realvalued function formula . under the assumption that the formula matrix formula have rank formula they show that the minimizer formula in formula . admit a representation of the form . where formula and the formula be all uniquely determined . the condition under which a representer theorem exists were investigate by argyriou miccheli and pontil who prove the follow . theorem let formula be a nonempty set formula a positivedefinite realvalued kernel on formula with correspond reproduce kernel hilbert space formula and let formula be a differentiable regularization function . then give a training sample formula and an arbitrary empirical risk function formula a minimizer . of the regularized empirical risk minimization problem admit a representation of the form . where formula for all formula if and only if there exist a nondecreasing function formula for which . effectively this result provide a necessary and sufficient condition on a differentiable regularizer formula under which the correspond regularize empirical risk minimization formula will have a representer theorem . in particular this show that a broad class of regularize risk minimization much broad than those originally consider by kimeldorf and wahba have representer theorem . representer theorem be useful from a practical standpoint because they dramatically simplify the regularize empirical risk minimization problem formula . in most interesting application the search domain formula for the minimization will be an infinitedimensional subspace of formula and therefore the search a write doe not admit implementation on finitememory and finiteprecision computer . in contrast the representation of formula afford by a representer theorem reduce the original infinitedimensional minimization problem to a search for the optimal formuladimensional vector of coefficient formula formula can then be obtain by apply any standard function minimization algorithm . consequently representer theorem provide the theoretical basis for the reduction of the general machine learn problem to algorithm that can actually be implement on computer in practice .
in machine learning manifold regularization be a technique for use the shape of a dataset to constrain the function that should be learn on that dataset . in many machine learn problem the data to be learn do not cover the entire input space . for example a facial recognition system may not need to classify any possible image but only the subset of image that contain face . the technique of manifold learn assumes that the relevant subset of data come from a manifold a mathematical structure with useful property . the technique also assume that the function to be learn is smooth data with different label be not likely to be close together and so the labeling function should not change quickly in area where there be likely to be many data point . because of this assumption a manifold regularization algorithm can use unlabeled data to inform where the learned function is allow to change quickly and where it is not use an extension of the technique of tikhonov regularization . manifold regularization algorithm can extend supervise learn algorithm in semisupervised learning and transductive learning setting where unlabeled data be available . the technique ha been use for application include medical image geographical imaging and object recognition . manifold regularization be a type of regularization a family of technique that reduces overfitting and ensure that a problem is wellposed by penalize complex solution . in particular manifold regularization extends the technique of tikhonov regularization a apply to reproduce kernel hilbert space rkhss . under standard tikhonov regularization on rkhss a learning algorithm attempt to learn a function formula from among a hypothesis space of function formula . the hypothesis space be an rkhs meaning that it is associate with a kernel formula and so every candidate function formula have a norm formula which represent the complexity of the candidate function in the hypothesis space . when the algorithm consider a candidate function it take it norm into account in order to penalize complex function . formally give a set of label training data formula with formula and a loss function formula a learn algorithm using tikhonov regularization will attempt to solve the expressionwhere formula be a hyperparameter that control how much the algorithm will prefer simple function to function that fit the data better . manifold regularization add a second regularization term the intrinsic regularizer to the ambient regularizer use in standard tikhonov regularization . under the manifold assumption in machine learn the data in question do not come from the entire input space formula but instead from a nonlinear manifold formula . the geometry of this manifold the intrinsic space is use to determine the regularization norm . there be many possible choice for formula . many natural choice involve the gradient on the manifold formula which can provide a measure of how smooth a target function be . a smooth function should change slowly where the input data be dense that be the gradient formula should be small where the marginal probability density formula the probability density of a randomly drawn data point appear at formula be large . this give one appropriate choice for the intrinsic regularizerin practice this norm cannot be compute directly because the marginal distribution formula be unknown but it can be estimate from the provided data . in particular if the distance between input point are interpret a a graph then the laplacian matrix of the graph can help to estimate the marginal distribution . suppose that the input data include formula label example pair of an input formula and a label formula and formula unlabeled example input without associated label . define formula to be a matrix of edge weight for a graph where formula be a distance measure between the data point formula and formula . define formula to be a diagonal matrix with formula and formula to be the laplacian matrix formula . then a the number of data point formula increase formula converges to the laplacebeltrami operator formula which be the divergence of the gradient formula . then if formula be a vector of the value of formula at the data formula the intrinsic norm can be estimatedas the number of data point formula increase this empirical definition of formula converges to the definition when formula be known . use the weight formula and formula for the ambient and intrinsic regularizers the final expression to be solve becomesas with other kernel method formula may be an infinitedimensional space so if the regularization expression cannot be solve explicitly it be impossible to search the entire space for a solution . instead a representer theorem show that under certain condition on the choice of the norm formula the optimal solution formula must be a linear combination of the kernel center at each of the input point for some weight formulausing this result it be possible to search for the optimal solution formula by search the finitedimensional space define by the possible choice of formula . manifold regularization can extend a variety of algorithm that can be express use tikhonov regularization by choose an appropriate loss function formula and hypothesis space formula . two commonly use example be the family of support vector machine and regularize least square algorithm . regularized least square include the ridge regression algorithm the relate algorithm of lasso and elastic net regularization can be express a support vector machine . the extended version of these algorithm are call laplacian regularize least square abbreviate laprls and laplacian support vector machine lapsvm respectively . regularized least square rls be a family of regression algorithm algorithm that predict a value formula for it input formula with the goal that the predicted value should be close to the true label for the data . in particular rls is design to minimize the mean square error between the predict value and the true label subject to regularization . ridge regression be one form of rls in general rls be the same a ridge regression combine with the kernel method . the problem statement for rls result from choose the loss function formula in tikhonov regularization to be the mean squared errorthanks to the representer theorem the solution can be write a a weight sum of the kernel evaluate at the data pointsand solving for formula giveswhere formula be define to be the kernel matrix with formula and formula be the vector of data label . add a laplacian term for manifold regularization give the laplacian rls statementthe representer theorem for manifold regularization again givesand this yield an expression for the vector formula . let formula be the kernel matrix a above formula be the vector of data label and formula be the formula block matrix formulawith a solution oflaprls ha be apply to problems including sensor networksmedical imagingobject detectionspectroscopydocument classificationdrugprotein interactionsand compressing image and video . support vector machine svms be a family of algorithm often use for classify data into two or more group or class . intuitively an svm draw a boundary between class so that the closest label example to the boundary be as far away a possible . this can be directly express a a linear program but it be also equivalent to tikhonov regularization with the hinge loss function formulaadding the intrinsic regularization term to this expression give the lapsvm problem statementagain the representer theorem allow the solution to be express in term of the kernel evaluate at the data pointsformula can be find by write the problem a a linear program and solve the dual problem . again let formula be the kernel matrix and formula be the block matrix formula the solution can be show to bewhere formula be the solution to the dual problemand formula is define bylapsvm ha be apply to problem include geographical imagingmedical imagingface recognitionmachine maintenanceand braincomputer interface .
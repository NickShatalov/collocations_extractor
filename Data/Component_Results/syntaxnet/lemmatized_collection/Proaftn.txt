granular compute grc be an emerge computing paradigm of information processing . it concern the processing of complex information entity called information granule which arise in the process of data abstraction and derivation of knowledge from information or data . generally speaking information granule be collection of entity that usually originate at the numeric level and are arrange together due to their similarity functional or physical adjacency indistinguishability coherency or the like . at present granular compute be more a theoretical perspective than a coherent set of method or principle . a a theoretical perspective it encourage an approach to data that recognizes and exploit the knowledge present in data at various level of resolution or scale . in this sense it encompass all method which provide flexibility and adaptability in the resolution at which knowledge or information is extract and represent . as mention above granular computing be not an algorithm or process there be no particular method that is call granular compute . it be rather an approach to look at data that recognize how different and interest regularity in the data can appear at different level of granularity much a different feature become salient in satellite image of greater or less resolution . on a lowresolution satellite image for example one might notice interest cloud pattern represent cyclone or other largescale weather phenomenon while in a higherresolution image one miss these largescale atmospheric phenomenon but instead notice smallerscale phenomenon such a the interest pattern that be the street of manhattan . the same be generally true of all data at different resolution or granularity different feature and relationships emerge . the aim of granular computing be to try to take advantage of this fact in design more effective machinelearning and reason system . there be several type of granularity that are often encounter in data mining and machine learn and we review them belowone type of granulation be the quantization of variable . it be very common that in data mining or machinelearning application the resolution of variable need to be decrease in order to extract meaningful regularity . an example of this would be a variable such a outside temperature formula which in a given application might be record to several decimal place of precision depending on the sensing apparatus . however for purpose of extracting relationship between outside temperature and say number of healthclub application formula it will generally be advantageous to quantize outside temperature into a small number of interval . there be several interrelate reason for granulate variable in this fashionfor example a simple learner or pattern recognition system may seek to extract regularity satisfy a conditional probability threshold such a formula . in the special case where formula this recognition system be essentially detecting logical implication of the form formula or in word if formula then formula . the system ability to recognize such implication or in general conditional probability exceeding threshold be partially contingent on the resolution with which the system analyze the variable . a an example of this last point consider the feature space shown to the right . the variable may each be regard at two different resolution . variable formula may be regard at a high quaternary resolution wherein it take on the four value formula or at a low binary resolution wherein it take on the two value formula . similarly variable formula may be regard at a high quaternary resolution or at a low binary resolution where it take on the value formula or formula respectively . it will be note that at the high resolution there be no detectable implication of the form formula since every formula is associate with more than one formula and thus for all formula formula . however at the low binary variable resolution two bilateral implication become detectable formula and formula since every formula occurs iff formula and formula occurs iff formula . thus a pattern recognition system scanning for implication of this kind would find them at the binary variable resolution but would fail to find them at the high quaternary variable resolution . it be not feasible to exhaustively test all possible discretization resolution on all variable in order to see which combination of resolution yield interest or significant result . instead the feature space must be preprocessed often by an entropy analysis of some kind so that some guidance can be give a to how the discretization process should proceed . moreover one cannot generally achieve good result by naively analyzing and discretizing each variable independently since this may obliterate the very interaction that we had hop to discover . a sample of paper that address the problem of variable discretization in general and multiplevariable discretization in particular be a follow variable granulation be a term that could describe a variety of technique most of which are aim at reduce dimensionality redundancy and storage requirement . we briefly describe some of the idea here and present pointer to the literature . a number of classical method such a principal component analysis multidimensional scaling factor analysis and structural equation modeling and their relative fall under the genus of variable transformation . also in this category be more modern area of study such a dimensionality reduction projection pursuit and independent component analysis . the common goal of these method in general be to find a representation of the data in term of new variable which be a linear or nonlinear transformation of the original variable and in which important statistical relationship emerge . the result variable set are almost always small than the original variable set and hence these method can be loosely say to impose a granulation on the feature space . these dimensionality reduction method be all review in the standard text such a and . a different class of variable granulation method derive more from data clustering methodology than from the linear system theory inform the above method . it wa note fairly early that one may consider cluster relate variable in just the same way that one considers cluster related data . in data cluster one identifies a group of similar entity use a measure of similarity suitable to the domain and then in some sense replaces those entities with a prototype of some kind . the prototype may be the simple average of the data in the identified cluster or some other representative measure . but the key idea be that in subsequent operation we may be able to use the single prototype for the data cluster along with perhaps a statistical model describing how exemplar are derive from the prototype to stand in for the much large set of exemplar . these prototype be generally such a to capture most of the information of interest concern the entity . similarly it be reasonable to ask whether a large set of variable might be aggregate into a small set of prototype variable that capture the most salient relationship between the variable . although variable clustering method base on linear correlation have been propose more powerful method of variable clustering are base on the mutual information between variable . watanabe ha show that for any set of variable one can construct a polytomic i . nary tree represent a series of variable agglomeration in which the ultimate total correlation among the complete variable set be the sum of the partial correlation exhibit by each agglomerating subset see figure . watanabe suggest that an observer might seek to thus partition a system in such a way a to minimize the interdependence between the part . a if they were look for a natural division or a hidden crack . one practical approach to build such a tree be to successively choose for agglomeration the two variable either atomic variable or previously agglomerate variable which have the high pairwise mutual information . the product of each agglomeration be a new construct variable that reflect the local joint distribution of the two agglomerate variable and thus posse an entropy equal to their joint entropy . from a procedural standpoint this agglomeration step involves replace two column in the attributevalue tablerepresenting the two agglomerate variableswith a single column that have a unique value for every unique combination of value in the replaced column . no information is lose by such an operation however it should be note that if one is explore the data for intervariable relationship it would generally not be desirable to merge redundant variable in this way since in such a context it be likely to be precisely the redundancy or dependency between variable that be of interest and once redundant variable are merge their relationship to one another can no longer be study . in database system aggregation see e . olap aggregation and business intelligence system result in transform original data table often call information system into the table with different semantics of row and column wherein the row correspond to the group granule of original tuples and the column express aggregate information about original value within each of the group . such aggregation are usually base on sql and it extension . the resulting granule usually correspond to the group of original tuples with the same value or range over some preselected original column . there be also other approach wherein the group are define base on e . physical adjacency of row . for example infobright implement a database engine wherein data wa partition onto rough row each consist of k of physically consecutive or almost consecutive row . rough row were automatically label with compact information about their value on data column often involving multicolumn and multitable relationship . it result in a high layer of granulate information where object correspond to rough row and attribute to various aspect of rough information . database operation could be efficiently supported within such a new framework with an access to the original data piece still available . the origin of the granular computing ideology are to be find in the rough set and fuzzy set literature . one of the key insight of rough set researchalthough by no mean unique to itis that in general the selection of different set of feature or variable will yield different concept granulation . here a in elementary rough set theory by concept we mean a set of entity that be indistinguishable or indiscernible to the observer i . a simple concept or a set of entity that is compose from such simple concept i . a complex concept . to put it in other word by project a data set valueattribute system onto different set of variable we recognize alternative set of equivalenceclass concept in the data and these different set of concept will in general be conducive to the extraction of different relationship and regularity . we illustrate with an example . consider the attributevalue system belowwhen the full set of attribute formula is consider we see that we have the follow seven equivalence class or primitive simple conceptsthus the two object within the first equivalence class formula cannot be distinguish from one another base on the available attribute and the three object within the second equivalence class formula cannot be distinguish from one another base on the available attribute . the remain five object be each discernible from all other object . now let u imagine a projection of the attribute value system onto attribute formula alone which would represent for example the view from an observer which be only capable of detect this single attribute . then we obtain the follow much coarse equivalence class structure . this be in a certain regard the same structure a before but at a low degree of resolution large grain size . just as in the case of value granulation discretizationquantization it be possible that relationship dependencies may emerge at one level of granularity that be not present at another . a an example of this we can consider the effect of concept granulation on the measure know a attribute dependency a simple relative of the mutual information . to establish this notion of dependency see also rough set let formula represent a particular concept granulation where each formula be an equivalence class from the concept structure induce by attribute set formula . for example if the attribute set formula consists of attribute formula alone a above then the concept structure formula will be compose of formula formula and formula . the dependency of attribute set formula on another attribute set formula formula is give bythat be for each equivalence class formula in formula we add up the size of it low approximation see rough set by the attribute in formula i . formula . more simply this approximation be the number of object which on attribute set formula can be positively identified a belong to target set formula . add across all equivalence class in formula the numerator above represents the total number of object whichbased on attribute set formulacan be positively categorize accord to the classification induce by attribute formula . the dependency ratio therefore express the proportion within the entire universe of such classifiable object in a sense capture the synchronization of the two concept structure formula and formula . the dependency formula can be interpret a a proportion of such object in the information system for which it suffice to know the value of attribute in formula to determine the value of attribute in formula ziarko shan . have gotten definition now out of the way we can make the simple observation that the choice of concept granularity i . choice of attribute will influence the detect dependency among attribute . consider again the attribute value table from abovelet u consider the dependency of attribute set formulaon attribute set formula . that be we wish to know what proportion of object can be correctly classify into class of formula base on knowledge of formula . the equivalence class of formula and of formula are show below . the object that can be definitively categorize accord to concept structure formula base on formula be those in the set formula and since there be six of these the dependency of formula on formula formula . this might be consider an interesting dependency in it own right but perhaps in a particular data mining application only strong dependency are desire . we might then consider the dependency of the smaller attribute set formulaon the attribute set formula . the move from formula to formula induce a coarsening of the class structure formula a will be see shortly . we wish again to know what proportion of object can be correctly classify into the now large class of formula base on knowledge of formula . the equivalence class of the new formula and of formula are show below . clearly formula have a coarser granularity than it do earlier . the object that can now be definitively categorize accord to the concept structure formula base on formula constitute the complete universe formula and thus the dependency of formula on formula formula . that be knowledge of membership according to category set formula be adequate to determine category membership in formula with complete certainty in this case we might say that formula . thus by coarsen the concept structure we be able to find a stronger deterministic dependency . however we also note that the class induce in formula from the reduction in resolution necessary to obtain this deterministic dependency be now themselves large and few in number a a result the dependency we find while strong may be less valuable to u than the weaker dependency find early under the high resolution view of formula . in general it be not possible to test all set of attribute to see which induce concept structure yield the strongest dependency and this search must be therefore be guide with some intelligence . paper which discuss this issue and others relate to intelligent use of granulation be those by y . yao and lotfi zadeh list in the reference below . another perspective on concept granulation may be obtain from work on parametric model of category . in mixture model learn for example a set of data is explain a a mixture of distinct gaussian or other distribution . thus a large amount of data is replace by a small number of distribution . the choice of the number of these distribution and their size can again be view a a problem of concept granulation . in general a good fit to the data is obtain by a large number of distribution or parameter but in order to extract meaningful pattern it be necessary to constrain the number of distribution thus deliberately coarsen the concept resolution . find the right concept resolution be a tricky problem for which many method have been propose e . aic bic mdl etc . and these are frequently consider under the rubric of model regularization . granular computing can be conceive a a framework of theory methodology technique and tool that make use of information granule in the process of problem solving . in this sense granular computing is use a an umbrella term to cover topic that have been study in various field in isolation . by examine all of these existing study in light of the unified framework of granular computing and extract their commonality it may be possible to develop a general theory for problem solving . in a more philosophical sense granular computing can describe a way of think that rely on the human ability to perceive the real world under various level of granularity i . abstraction in order to abstract and consider only those thing that serve a specific interest and to switch among different granularity . by focus on different level of granularity one can obtain different level of knowledge as well a a greater understanding of the inherent knowledge structure . granular computing be thus essential in human problem solving and hence have a very significant impact on the design and implementation of intelligent system .
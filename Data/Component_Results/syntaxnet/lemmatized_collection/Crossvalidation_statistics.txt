crossvalidation sometimes call rotation estimation or outofsample testing be any of various similar model validation technique for assess how the result of a statistical analysis will generalize to an independent data set . it is mainly use in setting where the goal be prediction and one want to estimate how accurately a predictive model will perform in practice . in a prediction problem a model is usually give a dataset of known data on which training be run train dataset and a dataset of unknown data or first see data against which the model is test call the validation dataset or test set . the goal of crossvalidation be to test the model ability to predict new data that were not use in estimate it in order to flag problem like overfitting and to give an insight on how the model will generalize to an independent dataset i . an unknown dataset for instance from a real problem . one round of crossvalidation involves partition a sample of data into complementary subset perform the analysis on one subset call the training set and validate the analysis on the other subset call the validation set or test set . to reduce variability in most method multiple round of crossvalidation are perform use different partition and the validation result are combine e . average over the round to give an estimate of the model predictive performance . in summary crossvalidation combine average measure of fitness in prediction to derive a more accurate estimate of model prediction performance . suppose we have a model with one or more unknown parameter and a data set to which the model can be fit the training data set . the fitting process optimize the model parameter to make the model fit the training data as well a possible . if we then take an independent sample of validation data from the same population a the training data it will generally turn out that the model doe not fit the validation data as well a it fit the training data . the size of this difference be likely to be large especially when the size of the training data set be small or when the number of parameter in the model be large . crossvalidation be a way to estimate the size of this effect . in linear regression we have real response value y . y and n pdimensional vector covariates x . the component of the vector x are denote x . if we use least square to fit a function in the form of a hyperplane y a x to the data x y we could then assess the fit use the mean squared error mse . the mse for given estimate parameter value a and on the training set x y be . if the model is correctly specify it can be show under mild assumption that the expected value of the mse for the training set be npnp time the expect value of the mse for the validation set the expected value is take over the distribution of train set . thus if we fit the model and compute the mse on the training set we will get an optimistically bias assessment of how well the model will fit an independent data set . this biased estimate is call the insample estimate of the fit whereas the crossvalidation estimate be an outofsample estimate . since in linear regression it be possible to directly compute the factor npnp by which the training mse underestimate the validation mse under the assumption that the model specification is valid crossvalidation can be use for check whether the model ha be overfitted in which case the mse in the validation set will substantially exceed it anticipated value . crossvalidation in the context of linear regression be also useful in that it can be use to select an optimally regularize cost function . in most other regression procedure e . logistic regression there be no simple formula to compute the expect outofsample fit . crossvalidation be thus a generally applicable way to predict the performance of a model on unavailable data use numerical computation in place of theoretical analysis . two type of crossvalidation can be distinguish exhaustive and nonexhaustive crossvalidation . exhaustive crossvalidation method be crossvalidation method which learn and test on all possible way to divide the original sample into a training and a validation set . leavepout crossvalidation lpo cv involves use p observation a the validation set and the remaining observation a the training set . this is repeat on all way to cut the original sample on a validation set of p observation and a training set . lpo crossvalidation require training and validate the model formula time where n be the number of observation in the original sample and where formula be the binomial coefficient . for p and for even moderately large n lpo cv can become computationally infeasible . for example with n and p percent of a suggest above formula . leaveoneout crossvalidation loocv be a particular case of leavepout crossvalidation with p . the process look similar to jackknife however with crossvalidation one compute a statistic on the leftout sample while with jackknife one computes a statistic from the kept sample only . loo crossvalidation doe not have the same problem of excessive computation time a general lpo crossvalidation because formula . nonexhaustive cross validation method do not compute all way of split the original sample . those method be approximation of leavepout crossvalidation . in kfold crossvalidation the original sample be randomly partitioned into k equal size subsamples . of the k subsamples a single subsample is retain a the validation data for test the model and the remaining k subsamples are use a train data . the crossvalidation process is then repeat k time the fold with each of the k subsamples use exactly once a the validation data . the k result from the fold can then be average to produce a single estimation . the advantage of this method over repeated random subsampling see below be that all observation are use for both training and validation and each observation is use for validation exactly once . fold crossvalidation is commonly use but in general k remain an unfixed parameter . for example set k result in fold crossvalidation . in fold crossvalidation we randomly shuffle the dataset into two set d and d so that both set be equal size this is usually implement by shuffle the data array and then split it in two . we then train on d and validate on d follow by train on d and validate ond . when kn the number of observation the kfold crossvalidation be exactly the leaveoneout crossvalidation . in stratified kfold crossvalidation the fold are select so that the mean response value be approximately equal in all the fold . in the case of a dichotomous classification this mean that each fold contain roughly the same proportion of the two type of class label . in the holdout method we randomly assign data point to two set d and d usually call the training set and the test set respectively . the size of each of the set be arbitrary although typically the test set be small than the training set . we then train on d and test on d . in typical crossvalidation multiple run are aggregate together in contrast the holdout method in isolation involve a single run . while the holdout method can be frame a the simple kind of crossvalidation many source instead classify holdout a a type of simple validation rather than a simple or degenerate form of crossvalidation . this method also know a monte carlo crossvalidation randomly split the dataset into training and validation data . for each such split the model be fit to the training data and predictive accuracy is assess use the validation data . the result are then average over the split . the advantage of this method over kfold cross validation be that the proportion of the trainingvalidation split be not dependent on the number of iteration fold . the disadvantage of this method be that some observation may never be select in the validation subsample whereas others may be select more than once . in other word validation subset may overlap . this method also exhibit monte carlo variation meaning that the result will vary if the analysis is repeat with different random split . a the number of random split approach infinity the result of repeated random subsampling validation tend towards that of leavepout crossvalidation . in a stratified variant of this approach the random sample are generate in such a way that the mean response value i . the dependent variable in the regression be equal in the training and test set . this be particularly useful if the response be dichotomous with an unbalanced representation of the two response value in the data . the goal of crossvalidation be to estimate the expect level of fit of a model to a data set that be independent of the data that were use to train the model . it can be use to estimate any quantitative measure of fit that be appropriate for the data and model . for example for binary classification problem each case in the validation set is either predict correctly or incorrectly . in this situation the misclassification error rate can be use to summarize the fit although other measure like positive predictive value could also be use . when the value being predict is continuously distribute the mean squared error root mean squared error or median absolute deviation could be use to summarize the error . suppose we choose a measure of fit f and use crossvalidation to produce an estimate f of the expect fit ef of a model to an independent data set drawn from the same population a the training data . if we imagine sample multiple independent training set follow the same distribution the resulting value for f will vary . the statistical property of f result from this variation . the crossvalidation estimator f be very nearly unbiased for ef . the reason that it is slightly bias be that the training set in crossvalidation be slightly small than the actual data set e . for loocv the training set size be n when there be n observed case . in nearly all situation the effect of this bias will be conservative in that the estimate fit will be slightly bias in the direction suggest a poorer fit . in practice this bias be rarely a concern . the variance of f can be large . for this reason if two statistical procedure are compare base on the result of crossvalidation it be important to note that the procedure with the better estimate performance may not actually be the better of the two procedure i . it may not have the good value of ef . some progress ha been make on construct confidence interval around crossvalidation estimate but this is consider a difficult problem . most form of crossvalidation are straightforward to implement as long a an implementation of the prediction method being study be available . in particular the prediction method can be a black box there be no need to have access to the internals of it implementation . if the prediction method be expensive to train crossvalidation can be very slow since the training must be carry out repeatedly . in some case such a least square and kernel regression crossvalidation can be speed up significantly by precomputing certain value that are need repeatedly in the training or by use fast updating rule such a the shermanmorrison formula . however one must be careful to preserve the total blinding of the validation set from the training procedure otherwise bias may result . an extreme example of accelerate crossvalidation occurs in linear regression where the result of crossvalidation have a closedform expression know a the prediction residual error sum of square press . crossvalidation only yield meaningful result if the validation set and training set are draw from the same population and only if human bias are control . in many application of predictive model the structure of the system being study evolves over time i . it be nonstationary . both of these can introduce systematic difference between the training and validation set . for example if a model for predict stock value is train on data for a certain fiveyear period it be unrealistic to treat the subsequent fiveyear period a a draw from the same population . a another example suppose a model is develop to predict an individual risk for being diagnose with a particular disease within the next year . if the model is train use data from a study involve only a specific population group e . young people or male but be then applied to the general population the crossvalidation result from the training set could differ greatly from the actual predictive performance . in many application model also may be incorrectly specify and vary a a function of modeler bias andor arbitrary choice . when this occur there may be an illusion that the system change in external sample whereas the reason be that the model ha miss a critical predictor andor include a confounded predictor . new evidence be that crossvalidation by itself be not very predictive of external validity whereas a form of experimental validation known a swap sampling that doe control for human bias can be much more predictive of external validity . as define by this large maqcii study across model swap sample incorporates crossvalidation in the sense that prediction are test across independent train and validation sample . yet model are also develop across these independent sample and by modeler who be blinded to one another . when there be a mismatch in these model developed across these swap training and validation sample a happen quite frequently maqcii show that this will be much more predictive of poor external predictive validity than traditional crossvalidation . the reason for the success of the swapped sampling be a builtin control for human bias in model building . in addition to place too much faith in prediction that may vary across modeler and lead to poor external validity due to these confound modeler effect these be some other way that crossvalidation can be misuse . since the order of the data be important crossvalidation might be problematic for timeseries model . a more appropriate approach might be to use forward chain . crossvalidation can be use to compare the performance of different predictive modeling procedure . for example suppose we are interest in optical character recognition and we are consider use either support vector machine svm or k near neighbor knn to predict the true character from an image of a handwritten character . use crossvalidation we could objectively compare these two method in term of their respective fraction of misclassified character . if we simply compare the method base on their insample error rat the knn method would likely appear to perform good since it be more flexible and hence more prone to overfitting compare to the svm method . crossvalidation can also be use in variable selection . suppose we are use the expression level of protein to predict whether a cancer patient will respond to a drug . a practical goal would be to determine which subset of the feature should be use to produce the best predictive model . for most modeling procedure if we compare feature subset use the insample error rat the best performance will occur when all feature are use . however under crossvalidation the model with the best fit will generally include only a subset of the feature that are deem truly informative . a recent development in medical statistic be it use in metaanalysis . it form the basis of the validation statistic vn which is use to test the statistical validity of metaanalysis summary estimate . it ha also been use in a more conventional sense in metaanalysis to estimate the likely prediction error of metaanalysis result .
ray solomonoffs theory of universal inductive inference be a theory of prediction base on logical observation such a predict the next symbol base upon a give series of symbol . the only assumption that the theory make be that the environment follow some unknown but computable probability distribution . it be a mathematical formalization of occam razor and the principle of multiple explanation . prediction is do use a completely bayesian framework . the universal prior is calculate for all computable sequencesthis be the universal a priori probability distribution . no computable hypothesis will have a zero probability . this mean that bay rule of causation can be use in predict the continuation of any particular computable sequence . the theory is base in philosophical foundation and wa found by ray solomonoff around . it be a mathematically formalize combination of occam razor and the principle of multiple explanation . all computable theory which perfectly describe previous observation are use to calculate the probability of the next observation with more weight put on the shorter computable theory . marcus hutters universal artificial intelligence build upon this to calculate the expect value of an action . the proof of the razor is base on the known mathematical property of a probability distribution over a countable set . these property be relevant because the infinite set of all program be a denumerable set . the sum s of the probability of all program must be exactly equal to one a per the definition of probability thus the probability must roughly decrease a we enumerate the infinite set of all program otherwise s will be strictly great than one . to be more precise for every formula there be some length l such that the probability of all program longer than l be at most formula . this do not however preclude very long program from have very high probability . fundamental ingredient of the theory be the concept of algorithmic probability and kolmogorov complexity . the universal prior probability of any prefix p of a computable sequence x be the sum of the probability of all program for a universal computer that compute something start with p . give some p and any computable but unknown probability distribution from which x is sample the universal prior and bayes theorem can be use to predict the yet unseen part of x in optimal fashion . though solomonoffs inductive inference be not computable several aixiderived algorithm approximate it in order to make it run on a modern computer . the more compute power they are give the closer their prediction be to the prediction of inductive inference their mathematical limit be solomonoffs inductive inference . another direction of inductive inference is base on e . mark golds model of learn in the limit from and ha develop since then more and more model of learn . the general scenario be the follow give a class s of computable function be there a learner that be recursive functional which for any input of the form ff . fn output a hypothesis an index e with respect to a previously agree on acceptable numbering of all computable function the indexed function may be require consistent with the given value of f . a learner m learn a function f if almost all it hypothesis be the same index e which generate the function f m learns s if m learns every f in s . basic result be that all recursively enumerable class of function be learnable while the class rec of all computable function be not learnable . many related model have been consider and also the learning of class of recursively enumerable set from positive data be a topic study from gold pioneer paper in onwards . a far reaching extension of the gold approach is develop by schmidhubers theory of generalized kolmogorov complexity which be kind of superrecursive algorithm . the third mathematically base direction of inductive inference make use of the theory of automaton and computation . in this context the process of inductive inference is perform by an abstract automaton call an inductive turing machine burgin . inductive turing machine represent the next step in the development of computer science provide good model for contemporary computer and computer network burgin and form an important class of superrecursive algorithm a they satisfy all condition in the definition of algorithm . namely each inductive turing machine be a type of effective method in which a definite list of welldefined instruction for complete a task when give an initial state will proceed through a welldefined series of successive state eventually terminate in an endstate . the difference between an inductive turing machine and a turing machine be that to produce the result a turing machine have to stop while in some case an inductive turing machine can do this without stop . stephen kleene call procedure that could run forever without stop by the name calculation procedure or algorithm kleene . kleene also demand that such an algorithm must eventually exhibit some object kleene . this condition is satisfy by inductive turing machine a their result are exhibit after a finite number of step but inductive turing machine do not always tell at which step the result ha been obtain . simple inductive turing machine be equivalent to other model of computation . more advanced inductive turing machine be much more powerful . it is prove burgin that limit partial recursive function trial and error predicate general turing machine and simple inductive turing machine be equivalent model of computation . however simple inductive turing machine and general turing machine give direct construction of compute automaton which are thoroughly ground in physical machine . in contrast trial and error predicate limit recursive function and limit partial recursive function present syntactic system of symbol with formal rule for their manipulation . simple inductive turing machine and general turing machine are relate to limit partial recursive function and trial and error predicate a turing machine are relate to partial recursive function and lambdacalculus . note that only simple inductive turing machine have the same structure but different functioning semantics of the output mode a turing machine . other type of inductive turing machine have an essentially more advanced structure due to the structured memory and more powerful instruction . their utilization for inference and learn allows achieve high efficiency and better reflects learn of people burgin and klinger . some researcher confuse computation of inductive turing machine with nonstopping computation or with infinite time computation . first some of computation of inductive turing machine halt . a in the case of conventional turing machine some halting computation give the result while others do not give . second some nonstopping computation of inductive turing machine give result while others do not give . rule of inductive turing machine determine when a computation stopping or nonstopping give a result . namely an inductive turing machine produce output from time to time and once this output stop change it is consider the result of the computation . it be necessary to know that description of this rule in some paper be incorrect . for instance davis formulate the rule when result is obtain without stop a once the correct output ha been produce any subsequent output will simply repeat this correct result . third in contrast to the widespread misconception inductive turing machine give result when it happen always after a finite number of step in finite time in contrast to infinite and infinitetime computation . there be two main distinction between conventional turing machine and simple inductive turing machine . the first distinction be that even simple inductive turing machine can do much more than conventional turing machine . the second distinction be that a conventional turing machine always informs by halt or by come to a final state when the result is obtain while a simple inductive turing machine in some case do inform about reach the result while in other case where the conventional turing machine be helpless it doe not inform . people have an illusion that a computer always itself informs by halt or by other mean when the result be obtained . in contrast to this user themselves have to decide in many case whether the computed result be what they need or it be necessary to continue computation . indeed everyday desktop computer application like word processor and spreadsheet spend most of their time wait in event loop and do not terminate until direct to do so by user . evolutionary approach to inductive inference is accomplish by another class of automaton call evolutionary inductive turing machine burgin and eberbach . an evolutionary inductive turing machine be a possibly infinite sequence e at t . of inductive turing machine at each working on generation xt which are cod a word in the alphabet of the machine at . the goal be to build a population z satisfy the inference condition . the automaton at call a component or a level automaton of e represents encode a onelevel evolutionary algorithm that work with input generation xi of the population by apply the variation operator v and selection operator s . the first generation x is give a input to e and is process by the automaton a which generatesproduces the first generation x a it transfer output which go to the automaton a . for all t . the automaton at receive the generation xt a it input from at and then apply the variation operator v and selection operator s produce the generation xi and send it to at to continue evolution .
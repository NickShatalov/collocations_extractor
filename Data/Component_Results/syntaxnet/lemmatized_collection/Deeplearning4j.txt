supervise learn be the machine learning task of learn a function that map an input to an output base on example inputoutput pair . it infer a function from consisting of a set of training example . in supervised learn each example be a pair consist of an input object typically a vector and a desired output value also call the supervisory signal . a supervise learning algorithm analyze the training data and produce an inferred function which can be use for map new example . an optimal scenario will allow for the algorithm to correctly determine the class label for unseen instance . this require the learning algorithm to generalize from the training data to unseen situation in a reasonable way see inductive bias . the parallel task in human and animal psychology is often refer to a concept learn . in order to solve a give problem of supervise learn one ha to perform the follow stepsa wide range of supervise learning algorithm be available each with it strength and weakness . there be no single learning algorithm that work best on all supervise learning problem see the no free lunch theorem . there be four major issue to consider in supervise learninga first issue be the tradeoff between bias and variance . imagine that we have available several different but equally good train data set . a learning algorithm is bias for a particular input formula if when train on each of these data set it be systematically incorrect when predict the correct output for formula . a learning algorithm have high variance for a particular input formula if it predict different output value when train on different training set . the prediction error of a learned classifier is relate to the sum of the bias and the variance of the learn algorithm . generally there be a tradeoff between bias and variance . a learn algorithm with low bias must be flexible so that it can fit the data well . but if the learning algorithm be too flexible it will fit each training data set differently and hence have high variance . a key aspect of many supervise learning method be that they be able to adjust this tradeoff between bias and variance either automatically or by provide a biasvariance parameter that the user can adjust . the second issue be the amount of train data available relative to the complexity of the true function classifier or regression function . if the true function be simple then an inflexible learn algorithm with high bias and low variance will be able to learn it from a small amount of data . but if the true function be highly complex e . because it involve complex interaction among many different input feature and behaves differently in different part of the input space then the function will only be learnable from a very large amount of train data and use a flexible learn algorithm with low bias and high variance . a third issue be the dimensionality of the input space . if the input feature vector have very high dimension the learning problem can be difficult even if the true function only depend on a small number of those feature . this be because the many extra dimension can confuse the learn algorithm and cause it to have high variance . hence high input dimensionality typically require tune the classifier to have low variance and high bias . in practice if the engineer can manually remove irrelevant feature from the input data this be likely to improve the accuracy of the learned function . in addition there be many algorithm for feature selection that seek to identify the relevant feature and discard the irrelevant one . this be an instance of the more general strategy of dimensionality reduction which seek to map the input data into a lowerdimensional space prior to run the supervised learn algorithm . a fourth issue be the degree of noise in the desired output value the supervisory target variable . if the desired output value are often incorrect because of human error or sensor error then the learning algorithm should not attempt to find a function that exactly match the training example . attempt to fit the data too carefully lead to overfitting . you can overfit even when there be no measurement error stochastic noise if the function you are try to learn be too complex for your learn model . in such a situation the part of the target function that cannot be model corrupt your training data this phenomenon ha been call deterministic noise . when either type of noise be present it be good to go with a higher bias low variance estimator . in practice there be several approach to alleviate noise in the output value such a early stop to prevent overfitting as well a detect and remove the noisy training example prior to train the supervised learn algorithm . there be several algorithm that identify noisy training example and remove the suspect noisy training example prior to training have decrease generalization error with statistical significance . other factor to consider when choose and apply a learn algorithm include the followingwhen consider a new application the engineer can compare multiple learning algorithm and experimentally determine which one work best on the problem at hand see cross validation . tune the performance of a learning algorithm can be very timeconsuming . given fix resource it be often well to spend more time collect additional training data and more informative feature than it be to spend extra time tune the learn algorithm . the most widely use learn algorithm are give a set of formula training example of the form formula such that formula be the feature vector of the ith example and formula be it label i . class a learning algorithm seek a function formula where formula be the input space andformula be the output space . the function formula be an element of some space of possible function formula usually call the hypothesis space . it be sometimes convenient torepresent formula use a score function formula such that formula be define a return the formula value that give the high score formula . let formula denote the space of scoring function . although formula and formula can be any space of function many learn algorithm be probabilistic model where formula take the form of a conditional probability model formula or formula take the form of a joint probability model formula . for example naive bayes and linear discriminant analysis be joint probability model whereas logistic regression be a conditional probability model . there be two basic approach to choose formula or formula empirical risk minimization and structural risk minimization . empirical risk minimization seek the function that best fit the training data . structural risk minimization include a penalty function that control the biasvariance tradeoff . in both case it is assume that the train set consists of a sample of independent and identically distribute pair formula . in order to measure how well a function fit the training data a loss function formula is define . for train example formula the loss of predict the value formula be formula . the risk formula of function formula be define a the expected loss of formula . this can be estimate from the training data asin empirical risk minimization the supervise learning algorithm seek the function formula that minimize formula . hence a supervised learning algorithm can be construct by apply an optimization algorithm to find formula . when formula be a conditional probability distribution formula and the loss function be the negative log likelihood formula then empirical risk minimization be equivalent to maximum likelihood estimation . when formula contain many candidate function or the training set be not sufficiently large empirical risk minimization lead to high variance and poor generalization . the learning algorithm be ableto memorize the training example without generalize well . this is call overfitting . structural risk minimization seek to prevent overfitting by incorporate a regularization penalty into the optimization . the regularization penalty can be view a implement a form of occam razor that prefer simple function over more complex one . a wide variety of penalty have been employ that correspond to different definition of complexity . for example consider the case where the function formula be a linear function of the forma popular regularization penalty be formula which be the squared euclidean norm of the weight also known a the formula norm . other norm include the formula norm formula and the formula norm which be the number of nonzero formula . the penalty will be denote by formula . the supervise learning optimization problem be to find the function formula that minimizesthe parameter formula control the biasvariance tradeoff . when formula this give empirical risk minimization with low bias and high variance . when formula be large the learning algorithm will have high bias and low variance . the value of formula can be choose empirically via cross validation . the complexity penalty have a bayesian interpretation a the negative log prior probability of formula formula in which case formula be the posterior probabability of formula . the training method describe above be discriminative training method because they seek to find a function formula that discriminate well between the different output value see discriminative model . for the special case where formula be a joint probability distribution and the loss function be the negative log likelihood formula a risk minimization algorithm is say to perform generative training because formula can be regard a a generative model that explain how the data were generate . generative training algorithm be often simple and more computationally efficient than discriminative training algorithm . in some case the solution can be compute in close form a in naive bayes and linear discriminant analysis . there be several way in which the standard supervise learn problem can be generalize .
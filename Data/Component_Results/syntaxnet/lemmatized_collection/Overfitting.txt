in statistic overfitting be the production of an analysis that correspond too closely or exactly to a particular set of data and may therefore fail to fit additional data or predict future observation reliably . an overfitted model be a statistical model that contain more parameter than can be justify by the data . the essence of overfitting be to have unknowingly extract some of the residual variation i . the noise a if that variation represented underlying model structure . underfitting occurs when a statistical model cannot adequately capture the underlying structure of the data . an underfitted model be a model where some parameter or term that would appear in a correctly specified model are miss . underfitting would occur for example when fit a linear model to nonlinear data . such a model will tend to have poor predictive performance . overfitting and underfitting can occur in machine learn in particular . in machine learn the phenomenon be sometimes call overtraining and undertraining . the possibility of overfitting exists because the criterion use for select the model be not the same a the criterion use to judge the suitability of a model . for example a model might be select by maximize it performance it ability to predict the dependent variable value give some value of the independent variable on some set of training data and yet it suitability might be determine by it ability to perform well on unseen data then overfitting occurs when a model begin to memorize training data rather than learn to generalize from a trend . a an extreme example if the number of parameter be the same a or great than the number of observation then a model can perfectly predict the training data simply by memorize the data in it entirety . for an illustration see figure . such a model though will typically fail severely when make prediction . the potential for overfitting depend not only on the number of parameter and data but also the conformability of the model structure with the data shape and the magnitude of model error compare to the expect level of noise or error in the data . even when the fitted model doe not have an excessive number of parameter it be to be expect that the fitted relationship will appear to perform less well on a new data set than on the data set use for fit a phenomenon sometimes known a shrinkage . in particular the value of the coefficient of determination will shrink relative to the original data . to lessen the chance of or amount of overfitting several technique be available e . model comparison crossvalidation regularization early stopping pruning bayesian prior or dropout . the basis of some technique be either to explicitly penalize overly complex model or to test the model ability to generalize by evaluate it performance on a set of data not use for train which is assume to approximate the typical unseen data that a model will encounter . in statistic an inference is draw from a statistical model which ha been select via some procedure . burnham anderson in their muchcited text on model selection argue that to avoid overfitting we should adhere to the principle of parsimony . the author also state the follow . overfitting be more likely to be a serious concern when there be little theory available to guide the analysis in part because then there tend to be a large number of model to select from . the book model selection and model averaging put it this way . in regression analysis overfitting occurs frequently . in the extreme case if there be p variable in a linear regression with p data point the fit hyperplane will go exactly through every point . a study in suggests that two observation per independent variable be sufficient for linear regression . for logistic regression or cox proportional hazard model there be a variety of rule of thumb e . and the guideline of observation per independent variable is know a the one in ten rule . in the process of regression model selection the mean squared error of the random regression function can be split into random noise approximation bias and variance in the estimate of the regression function and the biasvariance tradeoff is often use to overcome overfit model . by freedman paradox with a large set of explanatory variable that actually have no relation to the dependent variable being predict some explanators will in general be spuriously find to be statistically significant and the researcher may thus retain them in the model thereby overfitting the model . usually a learning algorithm is train use some set of train data exemplary situation for which the desired output be known . the goal be that the algorithm will also perform well on predict the output when feed validation data that wa not encounter during it training . overfitting be the use of model or procedure that violate occams razor for example by include more adjustable parameter than be ultimately optimal or by use a more complicate approach than be ultimately optimal . for an example where there be too many adjustable parameter consider a dataset where train data for can be adequately predict by a linear function of two dependent variable . such a function require only three parameter the intercept and two slope . replace this simple function with a new more complex quadratic function or with a new more complex linear function on more than two dependent variable carry a risk occam razor implies that any give complex function be a priori le probable than any give simple function . if the new more complicated function be select instead of the simple function and if there be not a large enough gain in trainingdata fit to offset the complexity increase then the new complex function overfits the data and the complex overfitted function will likely perform bad than the simple function on validation data outside the training dataset even though the complex function perform as well or perhaps even good on the training dataset . when compare different type of model complexity cannot be measure solely by count how many parameter exist in each model the expressivity of each parameter must be consider as well . for example it be nontrivial to directly compare the complexity of a neural net which can track curvilinear relationship with parameter to a regression model with parameter . overfitting is especially likely in case where learn wa perform too long or where train example are rare cause the learner to adjust to very specific random feature of the training data that have no causal relation to the target function . in this process of overfitting the performance on the training example still increase while the performance on unseen data become worse . a a simple example consider a database of retail purchase that include the item buy the purchaser and the date and time of purchase . it easy to construct a model that will fit the training set perfectly by use the date and time of purchase to predict the other attribute but this model will not generalize at all to new data because those past time will never occur again . generally a learning algorithm is say to overfit relative to a simple one if it be more accurate in fit known data hindsight but le accurate in predict new data foresight . one can intuitively understand overfitting from the fact that information from all past experience can be divide into two group information that be relevant for the future and irrelevant information noise . everything else be equal the more difficult a criterion be to predict i . the higher it uncertainty the more noise exist in past information that need to be ignore . the problem be determine which part to ignore . a learning algorithm that can reduce the chance of fit noise is call robust . the most obvious consequence of overfitting be poor performance on the validation dataset . other negative consequence include . the optimal function usually need verification on big or completely new datasets . there be however method like minimum spanning tree or lifetime of correlation that apply the dependence between correlation coefficient and timeseries window width . whenever the window width be big enough the correlation coefficient be stable and dont depend on the window width size anymore . therefore a correlation matrix can be create by calculate a coefficient of correlation between investigate variable . this matrix can be represent topologically a a complex network where direct and undirect influence between variable are visualize . underfitting occurs when a statistical model or machine learn algorithm cannot adequately capture the underlying structure of the data . it occur when the model or algorithm doe not fit the data enough . underfitting occurs if the model or algorithm show low variance but high bias to contrast the opposite overfitting from high variance and low bias . it be often a result of an excessively simple model . burnham anderson state the follow . however underfitting can also occur in the absence of bias . for example if a good model would include two mutually uncorrelated explanatory variable x and z but only x is use in the model the effect of x could be estimate without bias but the model would fit the data well both in the training set of data and for other data if z were also include .
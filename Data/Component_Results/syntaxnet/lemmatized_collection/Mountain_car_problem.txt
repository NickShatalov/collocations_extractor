mountain car a standard testing domain in reinforcement learning be a problem in which an underpowered car must drive up a steep hill . since gravity be strong than the car engine even at full throttle the car cannot simply accelerate up the steep slope . the car is situate in a valley and must learn to leverage potential energy by drive up the opposite hill before the car be able to make it to the goal at the top of the rightmost hill . the domain ha been use a a test bed in various reinforcement learn paper . the mountain car problem although fairly simple be commonly apply because it require a reinforcement learn agent to learn on two continuous variable position and velocity . for any give state position and velocity of the car the agent is give the possibility of drive left drive right or not use the engine at all . in the standard version of the problem the agent receive a negative reward at every time step when the goal is not reach the agent have no information about the goal until an initial success . the mountain car problem appear first in andrew moore phd thesis . it be later more strictly define in singh and suttons reinforcement lean paper with eligibility trace . the problem become more widely study when sutton and barto add it to their book reinforcement learn an introduction . throughout the year many version of the problem have been use such a those which modify the reward function termination condition andor the start state . qlearning and similar technique for map discrete state to discrete action need to be extend to be able to deal with the continuous state space of the problem . approach often fall into one of two category state space discretization or function approximation . in this approach two continuous state variable are push into discrete state by bucket each continuous variable into multiple discrete state . this approach work with properly tune parameter but a disadvantage is information gather from one state is not use to evaluate another state . tile cod can be use to improve discretization and involves continuous variable map into set of bucket offset from one another . each step of training have a wider impact on the value function approximation because when the offset grid are sum the information be diffused . function approximation be another way to solve the mountain car . by choose a set of basis function beforehand or by generate them a the car drive the agent can approximate the value function at each state . unlike the stepwise version of the value function create with discretization function approximation can more cleanly estimate the true smooth function of the mountain car domain . an interesting aspect of the problem involve the delay of actual reward . the agent isnt able to learn about the goal until a successful completion . give a naive approach without trace for each trial the car can only backup the reward of the goal slightly . this be a problem for naive discretization because each discrete state will only be backup once take a large number of episode to learn the problem . to alleviate this problem trace will automatically backup the reward give to state before dramatically increase the speed of learn . the mountain car problem ha undergo many iteration . this section will focus on the standard well define version from sutton . twodimensional continuous state space . formula . formula . onedimensional discrete action space . formula . for every time step . formula . for every time step . formula . formula . formula . optionally many implementation include randomness in both parameter to show well generalize learning . formula . formula . end the simulation when . formula . there be many version of the mountain car which deviate in different way from the standard model . variables that vary include but are not limit to change the constant gravity and steepness of the problem so specific tuning for specific policy become irrelevant and alter the reward function to affect the agent ability to learn in a different manner . an example is change the reward to be equal to the distance from the goal or change the reward to zero everywhere and one at the goal . additionally we can use a d mountain car with a d continuous state space .
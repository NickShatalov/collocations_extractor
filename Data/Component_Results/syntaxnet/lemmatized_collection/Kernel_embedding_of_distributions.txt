in machine learn the kernel embedding of distribution also call the kernel mean or mean map comprise a class of nonparametric method in which a probability distribution is represent a an element of a reproducing kernel hilbert space rkhs . a generalization of the individual datapoint feature mapping do in classical kernel method the embedding of distribution into infinitedimensional feature space can preserve all of the statistical feature of arbitrary distribution while allow one to compare and manipulate distribution use hilbert space operation such a inner product distance projection linear transformation and spectral analysis . this learn framework be very general and can be apply to distribution over any space formula on which a sensible kernel function measure similarity between element of formula may be define . for example various kernel have be propose for learn from data which be vector in formula discrete classescategories string graphsnetworks image time series manifold dynamical system and other structured object . the theory behind kernel embeddings of distribution ha be primarily develop by alex smola le song arthur gretton and bernhard schlkopf . a review of recent work on kernel embedding of distribution can be find in . the analysis of distribution is fundamental in machine learn and statistic and many algorithm in these field rely on information theoretic approach such a entropy mutual information or kullbackleibler divergence . however to estimate these quantity one must first either perform density estimation or employ sophisticate spacepartitioningbiascorrection strategy which be typically infeasible for highdimensional data . commonly method for model complex distribution rely on parametric assumption that may be unfounded or computationally challenge e . gaussian mixture model while nonparametric method like kernel density estimation note the smoothing kernel in this context have a different interpretation than the kernel discuss here or characteristic function representation via the fourier transform of the distribution break down in highdimensional setting . method base on the kernel embedding of distribution sidestep these problem and also possess the follow advantage . thus learn via the kernel embedding of distribution offer a principled dropin replacement for information theoretic approach and be a framework which not only subsume many popular method in machine learning and statistic a special case but also can lead to entirely new learn algorithm . let formula denote a random variable with codomain formula and distribution formula . give a kernel formula on formula the moorearonszajn theorem assert the existence of a rkhs formula a hilbert space of function formula equip with inner product formula and norm formula in which the element formula satisfy the reproducing property formula . one may alternatively consider formula an implicit feature map formula from formula to formula which be therefore also call the feature space so that formula can be view a a measure of similarity between point formula . while the similarity measure be linear in the feature space it may be highly nonlinear in the original space depend on the choice of kernel . the kernel embedding of the distribution formula in formula also call the kernel mean or mean map is give by . if formula allow a square integrable density formula then formula where formula be the hilbertschmidt integral operator . a kernel be characteristic if the mean embedding formula be injective . each distribution can thus be uniquely represent in the rkhs and all statistical feature of distribution are preserve by the kernel embedding if a characteristic kernel is use . give formula training example formula draw independently and identically distribute i . from formula the kernel embedding of formula can be empirically estimate as . if formula denote another random variable for simplicity assume the codomain of formula be also formula with the same kernel formula which satisfy formula then the joint distribution formula can be map into a tensor product feature space formula via . by the equivalence between a tensor and a linear map this joint embed may be interpret a an uncentered crosscovariance operator formula from which the crosscovariance of meanzero function formula can be compute as . give formula pair of training example formula draw i . from formula we can also empirically estimate the joint distribution kernel embed via . give a conditional distribution formula one can define the correspond rkhs embed as . note that the embedding of formula thus define a family of point in the rkhs index by the value formula taken by condition variable formula . by fix formula to a particular value we obtain a single element in formula and thus it be natural to define the operator . which give the feature mapping of formula output the conditional embedding of formula give formula . assume that for all formula it can be show that . this assumption be always true for finite domain with characteristic kernel but may not necessarily hold for continuous domain . nevertheless even in case where the assumption fails formula may still be use to approximate the conditional kernel embed formula and in practice the inversion operator is replace with a regularize version of itself formula where formula denote the identity matrix . give training example formula the empirical kernel conditional embedding operator may be estimate a . where formula be implicitly form feature matrix formula be the gram matrix for sample of formula and formula be a regularization parameter need to avoid overfitting . thus the empirical estimate of the kernel conditional embedding is give by a weight sum of sample of formula in the feature space . on compact subset of formula be universal . and support of formula be an entire space then formula be universal . for example gaussian rbf be universal sinc kernel be not universal . this section illustrate how basic probabilistic rule may be reformulate a multilinear algebraic operation in the kernel embedding framework and is primarily base on the work of song et al . the follow notation is adopt . in practice all embeddings are empirically estimate from data formula and it assume that a set of sample formula may be use to estimate the kernel embedding of the prior distribution formula . in probability theory the marginal distribution of formula can be compute by integrate out formula from the joint density include the prior distribution on formula . the analog of this rule in the kernel embedding framework state that formula the rkhs embed of formula can be compute via . in practical implementation the kernel sum rule take the follow form . where formula be the empirical kernel embedding of the prior distribution formula formula and formula be gram matrix with entry formula respectively . in probability theory a joint distribution can be factorize into a product between conditional and marginal distribution . the analog of this rule in the kernel embed framework state that formula the joint embedding of formula can be factorize a a composition of conditional embedding operator with the autocovariance operator associate with formula . in practical implementation the kernel chain rule take the follow form . in probability theory a posterior distribution can be express in term of a prior distribution and a likelihood function a . the analog of this rule in the kernel embedding framework express the kernel embedding of the conditional distribution in term of conditional embedding operator which are modify by the prior distribution . in practical implementation the kernel bayes rule take the follow form . where formula . two regularization parameter are use in this framework formula for the estimation of formula and formula for the estimation of the final conditional embedding operator formula . the latter regularization is do on square of formula because formula may not be positive definite . the maximum mean discrepancy mmd be a distancemeasure between distribution formula and formula which is define a the square distance between their embeddings in the rkhs . while most distancemeasures between distribution such a the widely use kullbackleibler divergence either require density estimation either parametrically or nonparametrically or space partitioningbias correction strategy the mmd be easily estimated a an empirical mean which is concentrate around the true value of the mmd . the characterization of this distance a the maximum mean discrepancy refer to the fact that compute the mmd be equivalent to find the rkhs function that maximize the difference in expectation between the two probability distribution . give n training example from formula and m sample from formula one can formulate a test statistic base on the empirical estimate of the mmd . to obtain a twosample test of the  hypothesis that both sample stem from the same distribution i . formula against the broad alternative formula . although learn algorithm in the kernel embedding framework circumvent the need for intermediate density estimation one may nonetheless use the empirical embedding to perform density estimation base on n sample draw from an underlying distribution formula . this can be do by solve the follow optimization problem . where the maximization is do over the entire space of distribution on formula . here formula be the kernel embedding of the propose density formula and formula be an entropylike quantity e . entropy kl divergence bregman divergence . the distribution which solve this optimization may be interpret a a compromise between fit the empirical kernel mean of the sample well while still allocate a substantial portion of the probability mass to all region of the probability space much of which may not be represent in the training example . in practice a good approximate solution of the difficult optimization may be find by restrict the space of candidate densities to a mixture of m candidate distribution with regularize mixing proportion . connection between the idea underlying gaussian process and conditional random field may be draw with the estimation of conditional probability distribution in this fashion if one view the feature mapping associate with the kernel a sufficient statistic in generalize possibly infinitedimensional exponential family . a measure of the statistical dependence between random variable formula and formula from any domain on which sensible kernel can be define can be formulate base on the hilbertschmidt independence criterion . and can be use a a principled replacement for mutual information pearson correlation or any other dependence measure use in learn algorithm . most notably hsic can detect arbitrary dependency when a characteristic kernel is use in the embeddings hsic be zero if and only if the variable be independent and can be use to measure dependence between different type of data e . image and text caption . give n i . sample of each random variable a simple parameterfree unbiased estimator of hsic which exhibit concentration about the true value can be compute in formula time where the gram matrix of the two datasets are approximate using formula with formula . the desirable property of hsic have lead to the formulation of numerous algorithm which utilize this dependence measure for a variety of common machine learn task such a feature selection bahsic clustering cluhsic and dimensionality reduction muhsic . hsic can be extend to measure the dependence of multiple random variable . the question of when hsic capture independence in this case ha recently been study for . more than two variable . belief propagation be a fundamental algorithm for inference in graphical model in which nod repeatedly pass and receive message correspond to the evaluation of conditional expectation . in the kernel embedding framework the message may be represent a rkhs function and the conditional distribution embeddings can be apply to efficiently compute message update . give n sample of random variable represent by node in a markov random field the incoming message to node t from node u can be express a formula if it assume to lie in the rkhs . the kernel belief propagation update message from t to node s is then give by . where formula denote the elementwise vector product formula be the set of node connect to t exclude node s formula formula be the gram matrix of the sample from variable formula respectively and formula be the feature matrix for the sample from formula . thus if the incoming message to node t be linear combination of feature mapped sample from formula then the outgoing message from this node be also a linear combination of feature mapped sample from formula . this rkhs function representation of messagepassing update therefore produce an efficient belief propagation algorithm in which the potential be nonparametric function infer from the data so that arbitrary statistical relationship may be model . in the hidden markov model hmm two key quantity of interest be the transition probability between hidden state formula and the emission probability formula for observation . use the kernel conditional distribution embedding framework these quantity may be express in term of sample from the hmm . a serious limitation of the embedding method in this domain be the need for train sample contain hidden state a otherwise inference with arbitrary distribution in the hmm be not possible . one common use of hmms is filter in which the goal be to estimate posterior distribution over the hidden state formula at time step t give a history of previous observation formula from the system . in filter a belief state formula is recursively maintain via a prediction step where update formula are compute by marginalize out the previous hidden state follow by a conditioning step where update formula are compute by apply bayes rule to condition on a new observation . the rkhs embed of the belief state at time t can be recursively expressed a . by compute the embeddings of the prediction step via the kernel sum rule and the embedding of the conditioning step via kernel bayes rule . assume a training sample formula is give one can in practice estimate formula and filter with kernel embeddings is thus implement recursively use the follow update for the weight formula . where formula denote the gram matrix of formula and formula respectively formula be a transfer gram matrix define a formula and formula . the support measure machine smm be a generalization of the support vector machine svm in which the training example be probability distribution pair with label formula . smms solve the standard svm dual optimization problem use the following expect kernel . which be computable in close form for many common specific distribution formula such a the gaussian distribution combine with popular embedding kernel formula e . the gaussian kernel or polynomial kernel or can be accurately empirically estimate from i . sample formula via . under certain choice of the embedding kernel formula the smm applied to training example formula be equivalent to a svm train on sample formula and thus the smm can be view a a flexible svm in which a different datadependent kernel specify by the assumed form of the distribution formula may be place on each training point . the goal of domain adaptation be the formulation of learn algorithm which generalize well when the training and test data have different distribution . give training example formula and a test set formula where the formula be unknown three type of difference are commonly assume between the distribution of the training example formula and the test distribution formula . by utilize the kernel embedding of marginal and conditional distribution practical approach to deal with the presence of these type of difference between train and test domain can be formulate . covariate shift may be account for by reweighting example via estimate of the ratio formula obtain directly from the kernel embeddings of the marginal distribution of formula in each domain without any need for explicit estimation of the distribution . target shift which cannot be similarly deal with since no sample from formula be available in the test domain is account for by weight training example use the vector formula which solve the follow optimization problem where in practice empirical approximation must be use . to deal with location scale conditional shift one can perform a ls transformation of the training point to obtain new transformed training data formula where formula denote the elementwise vector product . to ensure similar distribution between the new transformed training sample and the test data formula are estimate by minimize the follow empirical kernel embed distance . in general the kernel embedding method for deal with ls conditional shift and target shift may be combine to find a reweighted transformation of the training data which mimic the test distribution and these method may perform well even in the presence of conditional shift other than locationscale change . give n set of train example sample i . from distribution formula the goal of domain generalization be to formulate learning algorithm which perform well on test example sample from a previously unseen domain formula where no data from the test domain be available at training time . if conditional distribution formula are assume to be relatively similar across all domain then a learner capable of domain generalization must estimate a functional relationship between the variable which be robust to change in the marginals formula . base on kernel embeddings of these distribution domain invariant component analysis dica be a method which determine the transformation of the training data that minimize the difference between marginal distribution while preserve a common conditional distribution share between all training domain . dica thus extract invariants feature that transfer across domain and may be view a a generalization of many popular dimensionreduction method such a kernel principal component analysis transfer component analysis and covariance operator inverse regression . define a probability distribution formula on the rkhs formula with formula dica measure dissimilarity between domain via distributional variance which is compute as . so formula be a formula gram matrix over the distribution from which the training data are sample . find an orthogonal transform onto a lowdimensional subspace b in the feature space which minimize the distributional variance dica simultaneously ensures that b aligns with the base of a central subspace c for which formula become independent of formula give formula across all domain . in the absence of target value formula an unsupervised version of dica may be formulate which find a lowdimensional subspace that minimizes distributional variance while simultaneously maximize the variance of formula in the feature space across all domain rather than preserve a central subspace . in distribution regression the goal be to regress from probability distribution to real or vector . many important machine learn and statistical task fit into this framework include multiinstance learning and point estimation problem without analytical solution such a hyperparameter or entropy estimation . in practice only sample from sampled distribution be observable and the estimate have to rely on similarity compute between set of point . distribution regression ha been successfully apply for example in supervise entropy learn and aerosol prediction use multispectral satellite image . give formula training data where the formula bag contain sample from a probability distribution formula and the formula output label be formula one can tackle the distribution regression task by take the embeddings of the distribution and learn the regressor from the embeddings to the output . in other word one can consider the follow kernel ridge regression problem formula . where formula with a formula kernel on the domain of formulas formula formula be a kernel on the embedded distribution and formula be the rkhs determine by formula . example for formula include the linear kernel formula the gaussian kernel formula the exponential kernel formula the cauchy kernel formula the generalize tstudent kernel formula or the inverse multiquadrics kernel formula . the prediction on a new distribution formula take the simple analytical form . where formula formula formula formula . under mild regularity condition this estimator can be show to be consistent and it can achieve the onestage sample a if one have access to the true formulas minimax optimal rate . in the formula objective function formula be real number the result can also be extend to the case when formula be formuladimensional vector or more generally element of a separable hilbert space using operatorvalued formula kernel . in this simple example which is take from song et al . formula are assume to be discrete random variable which take value in the set formula and the kernel be chosen to be the kronecker delta function so formula . the feature map correspond to this kernel be the standard basis vector formula . the kernel embeddings of such a distribution be thus vector of marginal probability while the embeddings of joint distribution in this setting be formula matrix specify joint probability table and the explicit form of these embeddings be . the conditional distribution embedding operator formula be in this set a conditional probability table . thus the embeddings of the conditional distribution under a fix value of formula may be compute as . in this discretevalued set with the kronecker delta kernel the kernel sum rule becomes . the kernel chain rule in this case is give by .
in machine learn a hyperparameter be a parameter whose value is set before the learning process begin . by contrast the value of other parameter are derive via train . different model training algorithm require different hyperparameters some simple algorithm such a ordinary least square regression require none . give these hyperparameters the training algorithm learn the parameter from the data . for instance lasso be an algorithm that add a regularization hyperparameter to ordinary least square regression which have to be set before estimate the parameter through the training algorithm . the time require to train and test a model can depend upon the choice of it hyperparameters . a hyperparameter be usually of continuous or integer type lead to mixedtype optimization problem . the existence of some hyperparameters be conditional upon the value of others e . the size of each hide layer in a neural network can be conditional upon the number of layer . most performance variation can be attribute to just a few hyperparameters . the tunability of an algorithm hyperparameter or interacting hyperparameters be a measure of how much performance can be gain by tune it . for an lstm while the learning rate follow by the network size be it most crucial hyperparameters whereas batching and momentum have no significant effect on it performance . although some research ha advocate the use of minibatch size in the thousand other work ha find the best performance with minibatch size between and . an inherent stochasticity in learn directly implies that the empirical hyperparameter performance be not necessarily it true performance . method that be not robust to simple change in hyperparameters random seed or even different implementation of the same algorithm cannot be integrate into mission critical control system without significant simplification and robustification . reinforcement learn algorithm in particular require measure their performance over a large number of random seed and also measure their sensitivity to choice of hyperparameters . their evaluation with a small number of random seed do not capture performance adequately due to high variance . some reinforcement learn method e . ddpg deep deterministic policy gradient be more sensitive to hyperparameter choice than others . hyperparameter optimization find a tuple of hyperparameters that yield an optimal model which minimize a predefined loss function on give test data . the objective function take a tuple of hyperparameters and return the associated loss . apart from tune hyperparameters machine learn involves storing and organize the parameter and result and make sure they be reproducible . in the absence of a robust infrastructure for this purpose research code often evolve quickly and compromise essential aspect like bookkeeping and reproducibility . online collaboration platform for machine learn go far by allow scientist to automatically share organize and discuss experiment data and algorithm . a number of relevant service and open source software exist .
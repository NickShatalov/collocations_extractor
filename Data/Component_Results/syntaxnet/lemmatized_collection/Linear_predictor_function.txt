in statistic and in machine learn a linear predictor function be a linear function linear combination of a set of coefficient and explanatory variable independent variable whose value is use to predict the outcome of a dependent variable . this sort of function usually come in linear regression where the coefficient are call regression coefficient . however they also occur in various type of linear classifier e . logistic regression perceptrons support vector machine and linear discriminant analysis as well a in various other model such a principal component analysis and factor analysis . in many of these model the coefficient are refer to a weight . the basic form of a linear predictor function formula for data point i consist of p explanatory variable for i . n be . where formula be the coefficient regression coefficient weight etc . indicate the relative effect of a particular explanatory variable on the outcome . it be common to write the predictor function in a more compact form a follow . this make it possible to write the linear predictor function a follows . use the notation for a dot product between two vector . an equivalent form using matrix notation be a follow . where formula and formula are assume to be a pby column vectors formula be the matrix transpose of formula so formula be a byp row vector and formula indicates matrix multiplication between the byp row vector and the pby column vector produce a by matrix that is take to be a scalar . an example of the usage of a linear predictor function be in linear regression where each data point is associate with a continuous outcome y and the relationship write . where formula be a disturbance term or error variable an unobserved random variable that add noise to the linear relationship between the dependent variable and predictor function . in some model standard linear regression in particular the equation for each of the data point i . n are stack together and write in vector form as . where . the matrix x is know a the design matrix and encode all known information about the independent variable . the variable formula be random variable which in standard linear regression are distribute according to a standard normal distribution they express the influence of any unknown factor on the outcome . this make it possible to find optimal coefficient through the method of least square use simple matrix operation . in particular the optimal coefficient formula a estimate by least square can be write a follows . the matrix formula be known a the moorepenrose pseudoinverse of x . the use of the matrix inverse in this formula require that x be of full rank i . there be not perfect multicollinearity among different explanatory variable i . no explanatory variable can be perfectly predict from the others . in such case the singular value decomposition can be use to compute the pseudoinverse . although the outcomes dependent variable to be predict are assume to be random variable the explanatory variable themselves are usually not assume to be random . instead they are assume to be fix value and any random variable e . the outcome are assume to be conditional on them . a a result the data analyst be free to transform the explanatory variable in arbitrary way include create multiple copy of a give explanatory variable each transform use a different function . other common technique be to create new explanatory variable in the form of interaction variable by take product of two or sometimes more existing explanatory variable . when a fixed set of nonlinear function are use to transform the value of a data point these function are know a basis function . an example be polynomial regression which use a linear predictor function to fit an arbitrary degree polynomial relationship up to a give order between two set of data point i . a single realvalued explanatory variable and a relate realvalued dependent variable by add multiple explanatory variable correspond to various power of the existing explanatory variable . mathematically the form look like this . in this case for each data point i a set of explanatory variable is create a follows . and then standard linear regression be run . the basis function in this example would be . this example show that a linear predictor function can actually be much more powerful than it first appear it only really need to be linear in the coefficient . all sort of nonlinear function of the explanatory variable can be fit by the model . there be no particular need for the input to basis function to be univariate or singledimensional or their output for that matter although in such a case a kdimensional output value be likely to be treat a k separate scalaroutput basis function . an example of this be radial basis function rbfs which compute some transform version of the distance to some fixed point . an example be the gaussian rbf which have the same functional form a the normal distribution . which drop off rapidly a the distance from c increase . a possible usage of rbfs be to create one for every observed data point . this mean that the result of an rbf apply to a new data point will be close to unless the new point be near to the point around which the rbf wa apply . that be the application of the radial basis function will pick out the near point and it regression coefficient will dominate . the result will be a form of near neighbor interpolation where prediction are make by simply use the prediction of the nearest observe data point possibly interpolate between multiple nearby data point when they be all similar distance away . this type of near neighbor method for prediction is often consider diametrically oppose to the type of prediction use in standard linear regression but in fact the transformation that can be apply to the explanatory variable in a linear predictor function be so powerful that even the near neighbor method can be implement a a type of linear regression . it be even possible to fit some function that appear nonlinear in the coefficient by transform the coefficient into new coefficient that do appear linear . for example a function of the form formula for coefficient formula could be transform into the appropriate linear function by apply the substitution formula lead to formula which be linear . linear regression and similar technique could be apply and will often still find the optimal coefficient but their error estimate and such will be wrong . the explanatory variable may be of any type realvalued binary categorical etc . the main distinction be between continuous variable e . income age blood pressure etc . and discrete variable e . sex race political party etc . discrete variable refer to more than two possible choice are typically cod use dummy variable or indicator variable i . separate explanatory variable take the value or are create for each possible value of the discrete variable with a mean variable doe have the give value and a mean variable do not have the give value . for example a fourway discrete variable of blood type with the possible value a b ab o would be convert to separate twoway dummy variable isa isb isab iso where only one of them have the value and all the rest have the value . this allow for separate regression coefficient to be match for each possible value of the discrete variable . note that for k category not all k dummy variable be independent of each other . for example in the above blood type example only three of the four dummy variable be independent in the sense that once the value of three of the variable are know the fourth be automatically determined . thus it really only necessary to encode three of the four possibility a dummy variable and in fact if all four possibility are encode the overall model become nonidentifiable . this cause problem for a number of method such a the simple closedform solution use in linear regression . the solution be either to avoid such case by eliminate one of the dummy variable andor introduce a regularization constraint which necessitate a more powerful typically iterative method for find the optimal coefficient .
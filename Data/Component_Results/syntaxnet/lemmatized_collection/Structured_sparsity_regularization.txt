structure sparsity regularization be a class of method and an area of research in statistical learning theory that extend and generalize sparsity regularization learn method . both sparsity and structure sparsity regularization method seek to exploit the assumption that the output variable formula i . response or dependent variable to be learn can be describe by a reduce number of variable in the input space formula i . the domain space of feature or explanatory variable . sparsity regularization method focus on select the input variable that best describe the output . structure sparsity regularization method generalize and extend sparsity regularization method by allow for optimal selection over structure like group or network of input variable in formula . common motivation for the use of structured sparsity method be model interpretability highdimensional learning where dimensionality of formula may be high than the number of observation formula and reduction of computational complexity . moreover structured sparsity method allow to incorporate prior assumption on the structure of the input variable such a overlap group nonoverlapping group and acyclic graph . example of us of structured sparsity method include face recognition magnetic resonance image mri process sociolinguistic analysis in natural language process and analysis of genetic expression in breast cancer . consider the linear kernel regularize empirical risk minimization problem with a loss function formula and the formula norm a the regularization penalty . where formula and formula denotes the formula norm define a the number of nonzero entry of the vector formula . formula is say to be sparse if formula . which mean that the output formula can be describe by a small subset of input variable . more generally assume a dictionary formula with formula be give such that the target function formula of a learning problem can be write a . the formula norm formula a the number of nonzero component of formula is define a . formula is say to be sparse if formula . however while use the formula norm for regularization favor sparse solution it be computationally difficult to use and additionally be not convex . a computationally more feasible norm that favor sparser solution be the formula norm this ha been show to still favor sparser solution and be additionally convex . structure sparsity regularization extends and generalize the variable selection problem that characterize sparsity regularization . consider the above regularize empirical risk minimization problem with a general kernel and associate feature map formula with formula . the regularization term formula penalize each formula component independently which mean that the algorithm will suppress input variable independently from each other . in several situation we may want to impose more structure in the regularization process so that for example input variable are suppress accord to predefined group . structure sparsity regularization method allow to impose such structure by add structure to the norm define the regularization term . the nonoverlapping group case be the most basic instance of structured sparsity . in it an a priori partition of the coefficient vector formula in formula nonoverlapping group is assume . let formula be the vector of coefficient in group formula we can define a regularization term and it group norm as . where formula be the group formula norm formula formula be group formula and formula be the jth component of group formula . the above norm is also refer to a group lasso . this regularizer will force entire coefficient group towards zero rather than individual coefficient . a the group are nonoverlapping the set of nonzero coefficient can be obtain a the union of the group that were not set to zero and conversely for the set of zero coefficient . overlap group be the structure sparsity case where a variable can belong to more than one group formula . this case be often of interest a it can represent a more general class of relationship among variable than nonoverlapping group can such a tree structure or other type of graph . there be two type of overlap group sparsity regularization approach which are use to model different type of input variable relationship . the intersection of complement approach is use in case when we want to select only those input variable that have positive coefficient in all group they belong to . consider again the group lasso for a regularized empirical risk minimization problem . where formula be the group formula norm formula be group formula and formula be the jth component of group formula . a in the nonoverlapping group case the group lasso regularizer will potentially set entire group of coefficient to zero . select variable be those with coefficient formula . however a in this case group may overlap we take the intersection of the complement of those group that be not set to zero . this intersection of complement selection criterion imply the modeling choice that we allow some coefficient within a particular group formula to be set to zero while others within the same group formula may remain positive . in other word coefficient within a group may differ depend on the several group membership that each variable within the group may have . a different approach be to consider union of group for variable selection . this approach capture the modeling situation where variable can be select as long a they belong at least to one group with positive coefficient . this modeling perspective implies that we want to preserve group structure . the formulation of the union of group approach is also refer to a latent group lasso and requires to modify the group formula norm consider above and introduce the follow regularizer . where formula formula be the vector of coefficient of group g and formula be a vector with coefficients formula for all variable formula in group formula and formula in all others i . formula if formula in group formula and formula otherwise . this regularizer can be interpret a effectively replicate variable that belong to more than one group therefore conserve group structure . as intend by the union of group approach require formula produce a vector of weight w that effectively sum up the weight of all variable across all group they belong to . the objective function use group lasso consists of an error function which be generally required to be convex but not necessarily strongly convex and a group formula regularization term . an issue with this objective function be that it be convex but not necessarily strongly convex and thus generally doe not lead to unique solution . an example of a way to fix this be to introduce the squared formula norm of the weight vector a an additional regularization term while keep the formula regularization term from the group lasso approach . if the coefficient of the square formula norm term be great than formula then because the squared formula norm term be strongly convex the result objective function will also be strongly convex . provide that the formula coefficient be suitably small but still positive the weight vector minimize the resulting objective function be generally very close to a weight vector that minimize the objective function that would result from remove the group formula regularization term altogether from the original objective function the latter scenario corresponds to the group lasso approach . thus this approach allow for simple optimization while maintain sparsity . see submodular set function . besides the norm discuss above other norm use in structured sparsity method include hierarchical norm and norm define on grid . these norm arise from submodular function and allow the incorporation of prior assumption on the structure of the input variable . in the context of hierarchical norm this structure can be represent a a direct acyclic graph over the variable while in the context of gridbased norm the structure can be represent use a grid . see unsupervised learn . unsupervised learn method are often use to learn the parameter of latent variable model . latent variable model be statistical model where in addition to the observe variable a set of latent variable also exist which is not observe . often in such model hierarchy are assume between the variable of the system this system of hierarchy can be represent using direct acyclic graph . hierarchy of latent variable have emerge a a natural structure in several application notably to model text document . hierarchical model using bayesian nonparametric method have been use to learn topic model which be statistical model for discover the abstract topic that occur in a collection of document . hierarchy have also been consider in the context of kernel method . hierarchical norm have be apply to bioinformatics computer vision and topic model . if the structure assume over variable be in the form of a d d or d grid then submodular function base on overlap group can be consider a norm lead to stable set equal to rectangular or convex shape . such method have application in computer vision . the problem of choose the best subset of input variable can be naturally formulated under a penalization framework a . where formula denote the formula norm define a the number of nonzero entry of the vector formula . although this formulation make sense from a modeling perspective it be computationally unfeasible a it be equivalent to an exhaustive search evaluate all possible subset of variable . two main approach for solve the optimization problem be greedy method such a stepwise regression in statistic or match pursuit in signal processing and convex relaxation formulation approach and proximal gradient optimization method . a natural approximation for the best subset selection problem be the formula norm regularization . such a scheme is call basis pursuit or the lasso which substitute the formula norm for the convex nondifferentiable formula norm . proximal gradient method also call forwardbackward splitting be optimization methods useful for minimizing function with a convex and differentiable component and a convex potentially nondifferentiable component . a such proximal gradient method be useful for solving sparsity and structure sparsity regularization problem of the follow form . where formula be a convex and differentiable loss function like the quadratic loss and formula be a convex potentially nondifferentiable regularizer such a the formula norm . structured sparsity regularization can be apply in the context of multiple kernel learn . multiple kernel learning refer to a set of machine learn method that use a predefined set of kernel and learn an optimal linear or nonlinear combination of kernel a part of the algorithm . in the algorithm mention above a whole space wa take into consideration at once and wa partition into group i . subspace . a complementary point of view be to consider the case in which distinct space are combine to obtain a new one . it be useful to discuss this idea consider finite dictionary . finite dictionary with linearly independent element these element are also know a atom refer to finite set of linearly independent basis function the linear combination of which define hypothesis space . finite dictionary can be use to define specific kernel a will be shown . assume for this example that rather than only one dictionary several finite dictionary are consider . for simplicity the case in which there be only two dictionary formula and formula where formula and formula be integer will be consider . the atom in formula as well a the atom in formula are assume to be linearly independent . let formula be the union of the two dictionary . consider the linear space of function formula give by linear combination of the form . formula . for some coefficient vector formula where formula . assume the atom in formula to still be linearly independent or equivalently that the map formula be one to one . the function in the space formula can be see a the sum of two component one in the space formula the linear combination of atom in formula and one in formula the linear combination of the atom in formula . one choice of norm on this space be formula . note that we can now view formula a a function space in which formula formula be subspace . in view of the linear independence assumption formula can be identify with formula and formula with formula respectively . the norm mention above can be see a the group norm in formulaassociated to the subspace formula formula provide a connection to structured sparsity regularization . here formula formula and formula can be see to be the reproduce kernel hilbert space with correspond feature map formula give by formula formula give by formula and formula give by the concatenation of formula respectively . in the structured sparsity regularization approach to this scenario the relevant group of variable which the group norm consider correspond to the subspace formula and formula . this approach promotes set the group of coefficient corresponding to these subspace to zero a oppose to only individual coefficient promoting sparse multiple kernel learn . the above reason directly generalizes to any finite number of dictionary or feature map . it can be extend to feature map inducing infinite dimensional hypothesis . space . consider sparse multiple kernel learning be useful in several situation include the follow . data fusion when each kernel correspond to a different kind of modalityfeature . nonlinear variable selection consider kernels formula depending only one dimension of the input . generally sparse multiple kernel learning be particularly useful when there be many kernel and model selection and interpretability be important . structured sparsity regularization method have been use in a number of setting where it is desire to impose an a priori input variable structure to the regularization process . some such application be .
in the field of statistical learning theory matrix regularization generalizes notion of vector regularization to case where the object to be learn be a matrix . the purpose of regularization be to enforce condition for example sparsity or smoothness that can produce stable predictive function . for example in the more common vector framework tikhonov regularization optimizes over . to find a vector formula that be a stable solution to the regression problem . when the system is describe by a matrix rather than a vector this problem can be write a . where the vector norm enforce a regularization penalty on formula ha been extend to a matrix norm on formula . matrix regularization have application in matrix completion multivariate regression and multitask learn . idea of feature and group selection can also be extend to matrix and these can be generalize to the nonparametric case of multiple kernel learn . consider a matrix formula to be learn from a set of example formula where formula go from formula to formula and formula go from formula to formula . let each input matrix formula be formula and let formula be of size formula . a general model for the output formula can be pose a . where the inner product be the frobenius inner product . for different application the matrix formula will have different form but for each of these the optimization problem to infer formula can be write a . where formula define the empirical error for a give formula and formula be a matrix regularization penalty . the function formula be typically chosen to be convex and is often select to enforce sparsity using formulanorms andor smoothness using formulanorms . finally formula be in the space of matrix formula with forbenius inner product . in the problem of matrix completion the matrix formula take the form . where formula and formula be the canonical basis in formula and formula . in this case the role of the frobenius inner product be to select individual element formula from the matrix formula . thus the output formula be a sampling of entry from the matrix formula . the problem of reconstruct formula from a small set of sampled entry be possible only under certain restriction on the matrix and these restriction can be enforce by a regularization function . for example it might be assume that formula be lowrank in which case the regularization penalty can take the form of a nuclear norm . where formula with formula from formula to formula be the singular value of formula . model use in multivariate regression are parameterized by a matrix of coefficient . in the frobenius inner product above each matrix formula be . such that the output of the inner product be the dot product of one row of the input with one column of the coefficient matrix . the familiar form of such model be . many of the vector norm use in single variable regression can be extend to the multivariate case . one example be the squared frobenius norm which can be view a an formulanorm acting either entrywise or on the singular value of the matrix . in the multivariate case the effect of regularize with the frobenius norm be the same a the vector case very complex model will have large norm and thus will be penalize more . the setup for multitask learning be almost the same a the setup for multivariate regression . the primary difference be that the input variable are also index by task column of formula . the representation with the frobenius inner product be then . the role of matrix regularization in this setting can be the same a in multivariate regression but matrix norm can also be use to couple learning problem across task . in particular note that for the optimization problem . the solution correspond to each column of formula be decoupled . that be the same solution can be find by solve the joint problem or by solve an isolate regression problem for each column . the problem can be couple by add an additional regulatization penalty on the covariance of solution . where formula model the relationship between task . this scheme can be use to both enforce similarity of solution across task and to learn the specific structure of task similarity by alternate between optimization of formula and formula . when the relationship between task is know to lie on a graph the laplacian matrix of the graph can be use to couple the learn problem . regularization by spectral filtering ha been use to find stable solution to problems such a those discuss above by address illposed matrix inversion see for example filter function for tikhonov regularization . in many case the regularization function act on the input or kernel to ensure a bound inverse by eliminate small singular value but it can also be useful to have spectral norm that act on the matrix that be to be learn . there be a number of matrix norm that act on the singular value of the matrix . frequently use example include the schatten pnorms with por . for example matrix regularization with a schatten norm also call the nuclear norm can be use to enforce sparsity in the spectrum of a matrix . this ha been use in the context of matrix completion when the matrix in question is believe to have a restricted rank . in this case the optimization problem becomes . spectral regularization is also use to enforce a reduce rank coefficient matrix in multivariate regression . in this set a reduce rank coefficient matrix can be find by keep just the top formula singular value but this can be extend to keep any reduced set of singular value and vector . sparse optimization ha become the focus of much research interest a a way to find solution that depend on a small number of variable see e . the lasso method . in principle entrywise sparsity can be enforce by penalize the entrywise formulanorm of the matrix but the formulanorm be not convex . in practice this can be implement by convex relaxation to the formulanorm . while entrywise regularization with an formulanorm will find solution with a small number of nonzero element apply an formulanorm to different group of variable can enforce structure in the sparsity of solution . the most straightforward example of structured sparsity use the formula norm with formula and formula . for example the formula norm is use in multitask learn to group feature across task such that all the element in a give row of the coefficient matrix can be force to zero a a group . the grouping effect is achieve by take the formulanorm of each row and then take the total penalty to be the sum of these rowwise norm . this regularization result in row that will tend to be all zero or dense . the same type of regularization can be use to enforce sparsity columnwise by take the formulanorms of each column . more generally the formula norm can be apply to arbitrary group of variable . where the index formula be across group of variable and formula indicate the cardinality of group formula . algorithm for solve these group sparsity problem extend the more wellknown lasso and group lasso method by allow overlapping group for example and have be implement via match pursuit and proximal gradient method . by write the proximal gradient with respect to a give coefficient formula it can be see that this norm enforce a groupwise soft threshold . where formula be the indicator function for group norm formula . thus using formula norm it be straightforward to enforce structure in the sparsity of a matrix either rowwise columnwise or in arbitrary block . by enforce group norm on block in multivariate or multitask regression for example it be possible to find group of input and output variable such that define subset of output variable column in the matrix formula will depend on the same sparse set of input variable . the idea of structured sparsity and feature selection can be extend to the nonparametric case of multiple kernel learn . this can be useful when there be multiple type of input data color and texture for example with different appropriate kernel for each or when the appropriate kernel be unknown . if there be two kernel for example with feature map formula and formula that lie in correspond reproducing kernel hilbert space formula then a large space formula can be create a the sum of two space . assume linear independence in formula and formula . in this case the formulanorm be again the sum of norm . thus by choose a matrix regularization function a this type of norm it be possible to find a solution that be sparse in term of which kernel are use but dense in the coefficient of each use kernel . multiple kernel learn can also be use a a form of nonlinear variable selection or a a model aggregation technique e . by take the sum of square norm and relax sparsity constraint . for example each kernel can be take to be the gaussian kernel with a different width .
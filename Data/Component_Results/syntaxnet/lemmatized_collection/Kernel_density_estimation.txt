in statistic kernel density estimation kde be a nonparametric way to estimate the probability density function of a random variable . kernel density estimation be a fundamental data smoothing problem where inference about the population are make base on a finite data sample . in some field such a signal processing and econometrics it is also term the parzenrosenblatt window method after emanuel parzen and murray rosenblatt who are usually credit with independently create it in it current form . let x x x be a univariate independent and identically distribute sample drawn from some distribution with an unknown density . we be interested in estimate the shape of this function . it kernel density estimator be . where k be the kernel a nonnegative function that integrate to one and be a smoothing parameter call the bandwidth . a kernel with subscript h is call the scale kernel and define a . intuitively one want to choose h as small a the data will allow however there be always a tradeoff between the bias of the estimator and it variance . the choice of bandwidth be discuss in more detail below . a range of kernel function are commonly use uniform triangular biweight triweight epanechnikov normal and others . the epanechnikov kernel be optimal in a mean square error sense though the loss of efficiency be small for the kernel list previously and due to it convenient mathematical property the normal kernel is often use which mean where be the standard normal density function . the construction of a kernel density estimate find interpretation in field outside of density estimation . for example in thermodynamics this be equivalent to the amount of heat generate when heat kernels the fundamental solution to the heat equation are place at each data point location x . similar method are use to construct discrete laplace operator on point cloud for manifold learn . kernel density estimate be closely related to histogram but can be endow with property such a smoothness or continuity by use a suitable kernel . to see this we compare the construction of histogram and kernel density estimator use these data point . for the histogram first the horizontal axis is divide into subintervals or bin which cover the range of the data . in this case we have bin each of width . whenever a data point fall inside this interval we place a box of height . if more than one data point fall inside the same bin we stack the box on top of each other . for the kernel density estimate we place a normal kernel with variance . indicate by the red dashed line on each of the data point x . the kernel are sum to make the kernel density estimate solid blue curve . the smoothness of the kernel density estimate be evident compare to the discreteness of the histogram a kernel density estimate converge faster to the true underlying density for continuous random variable . the bandwidth of the kernel be a free parameter which exhibit a strong influence on the resulting estimate . to illustrate it effect we take a simulate random sample from the standard normal distribution plot at the blue spike in the rug plot on the horizontal axis . the grey curve be the true density a normal density with mean and variance . in comparison the red curve be undersmoothed since it contain too many spurious data artifact arise from use a bandwidth h . which be too small . the green curve is oversmoothed since use the bandwidth h obscure much of the underlying structure . the black curve with a bandwidth of h . is consider to be optimally smooth since it density estimate be close to the true density . the most common optimality criterion use to select this parameter be the expect l risk function also term the mean integrate squared error . under weak assumption on and k be the generally unknown real density function . mise h amiseh onh h where o be the little o notation . the amise be the asymptotic mise which consists of the two lead term . where formula for a function g formula . and be the second derivative of . the minimum of this amise be the solution to this differential equation . or . neither the amise nor the h formula be able to be use directly since they involve the unknown density function or it second derivative so a variety of automatic databased method have been develop for select the bandwidth . many review study have been carry out to compare their efficacy with the general consensus that the plugin selector . substitute any bandwidth h which have the same asymptotic order n a h into the amise . give that amiseh on where o be the big o notation . it can be show that under weak assumption there cannot exist a nonparametric estimator that converge at a faster rate than the kernel estimator . note that the n rate be slow than the typical n convergence rate of parametric method . if the bandwidth be not held fix but is vary depend upon the location of either the estimate balloon estimator or the sample pointwise estimator this produce a particularly powerful method termed adaptive or variable bandwidth kernel density estimation . bandwidth selection for kernel density estimation of heavytailed distribution is say to be relatively difficult . if gaussian basis function are use to approximate univariate data and the underlying density being estimate be gaussian the optimal choice for h that be the bandwidth that minimise the mean integrate squared error be . where formula be the standard deviation of the sample . this approximation is term the normal distribution approximation gaussian approximation or silvermans rule of thumb . while this rule of thumb be easy to compute it should be use with caution a it can yield widely inaccurate estimate when the density be not close to be normal . for example consider estimate the bimodal gaussian mixture . from a sample of point . the figure on the right below show the true density and two kernel density estimate one use the ruleofthumb bandwidth and the other using . a solvetheequation bandwidth . the estimate base on the ruleofthumb bandwidth be significantly oversmoothed . the matlab script for this example us . kde . m and is give below . give the sample x x x it be natural to estimate the characteristic function a . know the characteristic function it be possible to find the correspond probability density function through the fourier transform formula . one difficulty with apply this inversion formula be that it lead to a diverge integral since the estimate formula be unreliable for large t . to circumvent this problem the estimator formula is multiply by a damping function which be equal to at the origin and then fall to at infinity . the bandwidth parameter h control how fast we try to dampen the function formula . in particular when h be small then t will be approximately one for a large range of t which mean that formula remains practically unaltered in the most important region of t . the most common choice for function be either the uniform function which effectively mean truncate the interval of integration in the inversion formula to or the gaussian function . once the function ha be choose the inversion formula may be apply and the density estimator will be . where k be the fourier transform of the damping function . thus the kernel density estimator coincide with the characteristic function density estimator . a nonexhaustive list of software implementation of kernel density estimators includes .
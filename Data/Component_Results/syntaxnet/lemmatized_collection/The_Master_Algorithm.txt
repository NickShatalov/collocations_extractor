depend on the type and variation in training data machine learn can be roughly categorize into three framework supervise learning unsupervised learning and reinforcement learning . multiple instance learn mil fall under the supervise learning framework where every training instance have a label either discrete or real valued . mil deal with problem with incomplete knowledge of label in train set . more precisely in multipleinstance learn the training set consists of label bag each of which be a collection of unlabeled instance . a bag is positively label if at least one instance in it be positive and is negatively label if all instance in it be negative . the goal of the mil be to predict the label of new unseen bag . convenient and simple example for mil wa give in . imagine several people and each of them have a key chain that contain few key . some of these people be able to enter a certain room and some arent . the task be then to predict whether a certain key or a certain key chain can get you into that room . to solve this problem we need to find the exact key that be common for all the positive key chain . if we can correctly identify this key we can also correctly classify an entire key chain positive if it contain the require key or negative if it doesnt . keeler et al . in his work in early s be the first one to explore the area of mil . the actual term multiinstance learn wa introduce in the middle of the s by dietterich et al . while they were investigate the problem of drug activity prediction . they try to create a learning system that could predict whether new molecule wa qualify to make some drug or not through analyze a collection of known molecule . molecule can have many alternative lowenergy state but only one or some of them are qualify to make a drug . the problem arose because scientist could only determine if molecule be qualified or not but they couldnt say exactly which of it lowenergy shape be responsible for that . one of the propose ways to solve this problem be to use supervise learn and regard all the lowenergy shape of the qualify molecule a positive training instance while all of the lowenergy shape of unqualified molecule a negative instance . dietterich et al . show that such method would have a high false positive noise from all lowenergy shape that are mislabeled a positive and thus wasnt really useful . their approach be to regard each molecule a a labeled bag and all the alternative lowenergy shape of that molecule a instance in the bag without individual label . thus formulate multipleinstance learn . solution to the multiple instance learn problem that dietterich et al . propose be three axisparallel rectangle apr algorithm . it attempt to search for appropriate axisparallel rectangle construct by the conjunction of the feature . they test the algorithm on musk dataset which be a concrete test data of drug activity prediction and the most popularly use benchmark in multipleinstance learn . apr algorithm achieve the best result but it should be note that apr wa design with musk data in mind . problem of multiinstance learn be not unique to drug find . in maron and ratan find another application of multiple instance learn to scene classification in machine vision and devised diverse density framework . give an image an instance is take to be one or more fixedsize subimages and the bag of instance is take to be the entire image . an image is label positive if it contain the target scene a waterfall for example and negative otherwise . multiple instance learn can be use to learn the property of the subimages which characterize the target scene . from there on these framework have been apply to a wide spectrum of application range from image concept learn and text categorization to stock market prediction . if the space of instance be formula then the set of bag be the set of function formula which be isomorphic to the set of multisubsets of formula . for each bag formula and each instance formula formula be viewed a the number of time formula occurs in formula . let formula be the space of label then a multiple instance concept be a map formula . the goal of mil be to learn such a concept . the remainder of the article will focus on binary classification where formula . most of the work on multiple instance learning include dietterich et al . and maron lozanoperez early paper make the assumption regard the relationship between the instance within a bag and the class label of the bag . because of it importance that assumption be often call standard mi assumption . the standard assumption take each instance formula to have an associated label formula which be hidden to the learner . the pair formula is call an instancelevel concept . a bag is now view a a multiset of instancelevel concept and is label positive if at least one of it instance have a positive label and negative if all of it instance have negative label . formally let formula be a bag . the label of formula be then formula . standard mi assumption be asymmetric which mean that if the positive and negative label are reverse the assumption have a different meaning . because of that when we use this assumption we need to be clear which label should be the positive one . standard assumption might be view a too strict and therefore in the recent year researcher try to relax that position which give rise to other more loose assumption . reason for this be the belief that standard mi assumption be appropriate for the musk dataset but since mli can be apply to numerous other problem some different assumption could probably be more appropriate . guide by that idea weidmann formulate a hierarchy of generalize instancebased assumption for mil . it consist of the standard mi assumption and three type of generalize mi assumptions each more general than the last formula with the countbased assumption be the most general and the standard assumption be the least general . one would expect an algorithm which perform well under one of these assumption to perform at least as well under the less general assumption . the presencebased assumption be a generalization of the standard assumption wherein a bag must contain one or more instance that belong to a set of require instancelevel concept in order to be label positive . formally let formula be the set of require instancelevel concept and let formula denote the number of time the instancelevel concept formula occurs in the bag formula . then formula for all formula . note that by take formula to contain only one instancelevel concept the presencebased assumption reduces to the standard assumption . a further generalization come with the thresholdbased assumption where each require instancelevel concept must occur not only once in a bag but some minimum threshold number of time in order for the bag to be label positive . with the notation above to each require instancelevel concept formula is associate a threshold formula . for a bag formula formula for all formula . the countbased assumption be a final generalization which enforce both low and upper bound for the number of time a required concept can occur in a positively labeled bag . each required instancelevel concept formula have a low threshold formula and upper threshold formula with formula . a bag formula is label according to formula for all formula . scott zhang and brown describe another generalization of the standard model which they call generalize multiple instance learn gmil . the gmil assumption specify a set of required instance formula . a bag formula is label positive if it contain instance which be sufficiently close to at least formula of the required instance formula . under only this condition the gmil assumption be equivalent to the presencebased assumption . however scott et . al . describe a further generalization in which there be a set of attraction point formula and a set of repulsion point formula . a bag is label positive if and only if it contain instance which be sufficiently close to at least formula of the attraction point and be sufficiently close to at most formula of the repulsion point . this condition be strictly more general than the presencebased though it doe not fall within the above hierarchy . in contrast to the previous assumption where the bag were view a fix the collective assumption view a bag formula a a distribution formula over instance formula and similarly view label a a distribution formula over instance . the goal of an algorithm operating under the collective assumption be then to model the distribution formula . since formula is typically consider fix but unknown algorithm instead focus on compute the empirical version formula where formula be the number of instance in bag formula . since formula be also typically take to be fix but unknown most collectiveassumption base methods focus on learn this distribution a in the singleinstance version . while the collective assumption weight every instance with equal importance foulds extend the collective assumption to incorporate instance weight . the weight collective assumption be then that formula where formula be a weight function over instance and formula . there be two major flavor of algorithm for multiple instance learn instancebased and metadatabased or embeddingbased algorithm . the term instancebased denote that the algorithm attempt to find a set of representative instance base on an mi assumption and classify future bag from these representative . by contrast metadatabased algorithm make no assumption about the relationship between instance and bag label and instead try to extract instanceindependent information or metadata about the bag in order to learn the concept . for a survey of some of the modern mi algorithm see foulds and frank the earliest propose mi algorithm be a set of iterateddiscrimination algorithm develop by dietterich et . al and diverse density developed by maron and lozanoprez . both of these algorithm operate under the standard assumption . broadly all of the iterateddiscrimination algorithm consist of two phase . the first phase be to grow an axis parallel rectangle apr which contain at least one instance from each positive bag and no instance from any negative bag . this is do iteratively start from a random instance formula in a positive bag the apr be expanded to the small apr cover any instance formula in a new positive bag formula . this process be repeated until the apr cover at least one instance from each positive bag . then each instance formula contain in the apr is give a relevance corresponding to how many negative point it exclude from the apr if remove . the algorithm then select candidate representative instance in order of decrease relevance until no instance contain in a negative bag is also contain in the apr . the algorithm repeat these growth and representative selection step until convergence where apr size at each iteration is take to be only along candidate representative . after the first phase the apr is think to tightly contain only the representative attribute . the second phase expand this tight apr a follow a gaussian distribution is center at each attribute and a looser apr is draw such that positive instance will fall outside the tight apr with fix probability . though iterated discrimination technique work well with the standard assumption they do not generalize well to other mi assumption . in it simple form diverse density dd assume a single representative instance formula a the concept . this representative instance must be dense in that it be much close to instance from positive bag than from negative bag as well a diverse in that it be close to at least one instance from each positive bag . let formula be the set of positively label bag and let formula be the set of negatively label bag then the best candidate for the representative instance is give by formula where the diverse density formula under the assumption that bag are independently distribute given the concept formula . let formula denote the jth instance of bag i the noisyor model givesformula is take to be the scaled distance formula where formula be the scaling vector . this way if every positive bag have an instance close to formula then formula will be high for each formula but if any negative bag formula have an instance close to formula formula will be low . hence formula be high only if every positive bag have an instance close to formula and no negative bag have an instance close to formula . the candidate concept formula can be obtain through gradient method . classification of new bag can then be do by evaluating proximity to formula . though diverse density wa originally propose by maron et . al . in more recent mil algorithm use the dd framework such a emdd in and ddsvm in and mile in a number of singleinstance algorithm have also been adapt to a multipleinstance context under the standard assumption includingpost there be a movement away from the standard assumption and the development of algorithm design to tackle the more general assumption list above . because of the high dimensionality of the new feature space and the cost of explicitly enumerating all apr of the original instance space gmil be inefficient both in term of computation and memory . gmil wa develop a a refinement of gmil in an effort to improve efficiency . gmil preprocesses the instance to find a set of candidate representative instance . gmil then map each bag to a boolean vector a in gmil but only consider apr correspond to unique subset of the candidate representative instance . this significantly reduce the memory and computational requirement . by map each bag to a feature vector of metadata metadatabased algorithm allow the flexibility of use an arbitrary singleinstance algorithm to perform the actual classification task . future bag are simply map embed into the feature space of metadata and label by the chosen classifier . therefore much of the focus for metadatabased algorithm be on what feature or what type of embed lead to effective classification . note that some of the previously mention algorithms such a tlc and gmil could be consider metadatabased . they define two variation of knn bayesianknn and citationknn a adaptation of the traditional nearestneighbor problem to the multipleinstance setting . so far this article ha consider multiple instance learn exclusively in the context of binary classifier . however the generalization of singleinstance binary classifier can carry over to the multipleinstance case .
{"url": "https://en.wikipedia.org/wiki?curid=53970843", "text": "Machine learning in bioinformatics\n\nMachine learning, a subfield of computer science involving the development of algorithms that learn how to make predictions based on data, has a number of emerging applications in the field of bioinformatics. Bioinformatics deals with computational and mathematical approaches for understanding and processing biological data. \n\nPrior to the emergence of machine learning algorithms, bioinformatics algorithms had to be explicitly programmed by hand which, for problems such as protein structure prediction, proves extremely difficult. Machine learning techniques such as deep learning enable the algorithm to make use of automatic feature learning which means that based on the dataset alone, the algorithm can learn how to combine multiple features of the input data into a more abstract set of features from which to conduct further learning. This multi-layered approach to learning patterns in the input data allows such systems to make quite complex predictions when trained on large datasets. In recent years, the size and number of available biological datasets have skyrocketed, enabling bioinformatics researchers to make use of these machine learning systems. Machine learning has been applied to six main subfields of bioinformatics: genomics, proteomics, microarrays, systems biology, evolution, and text mining.\n\nGenomics involves the study of the genome, the complete DNA sequence, of organisms. While genomic sequence data has historically been sparse due to the technical difficulty in sequencing a piece of DNA, the number of available sequences is growing exponentially. However, while raw data is becoming increasingly available and accessible, the biological interpretation of this data is occurring at a much slower pace. Therefore, there is an increasing need for the development of machine learning systems that can automatically determine the location of protein-encoding genes within a given DNA sequence. This is a problem in computational biology known as gene prediction.\n\nGene prediction is commonly performed through a combination of what are known as extrinsic and intrinsic searches. For the extrinsic search, the input DNA sequence is run through a large database of sequences whose genes have been previously discovered and their locations annotated. A number of the sequence's genes can be identified by determining which strings of bases within the sequence are homologous to known gene sequences. However, given the limitation in size of the database of known and annotated gene sequences, not all the genes in a given input sequence can be identified through homology alone. Therefore, an intrinsic search is needed where a gene prediction program attempts to identify the remaining genes from the DNA sequence alone.\n\nMachine learning is also been used for the problem of multiple sequence alignment which involves aligning many DNA or amino acid sequences in order to determine regions of similarity that could indicate a shared evolutionary history.\nIt can also be used to detect and visualize genome rearrangements.\n\nProteins, strings of amino acids, gain much of their function from protein folding in which they conform into a three-dimensional structure. This structure is composed of a number of layers of folding, including the primary structure (i.e. the flat string of amino acids), the secondary structure (alpha helices and beta sheets), the tertiary structure, and the quartenary structure.\n\nProtein secondary structure prediction is a main focus of this subfield as the further protein foldings (tertiary and quartenary structures) are determined based on the secondary structure. Solving the true structure of a protein is an incredibly expensive and time-intensive process, furthering the need for systems that can accurately predict the structure of a protein by analyzing the amino acid sequence directly. Prior to machine learning, researchers needed to conduct this prediction manually. This trend began in 1951 when Pauling and Corey released their work on predicting the hydrogen bond configurations of a protein from a polypeptide chain. Today, through the use of automatic feature learning, the best machine learning techniques are able to achieve an accuracy of 82-84%. The current state-of-the-art in secondary structure prediction uses a system called DeepCNF (deep convolutional neural fields) which relies on the machine learning model of artificial neural networks to achieve an accuracy of approximately 84% when tasked to classify the amino acids of a protein sequence into one of three structural classes (helix, sheet, or coil). The theoretical limit for three-state protein secondary structure is 88–90%.\n\nMachine learning has also been applied to proteomics problems such as protein side-chain prediction, protein loop modeling, and protein contact map prediction.\n\nMicroarrays, a type of lab-on-a-chip, are used for automatically collecting data about large amounts of biological material. Machine learning can aid in the analysis of this data, and it has been applied to expression pattern identification, classification, and genetic network induction.\n\nThis technology is especially useful for monitoring the expression of genes within a genome, aiding in diagnosing different types of cancer based on which genes are expressed. One of the main problems in this field is identifying which genes are expressed based on the collected data. In addition, due to the huge number of genes on which data is collected by the microarray, there is a large amount of irrelevant data to the task of expressed gene identification, further complicating this problem. Machine learning presents a potential solution to this problem as various classification methods can be used to perform this identification. The most commonly used methods are radial basis function networks, deep learning, Bayesian classification, decision trees, and random forest.\n\nSystems biology focuses on the study of the emergent behaviors from complex interactions of simple biological components in a system. Such components can include molecules such as DNA, RNA, proteins, and metabolites.\n\nMachine learning has been used to aid in the modelling of these complex interactions in biological systems in domains such as genetic networks, signal transduction networks, and metabolic pathways. Probabilistic graphical models, a machine learning technique for determining the structure between different variables, are one of the most commonly used methods for modeling genetic networks. In addition, machine learning has been applied to systems biology problems such as identifying transcription factor binding sites using a technique known as Markov chain optimization. Genetic algorithms, machine learning techniques which are based on the natural process of evolution, have been used to model genetic networks and regulatory structures.\n\nOther systems biology applications of machine learning include the task of enzyme function prediction, high throughput microarray data analysis, analysis of genome-wide association studies to better understand markers of Multiple Sclerosis, protein function prediction, and identification of NCR-sensitivity of genes in yeast.\n\nThe increase in available biological publications led to the issue of the increase in difficulty in searching through and compiling all the relevant available information on a given topic across all sources. This task is known as knowledge extraction. This is necessary for biological data collection which can then in turn be fed into machine learning algorithms to generate new biological knowledge. Machine learning can be used for this knowledge extraction task using techniques such as natural language processing to extract the useful information from human-generated reports in a database. Text Nailing, an alternative approach to machine learning, capable of extracting features from clinical narrative notes was introduced in 2017.\n\nThis technique has been applied to the search for novel drug targets, as this task requires the examination of information stored in biological databases and journals. Annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein, so additional information must be extracted from biomedical literature. Machine learning has been applied to automatic annotation of the function of genes and proteins, determination of the subcellular localization of a protein, analysis of DNA-expression arrays, large-scale protein interaction analysis, and molecule interaction analysis.\n\nAnother application of text mining is the detection and visualization of distinct DNA regions given sufficient reference data.\n", "id": "53970843", "title": "Machine learning in bioinformatics"}
{"url": "https://en.wikipedia.org/wiki?curid=54033657", "text": "Labeled data\n\nLabeled data is a group of samples that have been tagged with one or more labels. Labeling typically takes a set of unlabeled data and augments each piece of that unlabeled data with meaningful tags that are informative. For example, labels might be indicate whether a photo contains a horse or a cow, which words were uttered in an audio recording, what type of action is being performed in a video, what the topic of a news article is, what the overall sentiment of a tweet is, whether the dot in an x-ray is a tumor, etc.\n\nLabels can be obtained by asking humans to make judgments about a given piece of unlabeled data (e.g., \"Does this photo contain a horse or a cow?\"), and are significantly more expensive to obtain than the raw unlabeled data.\n\nAfter obtaining a labeled dataset, machine learning models can be applied to the data so that new unlabeled data can be presented to the model and a likely label can be guessed or predicted for that piece of unlabeled data.\n\nDataset could be of different types depending on use case, look at some open human labeled datasets :\n\nOpen Human Labeled Datasets\n", "id": "54033657", "title": "Labeled data"}
{"url": "https://en.wikipedia.org/wiki?curid=53631046", "text": "Caffe (software)\n\nCAFFE (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework, originally developed at UC Berkeley. It is open source, under a BSD license. It is written in C++, with a Python interface.\n\nYangqing Jia created the caffe project during his PhD at UC Berkeley. Now there are many contributors to the project, and it is hosted at GitHub.\n\nCaffe supports many different types of deep learning architectures geared towards image classification and image segmentation. It supports CNN, RCNN, LSTM and fully connected neural network designs. Caffe supports GPU- and CPU-based acceleration computational kernel libraries such as NVIDIA cuDNN and Intel MKL.\n\nCaffe is being used in academic research projects, startup prototypes, and even large-scale industrial applications in vision, speech, and multimedia. Yahoo! has also integrated caffe with Apache Spark to create CaffeOnSpark, a distributed deep learning framework.\n\nIn April 2017, Facebook announced Caffe2, which includes new features such as Recurrent Neural Networks.\nEnd of March 2018, Caffe2 was merged into PyTorch.\n\n\n", "id": "53631046", "title": "Caffe (software)"}
{"url": "https://en.wikipedia.org/wiki?curid=44577560", "text": "Occam learning\n\nIn computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set.\n\nOccam learnability implies PAC learning, and for a wide variety of concept classes, the converse is also true: PAC learnability implies Occam learnability.\n\nOccam Learning is named after Occam's razor, which is a principle stating that, given all other things being equal, a shorter explanation for observed data should be favored over a lengthier explanation. The theory of Occam learning is a formal and mathematical justification for this principle. It was first shown by Blumer, et al. that Occam learning implies PAC learning, which is the standard model of learning in computational learning theory. In other words, \"parsimony\" (of the output hypothesis) implies \"predictive power\".\n\nThe succinctness of a concept formula_1 in concept class formula_2 can be expressed by the length formula_3 of the shortest bit string that can represent formula_1 in formula_2. Occam learning connects the succinctness of a learning algorithm's output to its predictive power on unseen data.\n\nLet formula_2 and formula_7 be concept classes containing target concepts and hypotheses respectively. Then, for constants formula_8 and formula_9, a learning algorithm formula_10 is an formula_11-Occam algorithm for formula_2 using formula_7 iff, given a set formula_14 of formula_15 samples labeled according to a concept formula_16, formula_10 outputs a hypothesis formula_18 such that\nwhere formula_24 is the maximum length of any sample formula_25. An Occam algorithm is called \"efficient\" if it runs in time polynomial in formula_24, formula_15, and formula_28 We say a concept class formula_2 is \"Occam learnable\" with respect to a hypothesis class formula_7 if there exists an efficient Occam algorithm for formula_2 using formula_32\n\nOccam learnability implies PAC learnability, as the following theorem of Blumer, et al. shows:\n\nLet formula_10 be an efficient formula_11-Occam algorithm for formula_2 using formula_7. Then there exists a constant formula_37 such that for any formula_38, for any distribution formula_39, given formula_40 samples drawn from formula_39 and labelled according to a concept formula_42 of length formula_24 bits each, the algorithm formula_10 will output a hypothesis formula_45 such that formula_46 with probability at least formula_47 .Here, formula_48 is with respect to the concept formula_1 and distribution formula_39. This implies that the algorithm formula_10 is also a PAC learner for the concept class formula_2 using hypothesis class formula_7. A slightly more general formulation is as follows:\n\nLet formula_38. Let formula_10 be an algorithm such that, given formula_15 samples drawn from a fixed but unknown distribution formula_57 and labeled according to a concept formula_42 of length formula_24 bits each, outputs a hypothesis formula_60 that is consistent with the labeled samples. Then, there exists a constant formula_61 such that if formula_62, then formula_10 is guaranteed to output a hypothesis formula_60 such that formula_46 with probability at least formula_47.\n\nWhile the above theorems show that Occam learning is sufficient for PAC learning, it doesn't say anything about \"necessity.\" Board and Pitt show that, for a wide variety of concept classes, Occam learning is in fact necessary for PAC learning. They proved that for any concept class that is \"polynomially closed under exception lists,\" PAC learnability implies the existence of an Occam algorithm for that concept class. Concept classes that are polynomially closed under exception lists include Boolean formulas, circuits, deterministic finite automata, decision-lists, decision-trees, and other geometrically-defined concept classes.\n\nA concept class formula_2 is polynomially closed under exception lists if there exists a polynomial-time algorithm formula_68 such that, when given the representation of a concept formula_42 and a finite list formula_70 of \"exceptions\", outputs a representation of a concept formula_71 such that the concepts formula_1 and formula_73 agree except on the set formula_70.\n\nWe first prove the Cardinality version. Call a hypothesis formula_75 \"bad\" if formula_76, where again formula_48 is with respect to the true concept formula_1 and the underlying distribution formula_57. The probability that a set of samples formula_21 is consistent with formula_19 is at most formula_82, by the independence of the samples. By the union bound, the probability that there exists a bad hypothesis in formula_83 is at most formula_84, which is less than formula_85 if formula_86. This concludes the proof of the second theorem above.\n\nUsing the second theorem, we can prove the first theorem. Since we have a formula_11-Occam algorithm, this means that any hypothesis output by formula_10 can be represented by at most formula_89 bits, and thus formula_90. This is less than formula_91 if we set formula_92 for some constant formula_37. Thus, by the Cardinality version Theorem, formula_10 will output a consistent hypothesis formula_19 with probability at least formula_96. This concludes the proof of the first theorem above.\n\nThough Occam and PAC learnability are equivalent, the Occam framework can be used to produce tighter bounds on the sample complexity of classical problems including conjunctions, conjunctions with few relevant variables, and decision lists.\n\nOccam algorithms have also been shown to be successful for PAC learning in the presence of errors, probabilistic concepts, function learning and Markovian non-independent examples.\n\n", "id": "44577560", "title": "Occam learning"}
{"url": "https://en.wikipedia.org/wiki?curid=54625345", "text": "Right to explanation\n\nIn the regulation of algorithms, particularly artificial intelligence and its subfield of machine learning, a right to explanation (or right to \"an\" explanation) is a right to be given an explanation for an output of the algorithm. Such rights primarily refer to individual rights to be given an explanation for decisions that significantly affect an individual, particularly legally or financially. For example, a person who applies for a loan and is denied may ask for an explanation, which could be \"Credit bureau X reports that you declared bankruptcy last year; this is the main factor in considering you too likely to default, and thus we will not give you the loan you applied for.\"\n\nSome such legal rights already exist, while the scope of a general \"right to explanation\" is a matter of ongoing debate.\n\nCredit score in the United States – more generally, credit actions – have a well-established right to explanation. Under the Equal Credit Opportunity Act (Regulation B of the Code of Federal Regulations),\nTitle 12, Chapter X, Part 1002, §1002.9, creditors are required to notify applicants of action taken in certain circumstances, and such notifications must provide specific reasons, as detailed in §1002.9(b)(2):\n\nThe official interpretation of this section details what types of statements are acceptable.\n\nCredit agencies and data analysis firms such as FICO comply with this regulation by providing a list of reasons (generally at most 4, per interpretation of regulations), consisting of a numeric (as identifier) and an associated explanation, identifying the main factors affecting a credit score. An example might be:\n\nThe European Union General Data Protection Regulation (enacted 2016, taking effect 2018), extends the automated decision-making rights in the 1995 Data Protection Directive to provide a legally disputed form of a right to an explanation, stated as such in Recital 71: \"[the data subject should have] the right ... to obtain an explanation of the decision reached\". In full:\n\nHowever, the extent to which the regulations themselves provide a \"right to explanation\" is heavily debated. There are two main strands of criticism. There are significant legal issues with the right as found in Article 22 — as recitals are not binding, and the right to an explanation is not mentioned in the binding articles of the text, having been removed during the legislative process. In addition, there are significant restrictions on the types of automated decisions that are covered — which must be both \"solely\" based on automated processing, and have legal or similarly significant effect — which limits their applicability in many of the cases of algorithmic controversy that have been picked up in the media.\n\nA second source of such a right has been pointed to in Article 15, the \"right of access by the data subject\". This restates a similar provision from the 1995 Data Protection Directive, allowing the data subject access to \"meaningful information about the logic involved\" in the same significant, solely automated decision-making, found in Article 22. Yet this too suffers from alleged challenges that relate to the timing of when this right can be drawn upon, as well as practical challenges that mean it may not be binding in many cases of public concern.\n\nIn France the 2016 \"Loi pour une République numérique\" (Digital Republic Act or \"loi numérique\") amends the country's administrative code to introduce a new provision for the explanation of decisions made by public sector bodies about individuals. It notes that where there is \"a decision taken on the basis of an algorithmic treatment\", the rules that define that treatment and its “principal characteristics” must be communicated to the citizen upon request, where there is not an exclusion (e.g. for national security or defence). These should include the following:\nScholars have noted that this right, while limited to administrative decisions, goes beyond the GDPR right to explicitly apply to decision support rather than decisions \"solely\" based on automated processing, as well as provides a framework for explaining specific decisions. Indeed, the GDPR automated decision-making rights in the European Union, one of the places a \"right to an explanation\" has been sought within, find their origins in French law in the late 1970s.\n\nSome argue that a \"right to explanation\" is at best unnecessary, at worst harmful, and threatens to stifle innovation. Specific criticisms include: favoring human decisions over machine decisions; being redundant with existing laws; and focusing on process over outcome.\n\nMore fundamentally, many algorithms used in machine learning are not easily explainable. For example, the output of a deep neural network depends on many layers of computations, connected in a complex way, and no one input or computation may be a dominant factor. The field of Explainable AI seeks to provide better explanations from existing algorithms, and algorithms that are more easily explainable, but it is a young and active field.\n\nSimilarly, human decisions often cannot be easily explained: they may be based on intuition or a \"gut feeling\" that is hard to put into words. Requiring machines to meet a higher standard than humans is thus arguably unreasonable.\n\n\n", "id": "54625345", "title": "Right to explanation"}
{"url": "https://en.wikipedia.org/wiki?curid=54550729", "text": "Connectionist temporal classification\n\nConnectionist temporal classification (CTC) is a type of neural network output and associated scoring function, for training recurrent neural networks (RNNs) such as LSTM networks to tackle sequence problems where the timing is variable. It can be used for tasks like on-line handwriting recognition or recognizing phonemes in speech audio. CTC refers to the outputs and scoring, and is independent of the underlying neural network structure. It was introduced in 2006.\n\nThe input is a sequence of observations, and the outputs are a sequence of labels, which can include blank outputs. The difficulty of training comes from there being many more observations than there are labels. For example in speech audio there can be multiple time slices which correspond to a single phoneme. Since we don't know the alignment of the observed sequence with the target labels we predict a probability distribution at each time step. A CTC network has a continuous output (e.g. softmax), which is fitted through training to model the probability of a label. CTC does not attempt to learn boundaries and timings: Label sequences are considered equivalent if they differ only in alignment, ignoring blanks. Equivalent label sequences can occur in many ways – which makes scoring a non-trivial task. Fortunately there is an efficient forward–backward algorithm for that.\n\nCTC scores can then be used with the back-propagation algorithm to update the neural network weights.\n\nAlternative approaches to a CTC-fitted neural network include a hidden Markov model (HMM).\n", "id": "54550729", "title": "Connectionist temporal classification"}
{"url": "https://en.wikipedia.org/wiki?curid=52003586", "text": "End-to-end reinforcement learning\n\nIn end-to-end reinforcement learning, the end-to-end process, in other words, the entire process from sensors to motors in a robot or agent involves a single layered or recurrent neural network without modularization. The network is trained by reinforcement learning (RL). The approach has been proposed for a long time, but was reenergized by the successful results in learning to play Atari video games (2013–15) and AlphaGo (2016) by Google DeepMind. It employs supervised learning, without requiring sample (labeled, usually manually) data.\n\nRL traditionally required explicit design of state space and action space, while the mapping from state space to action space is learned. Therefore, RL has been limited to learning only for action, and human designers have to design how to construct state space from sensor signals and to give how the motion commands are generated for each action before learning. Neural networks have been often used in RL, to provide non-linear function approximation to avoid the curse of dimensionality. Recurrent neural networks have been also employed, mainly to avoid perceptual aliasing or partially observable Markov decision process (POMDP).\n\nEnd-to-end RL extends RL from learning only for actions to learning the entire process from sensors to motors including higher-level functions that are difficult to develop independently from other functions. Higher-level functions do not connect directly with either sensors or motors, and so even giving their inputs and outputs is difficult.\n\nThe approach originated in TD-Gammon (1992). In backgammon, the evaluation of the game situation during self-play was learned through TD(formula_1) using a layered neural network. Four inputs were used for the number of pieces of a given color at a given location on the board, totaling 198 input signals. With zero knowledge built in, the network learned to play the game at an intermediate level.\n\nShibata began working with this framework in 1997. They employed Q-learning and actor-critic for continuous motion tasks, and used a recurrent neural network for memory-required tasks. They applied this framework to some real robot tasks. They demonstrated learning of various functions.\n\nBeginning around 2013, Google DeepMind showed impressive learning results in video games and game of Go (AlphaGo). They used a deep convolutional neural network that showed superior results in image recognition. They used 4 frames of almost raw RGB pixels (84x84) as inputs. The network was trained based on RL with the reward representing the sign of the change in the game score. All 49 games were learned using the same network architecture and Q-learning with minimal prior knowledge, and outperformed competing methods on almost all the games and performed at a level that is comparable or superior to a professional human game tester. It is sometimes called Deep-Q network (DQN). In AlphaGo, deep neural networks are trained not only by reinforcement learning, but also by supervised learning and Monte Carlo tree search.\n\nShibata's group showed that various functions emerge in this framework, including: \n\n\nCommunications were established in this framework. Modes include:\n\n", "id": "52003586", "title": "End-to-end reinforcement learning"}
{"url": "https://en.wikipedia.org/wiki?curid=55075082", "text": "BigDL\n\nBigDL is a distributed deep learning framework for Apache Spark, created by Jason Dai at Intel.\nIt is hosted at GitHub.\n\n", "id": "55075082", "title": "BigDL"}
{"url": "https://en.wikipedia.org/wiki?curid=55375136", "text": "Highway network\n\nIn machine learning, a highway network is an approach to optimizing networks and increasing their depth. Highway networks use learned gating mechanisms to regulate information flow, inspired by Long Short-Term Memory (LSTM) recurrent neural networks. The gating mechanisms allow neural networks to have paths for information to follow across different layers (\"information highways\").\n\nHighway networks have been used as part of text sequence labeling and speech recognition tasks.\n", "id": "55375136", "title": "Highway network"}
{"url": "https://en.wikipedia.org/wiki?curid=54994687", "text": "Documenting Hate\n\nDocumenting Hate is a project of ProPublica, in collaboration with a number of journalistic, academic, and computing organizations, for systematic tracking of hate crimes and bias incidents. It uses an online form to facilitate reporting of incidents by the general public. Since August 2017, it has also used machine learning and natural language processing techniques to monitor and collect news stories about hate crimes and bias incidents. , over 100 news organizations had joined the project.\n\nDocumenting Hate was created in response to ProPublica's dissatisfaction with the quality of reporting and tracking of evidence of hate crimes and bias incidents after the United States presidential election of 2016. The project was launched on 17 January 2017, after the publication on 15 November 2016 of a ProPublica news story about the difficulty of obtaining hard data on hate crimes.\n\nOn 18 August 2017, ProPublica and Google announced the creation of the Documenting Hate News Index, which uses the Google Cloud Natural Language API for automated monitoring and collection of news stories about hate crimes and bias incidents. The API uses machine learning and natural language processing techniques. The findings of the Index are integrated with reports from members of the public. The Index is a joint project of ProPublica, Google News Lab, and the data visualization studio Pitch Interactive.\n\n, thousands of incidents had been reported via Documenting Hate. , over 100 news organizations had joined the project, including the \"Boston Globe\", the \"New York Times\", \"Vox\", and the Georgetown University \"Hoya\".\n\nAn education reporter for the conservative \"Daily Caller\" has criticized the project for ambiguity in the terms it uses to describe hate crimes, and for neglect of hate-crime hoaxes. Another \"Daily Caller\" journalist has likewise criticized the Documenting Hate News Index for underrepresentation of conservative outlets among the news sources it monitors. \n\nA policy analyst for the Center for Data Innovation (an affiliate of the Information Technology and Innovation Foundation), while supporting ProPublica's critique of the present state of hate-crime statistics, and praising ProPublica for drawing attention to the problem, has argued that a nongovernmental project like Documenting Hate cannot solve it unaided; instead, intervention at the federal level is needed.\n\n", "id": "54994687", "title": "Documenting Hate"}
{"url": "https://en.wikipedia.org/wiki?curid=1363880", "text": "Random forest\n\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\nThe first algorithm for random decision forests was created by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg.\n\nAn extension of the algorithm was developed by Leo Breiman and Adele Cutler, and \"Random Forests\" is their trademark. The extension combines Breiman's \"bagging\" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.\n\nThe general method of random decision forests was first proposed by Ho in 1995, who established that forests of trees splitting with oblique hyperplanes, if randomly restricted to be sensitive to only selected feature dimensions, can gain accuracy as they grow without suffering from overtraining. A subsequent work along the same lines concluded that other splitting methods, as long as they are randomly forced to be insensitive to some feature dimensions, behave similarly. Note that this observation of a more complex classifier (a larger forest) getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting. The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination.\nThe early development of Breiman's notion of random forests was influenced by the work of Amit and\nGeman who introduced the idea of searching over a random subset of the\navailable decisions when splitting a node, in the context of growing a single\ntree. The idea of random subspace selection from Ho was also influential in the design of random forests. In this method a forest of trees is grown,\nand variation among the trees is introduced by projecting the training data\ninto a randomly chosen subspace before fitting each tree or each node. Finally, the idea of\nrandomized node optimization, where the decision at each node is selected by a\nrandomized procedure, rather than a deterministic optimization was first\nintroduced by Dietterich.\n\nThe introduction of random forests proper was first made in a paper\nby Leo Breiman. This paper describes a method of building a forest of\nuncorrelated trees using a CART like procedure, combined with randomized node\noptimization and bagging. In addition, this paper combines several\ningredients, some previously known and some novel, which form the basis of the\nmodern practice of random forests, in particular:\n\n\nThe report also offers the first theoretical result for random forests in the\nform of a bound on the generalization error which depends on the strength of the\ntrees in the forest and their correlation.\n\nDecision trees are a popular method for various machine learning tasks. Tree learning \"come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining\", say Hastie \"et al.\", \"because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate\".\n\nIn particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model.\n\nThe training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set = , ..., with responses = , ..., , bagging repeatedly (\"B\" times) selects a random sample with replacement of the training set and fits trees to these samples:\n\nAfter training, predictions for unseen samples can be made by averaging the predictions from all the individual regression trees on :\n\nor by taking the majority vote in the case of classification trees.\n\nThis bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.\n\nAdditionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on :\n\nThe number of samples/trees, , is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. An optimal number of trees can be found using cross-validation, or by observing the \"out-of-bag error\": the mean prediction error on each training sample , using only the trees that did not have in their bootstrap sample.\nThe training and test error tend to level off after some number of trees have been fit.\n\nThe above procedure describes the original bagging algorithm for trees. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called \"feature bagging\". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho.\n\nTypically, for a classification problem with features, (rounded down) features are used in each split. For regression problems the inventors recommend (rounded down) with a minimum node size of 5 as the default.\n\nAdding one further step of randomization yields \"extremely randomized trees\", or ExtraTrees. These are trained using bagging and the random subspace method, like in an ordinary random forest, but additionally the top-down splitting in the tree learner is randomized. Instead of computing the locally \"optimal\" feature/split combination (based on, e.g., information gain or the Gini impurity), for each feature under consideration, a random value is selected for the split. This value is selected from the feature's empirical range (in the tree's training set, i.e., the bootstrap sample).\n\nRandom forests can be used to rank the importance of variables in a regression or classification problem in a natural way. The following technique was described in Breiman's original paper and is implemented in the R package randomForest.\n\nThe first step in measuring the variable importance in a data set formula_3 is to fit a random forest to the data. During the fitting process the out-of-bag error for each data point is recorded and averaged over the forest (errors on an independent test set can be substituted if bagging is not used during training).\n\nTo measure the importance of the formula_4-th feature after training, the values of the formula_4-th feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set. The importance score for the formula_4-th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees. The score is normalized by the standard deviation of these differences.\n\nFeatures which produce large values for this score are ranked as more important than features which produce small values. The statistical definition of the variable importance measure was given and analyzed by Zhu \"et al.\"\n\nThis method of determining variable importance has some drawbacks. For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Methods such as partial permutations\nand growing unbiased treescan be used to solve the problem. If the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups.\n\nA relationship between random forests and the -nearest neighbor algorithm (-NN) was pointed out by Lin and Jeon in 2002. It turns out that both can be viewed as so-called \"weighted neighborhoods schemes\". These are models built from a training set formula_7 that make predictions formula_8 for new points by looking at the \"neighborhood\" of the point, formalized by a weight function :\n\nHere, formula_10 is the non-negative weight of the 'th training point relative to the new point in the same tree. For any particular , the weights for points formula_11 must sum to one. Weight functions are given as follows:\n\n\nSince a forest averages the predictions of a set of trees with individual weight functions formula_14, its predictions are\n\nThis shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. The neighbors of in this interpretation are the points formula_11 sharing the same leaf in any tree formula_4. In this way, the neighborhood of depends in a complex way on the structure of the trees, and thus on the structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature.\n\nAs part of their construction, random forest predictors naturally lead to a dissimilarity measure among the observations. One can also define a random forest dissimilarity measure between unlabeled data: the idea is to construct a random forest predictor that distinguishes the “observed” data from suitably generated synthetic data.\nThe observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. A random forest dissimilarity can be attractive because it handles mixed variable types very well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. The random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the \"Addcl 1\" random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. The random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.\n\nInstead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes classifiers.\n\nIn machine learning, kernel random forests establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze.\n\nLeo Breiman was the first person to notice the link between random forest and kernel methods. He pointed out that random forests which are grown using i.i.d. random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani proposed Random Forest Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet first defined KeRF estimates and gave the explicit link between KeRF estimates and random forest. He also gave explicit expressions for kernels based on centered random forest and uniform random forest, two simplified models of random forest. He named these two KeRFs Centered KeRF and Uniform KeRF, and proved upper bounds on their rates of consistency.\n\nCentered forest is a simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level formula_18 is built, where formula_19 is a parameter of the algorithm.\n\nUniform forest is another simplified model for Breiman's original random forest, which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature.\n\nGiven a training sample formula_20 of formula_21-valued independent random variables distributed as the independent prototype pair formula_22, where formula_23. We aim at predicting the response formula_24, associated with the random variable formula_25, by estimating the regression function formula_26. A random regression forest is an ensemble of formula_27 randomized regression trees. Denote formula_28 the predicted value at point formula_29 by the formula_4-th tree, where formula_31 are independent random variables, distributed as a generic random variable formula_32, independent of the sample formula_33. This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate formula_34.\nFor regression trees, we have formula_35, where formula_36 is the cell containing formula_29, designed with randomness formula_38 and dataset formula_33, and formula_40.\n\nThus random forest estimates satisfy, for all formula_41, formula_42. Random regression forest has two level of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet defined KeRF by\n\nwhich is equal to the mean of the formula_44's falling in the cells containing formula_29 in the forest. If we define the connection function of the formula_27 finite forest as formula_47, i.e. the proportion of cells shared between formula_29 and formula_49, then almost surely we have formula_50, which defines the KeRF.\n\nThe construction of Centered KeRF of level formula_18 is the same as for centered forest, except that predictions are made by formula_52, the corresponding kernel function, or connection function is\n\nUniform KeRF is built in the same way as uniform forest, except that predictions are made by formula_52, the corresponding kernel function, or connection function is\n\nPredictions given by KeRF and random forests are close if the number of points in each cell is controlled:\nAssume that there exist sequences formula_56 such that, almost surely,\nThen almost surely,\nWhen the number of trees formula_27 goes to infinity, then we have infinite random forest and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:\nAssume that there exist sequences formula_60 such that, almost surely\nThen almost surely,\nAssume that formula_65, where formula_66 is a centered Gaussian noise, independent of formula_25, with finite variance formula_68. Moreover, formula_25 is uniformly distributed on formula_70 and formula_71 is Lipschitz. Scornet proved upper bounds on the rates of consistency for centered KeRF and uniform KeRF.\n\nProviding formula_72 and formula_73, there exists a constant formula_74 such that, for all formula_75,\nformula_76.\n\nProviding formula_72 and formula_73, there exists a constant formula_79 such that,\nformula_80.\n\nThe algorithm is often used in scientific works because of its advantages. For example, it can be used for quality assessment of Wikipedia articles.\n\n\n\n", "id": "1363880", "title": "Random forest"}
{"url": "https://en.wikipedia.org/wiki?curid=55817338", "text": "Algorithmic bias\n\nAlgorithmic bias occurs when a computer system behaves in ways that reflects the implicit values of humans involved in that data collection, selection, or use. Algorithmic bias has been identified and critiqued for its impact on search engine results, social media platforms, privacy, and racial profiling. In search results, this bias can create results reflecting racist, sexist, or other social biases, despite the presumed neutrality of the data. The study of algorithmic bias is most concerned with algorithms that reflect \"systematic and unfair\" discrimination.\n\nAs algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways unanticipated output and manipulation can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise, and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Nonetheless, bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or through use in unanticipated contexts or by audiences not considered in their initial design.\n\nAlgorithmic bias has been discovered or theorized in cases ranging from election outcomes to the spread of online hate speech. Problems in understanding, researching, and discovering algorithmic bias may come from the proprietary nature of algorithms, which are typically treated as trade secrets. Even with full transparency, understanding algorithms can be difficult because of their complexity, and because not every permutation of an algorithm's input or output can be anticipated or reproduced. In many cases, even within a single use-case (such as a website or app), there is no single \"algorithm\" to examine, but a vast network of interrelated programs and data inputs, even between users of the same service.\n\nAlgorithms are difficult to define, but may be generally understood as sets of instructions within computer programs that determine how these programs read, collect, process, and analyze data to generate some readable form of analysis or output. Newer computers can process millions of these algorithmic instructions per second, which has boosted the design and adoption of technologies such as machine learning and artificial intelligence. By analyzing and processing data, algorithms drive search engines, social media websites, recommendation engines, online retail, online advertising, and more.\n\nContemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications in order to understand their political effects, and to question the underlying assumptions of their neutrality. The term \"algorithmic bias\" is used to describe systematic and repeatable errors that create unfair outcomes, i.e., generating one result for certain users and another result for others. For example, a credit score algorithm may deny a loan based on certain factors without being unfair if it is consistently weighing relevant financial criteria. If that algorithm allows loans to some, but denies loans to another set of nearly identical users based on arbitrary criteria, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as \"biased\". This bias may be intentional or unintentional.\n\nBias can be introduced to an algorithm in several ways. During the assemblage of a database, data must be collected, digitized, adapted, and entered according to human-assisted cataloging criteria. Next, in the design of the algorithm, programmers assign certain priorities, or hierarchies, in how programs assess and sort that data. This requires human decisions about how data is categorized and which data is discarded. Some algorithms collect their own data based on human-selected criteria, which can reflect the bias of human users. Others may practice reinforcing stereotypes and preferences as they process and display \"relevant\" data for human users, as in selecting information based on previous choices of a user, or group of users.\n\nBeyond assembling the data, bias can emerge as a result of design. Examples may arise: In sorting processes that determine the allocation of resources or scrutiny (as in determining school placements), or classification and identification processes that may inadvertently discriminate against a category when assigning risk (as in credit scores). In processing associations, such as recommendation engines or inferred marketing traits, algorithms may be flawed in ways that reveal personal information. Inclusion and exclusion criteria may have unanticipated outcomes for search results, such as in flight recommendation software omitting flights that don't follow the sponsoring airline's preferred flight paths. Algorithms may also display an \"uncertainty bias\", offering more confident assessments when larger data sets are available. This can skew algorithmic processes toward results that more closely correspond with larger sample populations, which may not align with data from underrepresented populations.\n\nThe earliest computer programs reflected simple, human-derived operations, and were deemed to be functioning when they completed those operations. Artificial Intelligence pioneer Joseph Weizenbaum wrote that such programs are therefore understood to \"embody law\". Weizenbaum describes early, simple computer programs changing perceptions of machines from transferring power to transferring information. However, he noted that machines might transfer information with unintended consequences if there are errors in details provided to the machine, and if users interpret data in intuitive ways that cannot be formally communicated to, or from, a machine. Weizenbaum stated that all data fed to a machine must reflect \"human decisionmaking processes\" which have been translated into rules for the computer to follow. To do this, Weizenbaum asserted that programmers \"legislate the laws for a world one first has to create in imagination,\" and as a result, computer simulations can be built on models with incomplete or incorrect human data. Weizenbaum compared the results of such decisions to a tourist in a country who can make \"correct\" decisions through a coin toss, but has no basis of understanding how or why the decision was made.\n\nThe complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many pieces of code created for a single program; over time these decisions and their impact may be forgotten and taken as natural results of the program's output. These biases can create new patterns of behavior, or \"scripts,\" in relationship to specific technologies as the code interacts with other elements of society. Biases may also impact how society shapes itself around the data points that algorithms require.\n\nThe decisions of algorithmic programs can be weighed more heavily than the decisions of the human beings they are meant to assist, a process described by author Clay Shirky as \"algorithmic authority\". Shirky uses the term to describe \"the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources,\" such as search results. This neutrality can also be misrepresented through language frames used when results are presented to the public. For example, a list of news items selected and presented as \"trending\" or \"popular\" may be weighed based on significantly wider criteria than their popularity.\n\nBecause of their convenience and authority, algorithms are theorized as a means of delegating responsibility in decision making away from humans. This can have the effect of reducing alternative options, compromises, or flexibility. Sociologist Scott Lash has critiqued algorithms as a new form of \"generative power\" in that they are a virtual means of generating actual ends.\n\nPre-existing bias in an algorithm is a consequence of underlying social and institutional ideologies. Such ideas may reflect personal biases of individual designers or programmers, or can reflect social, institutional, or cultural assumptions. In both cases, such prejudices can be explicit and conscious, or implicit and unconscious. Poorly selected input data will influence the outcomes created by machines. In a critical view, encoding pre-existing bias into software can preserve social and institutional bias, and replicate it into all possible uses of the algorithm into the future.\n\nAn example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new UK citizens after the 1981 British Nationality Act. The program accurately reflected the tenets of the law, which stated that \"a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not.\" By attempting to appropriately articulate this logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm.\n\nTechnical bias emerges through limitations of a program, computational power, its design, or other constraint on the system. Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in an airline price display. Flaws in random number generation can also introduce bias into results.\n\nA \"decontextualized algorithm\" uses unrelated information to sort results, for example, a flight-pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines. The opposite may also apply, in which results are evaluated in different contexts from which they are collected. For example, data may be collected without crucial external context when facial recognition software is used by surveillance cameras, but evaluated by remote staff in another country or region, or evaluated by non-human algorithms with no awareness of what takes place beyond the camera's field of vision.\n\nLastly, technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior will correlate. For example, software that weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury, is displaying a form of technical bias.\n\nEmergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts. New forms of knowledge, such as drug or medical breakthroughs, new laws, business models, or shifting cultural norms, may be discovered without algorithms being adjusted to consider them. This may exclude groups through technology, without delineating clear outlines of authorship or personal responsibility. Similarly, problems may emerge when training data, i.e., the samples \"fed\" to a machine by which it models certain conclusions, do not align with uses that algorithm encounters in the real world.\n\nIn 1990, an example of emergent bias was identified in the software used to place US medical students into residencies, the National Residency Match Program (NRMP). The algorithm was designed at a time when few married couples would seek residencies together. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process calls for each applicant to provide a list of preferences for placement across the US, which is then sorted and assigned when a hospital and an applicant both agree to a match. In the case of married couples where both sought residencies, the algorithm weighed a \"lead member's\" location choices first. Once it identified an optimum placement for that person, it removed distant locations from their partner's preferences, reducing their list to the preferred locations within the same city as the partner. The result was a frequent assignment of high-rated schools for the first partner and lower-preference schools to the second partner, rather than sorting for compromises in placement preference.\n\nAdditional emergent biases include:\n\nUnpredictable correlations can emerge when large data sets are compared to each other in practice. For example, data collected about web-browsing patterns may align with signals marking sensitive data (such as race or sexual orientation). By \"discrimination\" against certain behavior or browsing patterns, the end effect would be almost identical to discrimination through the use of direct race or orientation data. In other cases, correlations can be inferred for reasons beyond the algorithm's ability to understand them, as when a triage program gave lower priority to asthmatics who had pneumonia. Because asthmatics with pneumonia were at the highest risk, hospitals typically give them the best and most immediate care; the algorithm simply compared survival rates.\n\nEmergent bias can occur when an algorithm is used by unanticipated audiences, such as machines that demand users can read, write, or understand numbers. Certain metaphors may not carry across different populations or skill sets. For example, the British National Act Program was created as a proof-of-concept by computer scientists and immigration lawyers to evaluate suitability for British citizenship. The designers therefore have expertise beyond the user, whose understanding of both software and immigration law would likely be unsophisticated. The agents administering the questions would not be aware of alternative pathways to citizenship outside of the software, and shifting case law and legal interpretations would lead the algorithm to outdated results.\n\nAn area of concern around emergent bias is that it may be compounded as biased technology is more deeply integrated into society. For example, users with vision impairments may not be able to use an ATM, but can easily go to a bank branch. If bank branches begin to close because ATMs replace them, they begin to exclude vision-impaired users from banking, an unintended consequence of a technology.\n\nEmergent bias may also create a feedback loop, or recursion, if data collected for an algorithm results in real-world responses which are fed back into the algorithm. For example, simulations of the predictive policing software, PredPol, deployed in Oakland, California, suggested an increased police presence in black neighborhoods based on crime data reported by the public. The simulation showed that public reports of crime could rise based on the sight of increased police activity, and could be interpreted by the software in modeling predictions of crime, and to encourage a further increase in police presence within the same neighborhoods. The Human Rights Data Analysis Group, which conducted the study, warned that in places where racial discrimination is a factor in arrests, such feedback loops could reinforce and perpetuate racial discrimination in policing.\n\nAn early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to St. George's Hospital Medical School per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with \"foreign-sounding names\" based on historical trends in admissions. Other examples include the display of higher-paying jobs to male applicants on job search websites.\n\nThe plagiarism-detection software Turnitin compares student-written texts to information found online and returns a probability that the student's work is copied. Because the software compares strings of text, it is more likely to identify non-native speakers of English than native speakers, who might be better able to adapt individual words and break up strings of plagiarized text, or obscure them through synonyms.\n\nSurveillance camera software may be considered inherently political as it requires algorithms to identify and flag normal from abnormal behaviors, and to determine who belongs in certain locations at certain times. The ability for such algorithms to recognize faces across a racial spectrum has been shown to be limited by the racial diversity of images in its training database: if the majority of photos belong to one race or gender, the technology is better at recognizing other members of that race or gender. A 2002 analysis of facial recognition software used to identify individuals in CCTV images found several examples of bias when run against criminal databases. The software was assessed as identifying men more frequently than women, older people more frequently than younger people, and identified Asians, African-Americans and other races more often than whites. Additional studies of facial recognition software have found the opposite to be true when built on non-criminal databases, with software offering lowest accuracy rates to darker-skinned females than any other race or gender.\n\nIn 2017 a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content, according to internal Facebook documents. The algorithm, which is a blend of computer programs and human content reviewers, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing \"Muslims\" would be blocked, while posts denouncing \"Radical Muslims\" would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the \"children\" subset of blacks, rather than \"all blacks,\" whereas \"all white men\" would trigger a block, because whites and males are not considered subsets. Facebook was also found to allow ad purchasers to target \"Jew haters\" as a category of users, which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data. The company's design also allowed ad buyers to block African-Americans from seeing housing ads.\n\nCorporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, American Airlines created a flight-finding algorithm in the 1980s. The software presented a range of flights from various airlines to customers, but weighed factors that boosted its own flights, regardless of price or convenience. In testimony to the United States Congress, the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment.\n\nIn a 1998 paper describing Google, the founders of the company adopted a policy of transparency in search results regarding paid placement, arguing that “advertising-funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers.” This bias would be an \"invisible\" manipulation of the user.\n\nA series of studies of undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20%. The researchers concluded that candidates have \"no means of competing\" if an algorithm, with or without intent, boosted page listings for a rival candidate.\n\nFacebook users who saw messages related to voting were more likely to vote themselves. A randomized trial of Facebook users showing an increased effect of 340,000 votes among users, and friends of users, who saw pro-voting messages in 2010. The legal scholar Jonathan Zittrain has warned that this could create a \"digital gerrymandering\" effect in elections, \"the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users,\" if intentionally manipulated.\n\nIn 2016, the professional networking site LinkedIn was discovered to recommend male variations of women's names in response to search queries for women. The site did not make similar recommendations in searches for male names. For example, \"Andrea\" would bring up a prompt asking if users meant \"Andrew,\" but queries for \"Andrew\" did not ask if users meant to find \"Andrea\". The company said this was the result of an analysis of users' interactions with the site.\n\nIn 2012, the department store franchise Target was cited for gathering data points to infer when women customers were pregnant, even if they hadn't announced it, and then sharing that information with marketing partners. Because the data had been predicted, rather than directly observed or reported, the company had no legal obligation to protect the privacy of those customers.\n\nWeb search algorithms have also been accused of bias. Google's results may prioritize pornographic content in search terms related to sexuality, for example, \"lesbian\". This bias extends to the search engine surfacing popular but sexualized content in neutral searches, as in \"Top 25 Sexiest Women Athletes\" articles displayed as first-page results in searches for \"women athletes\". In 2017 Google announced plans to curb search results that surfaced hate groups, racist views, child abuse and pornography, and other upsetting and offensive content.\n\nAlgorithms have been criticized as a method for obscuring racial prejudices in decision-making. Lisa Nakamura has noted that census machines were among the first to adopt the punch-card processes that lead to contemporary computing, and that their use as categorization and sorting machines for race has been long established and socially tolerated.\n\nOne example is the use of risk assessments in criminal sentencing and parole hearings, an algorithmically generated score intended to reflect the risk that a suspect or prisoner will repeat a crime. From 1920 until 1970, the nationality of a suspect's father was a consideration in such risk assessments. Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by ProPublica found that the scores were inaccurate 80% of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77% more often than whites.\n\nIn 2015, Google apologized when black users complained that an image-identification algorithm in its Photos application identified them as gorillas. In 2010, Nikon cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking. Such examples are the product of bias in biometric data sets. Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points.\n\nBiometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of any police record of that individual's name.\n\nIn 2011, users of the gay hookup app Grindr reported that the app was linked to sex-offender lookup apps in the Android app store's recommendation algorithm. Writer Mike Ananny criticized this association in The Atlantic, arguing that such associations further stigmatized gay men and may discourage closeted men to maintain secrecy. A 2009 incident with online retailer Amazon saw 57,000 books de-listed after a shift in the algorithm expanded its \"adult content\" blacklist for pornographic works to any books addressing sexuality or gay themes, for example, the critically acclaimed novel \"Brokeback Mountain\".\n\nSeveral challenges impede the study of large-scale algorithmic bias, hindering the application of academically rigorous studies and public understanding.\n\nCommercial algorithms are proprietary, and may be treated as trade secrets. This protects companies, such as a search engine, in cases where a transparent algorithm for ranking results would reveal techniques for manipulating the service. This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function. It can also be used to obscure possible unethical methods used in producing or processing algorithmic output. The closed nature of the code is not the only concern, however; as a certain degree of obscurity is protected by the complexity of contemporary programs, and the inability to know every permutations of a code's input or output.\n\nSocial scientist Bruno Latour has identified this process as blackboxing, a process in which \"scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become.\" Others have critiqued the black box metaphor, suggesting that current algorithms are not one black box, but a network of interconnected ones.\n\nAlgorithmic processes are complex, often exceeding the understanding of the people who use them. Large-scale operations may not be understood even by those involved in creating them. The social media site Facebook factored in at least 100,000 data points to determine the layout of a user's social media feed in 2013. Furthermore, large teams of programmers may operate in relative isolation from one another, and be unaware of the cumulative effects of small decisions with nested sections of sprawling algorithmic processes. Not all code is original, and may be borrowed from other libraries, creating a complicated set of relationships between data processing and data input systems.\n\nA significant barrier to understanding tackling bias in practice is that categories, such as demographics of individuals protected by anti-discrimination law, are often not explicitly held by those collecting and processing data. In some cases, there is little opportunity to collect this data explicitly, such as in device fingerprinting, ubiquitous computing and the Internet of Things. In other cases, the data controller may not wish to collect such data for reputational reasons, or because it represents a heightened liability and security risk. It may also be the case that, at least in relation to the General Data Protection Regulation, such data falls under the 'special category' provisions (Article 9), and therefore comes with more restrictions on potential collection and processing.\n\nAlgorithmic bias does not only relate to protected categories, but can also concern something less easily observable or codifiable, such as political viewpoints. In these cases, there is rarely an easily accessible or non-controversial ground truth, and 'debiasing' such a system becomes considerably more tricky.\n\nFurthermore, false and accidental correlations can emerge from a lack of understanding of protected categories, for example, insurance rates based on historical data of car accidents which may overlap with residential clusters of ethnic minorities.\n\nPersonalization of algorithms based on user interactions such as clicks, time on site, and other metrics, can confuse attempts to understand them. One unidentified streaming radio service reported it had five unique music-selection algorithms it selected for its users based on behavior. This creates widely disparate experiences of the same streaming product between different users.\nCompanies also run frequent A/B tests to fine-tune algorithms based on user response. For example, the search engine Bing can run up to ten million subtle variations of its service per day, segmenting the experience of an algorithm between users, or among the same users.\n\nComputer programs and systems can quickly spread among users, embedding biased algorithms into broader society before their impact can be recognized or remedied.\n\nThere might be competing motives within an organization, for example, the availability of loans from a bank as opposed to the bank's profit incentive and the banks reputation worry for being caught discriminating an already suppressed minority. A simple algorithm designed with a single purpose, such as expanding profits, would reduce loans to higher-risk applicants. In order to minimize discrimination between different groups of applicants (for ex. a majority group and a minority group), a banking algorithm would have to make choices between conflicting strategies such as these: maximize its interest in short-term profit, apply the same technical criteria to all applicants irrespective of groups, ensure an equal ratio of loan grants for each group of applicants irrespective of group members merits, or grant loans to an equal ratio of qualified applicants from each group.\n\nThe General Data Protection Regulation (GDPR), the European Union's revised data protection regime that enters force in 2018, addresses \"Automated individual decision-making, including profiling\" in Article 22. These rules prohibit \"solely\" automated decisions which have a \"significant\" or \"legal\" effect on an individual, unless they are explicitly authorised by consent, contract, or member state law. Where they are permitted, there must be safeguards in place, such as a right to a human-in-the-loop, and allegedly (although for political reasons, only in a non-binding recital) a right to an explanation of decisions reached. While these are commonly considered to be new, it is the case that nearly identical provisions have existed across Europe since 1995 in Article 15 of the Data Protection Directive, with the original automated decision rules and safeguards originating in French law in the later 1970s. They have rarely been used, given the heavy carve-outs that exist, and are not discussed in any case law of the European Court of Justice.\n\nThe GDPR does address algorithmic bias in profiling systems, as well as the statistical approaches possible to clean it, directly in recital 71, noting thatthe controller should use appropriate mathematical or statistical procedures for the profiling, implement technical and organisational measures appropriate [...] that prevents, inter alia, discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or that result in measures having such an effect.Like the alleged right to an explanation in recital 71, this suffers from the non-binding nature of recitals compared to the binding articles, and while it has been treated as a requirement by the Article 29 Working Party that advise on the implementation of data protection law, its practical dimensions are unclear. It has been argued that the obligatory Data Protection Impact Assessments for high risk data profiling, in tandem with other pre-emptive measures within data protection, may be a better way to tackle issues of algorithmic discrimination than relying on individual transparency rights, as information rights have traditionally fatigued individuals who are too overwhelmed and overburdened to use them effectively.\n\nThe United States has no overall legislation regulating controls for algorithmic bias, approaching the topic through various state and federal laws that might vary by industries, sectors, and uses. Many policies are self-enforced or controlled by the Federal Trade Commission. In 2016, the Obama administration released the National Artificial Intelligence Research and Development Strategic Plan, which called for a critical assessment of algorithms and for researchers to \"design these systems so that their actions and decision-making are transparent and easily interpretable by humans, and thus can be examined for any bias they may contain, rather than just learning and repeating these biases\".\n", "id": "55817338", "title": "Algorithmic bias"}
{"url": "https://en.wikipedia.org/wiki?curid=55843837", "text": "Automated machine learning\n\nAutomated machine learning (AutoML) is the process of automating the end-to-end process of applying machine learning to real-world problems. In a typical machine learning application, practitioners must apply the appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods that make the dataset amenable for machine learning. Following those preprocessing steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their final machine learning model. As many of these steps are often beyond the abilities of non-experts, AutoML was proposed as an artificial intelligence-based solution to the ever-growing challenge of applying machine learning. Automating the end-to-end process of applying machine learning offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform models that were designed by hand.\n\nAutomated machine learning can target various stages of the machine learning process:\n\nSoftware tackling various stages of AutoML:\n\n\n\n\n", "id": "55843837", "title": "Automated machine learning"}
{"url": "https://en.wikipedia.org/wiki?curid=30992863", "text": "Proaftn\n\nProaftn is a fuzzy classification method that belongs to the class of supervised learning algorithms. The acronym Proaftn stands for: (PROcédure d'Affectation Floue pour la problématique du Tri Nominal), which means in English: Fuzzy Assignment Procedure for Nominal Sorting.\nThe method enables to determine the fuzzy indifference relations by generalizing the indices (concordance and discordance) used in the ELECTRE III method. To determine the fuzzy indifference relations, PROAFTN uses the general scheme of the discretization technique described in, that establishes a set of pre-classified cases called a training set.\n\nTo resolve the classification problems, Proaftn proceeds by the following stages:\n\nStage 1. Modeling of classes: In this stage, the prototypes of the classes are conceived using the two following steps:\n\nDirect technique: It consists in adjusting the parameters through the training set and with the expert intervention.\n\nIndirect technique: It consists in fitting the parameters without the expert intervention as used in machine learning approaches. \nIn multicriteria classification problem, the indirect technique is known as \"preference disaggregation analysis\". This technique requires less cognitive effort than the former technique; it uses an automatic method to determine the optimal parameters, which minimize the classification errors.\nFurthermore, several heuristics and metaheuristics were used to learn the multicriteria classification method Proaftn.\n\nStage 2. Assignment: After conceiving the prototypes, Proaftn proceeds to assign the new objects to specific classes.\n\n", "id": "30992863", "title": "Proaftn"}
{"url": "https://en.wikipedia.org/wiki?curid=43385931", "text": "Data exploration\n\nData exploration is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems. These characteristics can include size or amount of data, completeness of the data, correctness of the data, possible relationships amongst data elements or files/tables in the data.\n\nData exploration is typically conducted using a combination of automated and manual activities. Automated activities can include data profiling or data visualization or tabular reports to give the analyst an initial view into the data and an understanding of key characteristics.\n\nThis is often followed by manual drill-down or filtering of the data to identify anomalies or patterns identified through the automated actions. Data exploration can also require manual scripting and queries into the data (e.g. using languages such as SQL or R) or using Excel or similar tools to view the raw data.\n\nAll of these activities are aimed at creating a clear mental model and understanding of the data in the mind of the analyst, and defining basic metadata (statistics, structure, relationships) for the data set that can be used in further analysis.\n\nOnce this initial understanding of the data is had, the data can be pruned or refined by removing unusable parts of the data, correcting poorly formatted elements and defining relevant relationships across datasets. This process is also known as determining data quality.\n\nData exploration can also refer to the ad hoc querying and visualization of data to identify potential relationships or insights that may be hidden in the data.\n\nTraditionally, this had been a key area of focus for statisticians, with John Tukey being a key evangelist in the field. . Today, data exploration is more widespread and is the focus of data analysts and data scientists; the latter being a relatively new role within enterprises and larger organizations.\n\nThis area of data exploration has become an area of interest in the field of machine learning. This is a relatively new field and is still evolving. As its most basic level, a machine-learning algorithm can be fed a data set and can be used to identify whether a hypothesis is true based on the dataset. Common machine learning algorithms can focus on identifying specific patterns in the data. Many common patterns include regression and classification or clustering, but there are many possible patterns and algorithms that can be applied to data via machine learning.\n\nBy employing machine learning, it is possible to find patterns or relationships in the data that would be difficult or impossible to find via manual inspection, trial and error or traditional exploration techniques.\n\n", "id": "43385931", "title": "Data exploration"}
{"url": "https://en.wikipedia.org/wiki?curid=49082762", "text": "List of datasets for machine learning research\n\nThese datasets are used for machine-learning research and have been cited in peer-reviewed academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.\n\nDatasets consisting primarily of images or videos for tasks such as object detection, facial recognition, and multi-label classification.\n\nIn computer vision, face images have been used extensively to develop facial recognition systems, face detection, and many other projects that use images of faces.\nDatasets consisting primarily of text for tasks such as natural language processing, sentiment analysis, translation, and cluster analysis.\n\nDatasets of sounds and sound features.\n\nDatasets containing electric signal information requiring some sort of Signal processing for further analysis.\n\nDatasets from physical systems\n\nDatasets from biological systems.\n\nThis section includes datasets that deals with structured data.\n\nDatasets consisting of rows of observations and columns of attributes characterizing those observations. Typically used for regression analysis or classification but other types of algorithms can also be used. This section includes datasets that do not fit in the above categories.\n\nAs datasets come in myriad formats and can sometimes be difficult to use, there has been considerable work put into curating and standardizing the format of datasets to make them easier to use for machine learning research.\n\n\n", "id": "49082762", "title": "List of datasets for machine learning research"}
{"url": "https://en.wikipedia.org/wiki?curid=53587467", "text": "Outline of machine learning\n\nThe following outline is provided as an overview of and topical guide to machine learning:\n\nMachine learning – subfield of computer science (more particularly soft computing) that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a \"Field of study that gives computers the ability to learn without being explicitly programmed\". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example \"training set\" of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.\n\n\nSubfields of machine learning\n\nCross-disciplinary fields involving machine learning\n\nApplications of machine learning\n\nMachine learning hardware\n\nMachine learning tools   (list)\n\nMachine learning framework\n\nProprietary machine learning frameworks\n\nOpen source machine learning frameworks\n\nMachine learning library   \n\nMachine learning algorithm\n\n\nMachine learning method   (list)\n\nDimensionality reduction\n\nEnsemble learning\n\nMeta learning\n\nReinforcement learning\n\nSupervised learning\n\nBayesian statistics\n\nDecision tree algorithm\n\nLinear classifier\n\nUnsupervised learning\n\nArtificial neural network\n\nAssociation rule learning\n\nHierarchical clustering\n\nCluster analysis\n\nAnomaly detection\n\nSemi-supervised learning\n\nDeep learning\n\n\nMachine learning research\n\nHistory of machine learning\n\nMachine learning projects\n\nMachine learning organizations\n\n\nBooks about machine learning\n\n\n\n\n\n\n\n\n", "id": "53587467", "title": "Outline of machine learning"}
{"url": "https://en.wikipedia.org/wiki?curid=54361643", "text": "Hyperparameter optimization\n\nIn machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.\n\nThe same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data. The objective function takes a tuple of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance.\n\nThe traditional way of performing hyperparameter optimization has been \"grid search\", or a \"parameter sweep\", which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set\nor evaluation on a held-out validation set.\n\nSince the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, manually set bounds and discretization may be necessary before applying grid search.\n\nFor example, a typical soft-margin SVM classifier equipped with an RBF kernel has at least two hyperparameters that need to be tuned for good performance on unseen data: a regularization constant \"C\" and a kernel hyperparameter γ. Both parameters are continuous, so to perform grid search, one selects a finite set of \"reasonable\" values for each, say\n\nGrid search then trains an SVM with each pair (\"C\", γ) in the Cartesian product of these two sets and evaluates their performance on a held-out validation set (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair). Finally, the grid search algorithm outputs the settings that achieved the highest score in the validation procedure.\n\nGrid search suffers from the curse of dimensionality, but is often embarrassingly parallel because typically the hyperparameter settings it evaluates are independent of each other.\n\nRandom Search replaces the exhaustive enumeration of all combinations by selecting them randomly. This can be simply applied to the discrete setting described above, but also generalizes to continuous and mixed spaces. It can outperform Grid search, especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm. In this case, the optimization problem is said to have a low intrinsic dimensionality. Random Search is also embarrassingly parallel, and additionally allows to include prior knowledge by specifying the distribution from which to sample. \n\nBayesian optimization is a global optimization method for noisy black-box functions. Applied to hyperparameter optimization, Bayesian optimization builds a probabilistic model of the function mapping from hyperparameter values to the objective evaluated on a validation set. By iteratively evaluating a promising hyperparameter configuration based on the current model, and then updating it, Bayesian optimization, aims to gather observations revealing as much information as possible about this function and, in particular, the location of the optimum. It tries to balance exploration (hyperparameters for which the outcome is most uncertain) and exploitation (hyperparameters expected close to the optimum). In practice, Bayesian optimization has been shown to obtain better results in fewer evaluations compared to grid search and random search, due to the ability to reason about the quality of experiments before they are run.\n\nFor specific learning algorithms, it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent. The first usage of these techniques was focused on neural networks. Since then, these methods have been extended to other models such as support vector machines or logistic regression.\n\nA different approach in order to obtain a gradient with respect to hyperparameters consists in differentiating the steps of an iterative optimization algorithm using automatic differentiation.\n\nEvolutionary optimization is a methodology for the global optimization of noisy black-box functions. In hyperparameter optimization, evolutionary optimization uses evolutionary algorithms to search the space of hyperparameters for a given algorithm. Evolutionary hyperparameter optimization follows a process inspired by the biological concept of evolution:\n\n\nEvolutionary optimization has been used in hyperparameter optimization for statistical machine learning algorithms, automated machine learning, deep neural network architecture search, as well as training of the weights in deep neural networks.\n\nRBF and spectral approaches have also been developed.\n\n\n\n\n\n\n\n", "id": "54361643", "title": "Hyperparameter optimization"}
{"url": "https://en.wikipedia.org/wiki?curid=406624", "text": "Time series\n\nA time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.\n\nTime series are very frequently plotted via line charts. Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.\n\nTime series \"analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series \"forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test theories that the current values of one or more independent time series affect the current value of another time series, this type of analysis of time series is not called \"time series analysis\", which focuses on comparing values of a single time series or multiple dependent time series at different points in time. Interrupted time series analysis is the analysis of interventions on a single time series.\n\nTime series data have a natural temporal ordering. This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order). Time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see time reversibility.)\n\nTime series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language).\n\nMethods for time series analysis may be divided into two classes: frequency-domain methods and time-domain methods. The former include spectral analysis and wavelet analysis; the latter include auto-correlation and cross-correlation analysis. In the time domain, correlation and analysis can be made in a filter-like manner using scaled correlation, thereby mitigating the need to operate in the frequency domain.\n\nAdditionally, time series analysis techniques may be divided into parametric and non-parametric methods. The parametric approaches assume that the underlying stationary stochastic process has a certain structure which can be described using a small number of parameters (for example, using an autoregressive or moving average model). In these approaches, the task is to estimate the parameters of the model that describes the stochastic process. By contrast, non-parametric approaches explicitly estimate the covariance or the spectrum of the process without assuming that the process has any particular structure.\n\nMethods of time series analysis may also be divided into linear and non-linear, and univariate and multivariate.\n\nA time series is one type of panel data. Panel data is the general class, a multidimensional data set, whereas a time series data set is a one-dimensional panel (as is a cross-sectional dataset). A data set may exhibit characteristics of both panel data and time series data. One way to tell is to ask what makes one data record unique from the other records. If the answer is the time data field, then this is a time series data set candidate. If determining a unique record requires a time data field and an additional identifier which is unrelated to time (student ID, stock symbol, country code), then it is panel data candidate. If the differentiation lies on the non-time identifier, then the data set is a cross-sectional data set candidate.\n\nThere are several types of motivation and data analysis available for time series which are appropriate for different purposes and etc.\n\nIn the context of statistics, econometrics, quantitative finance, seismology, meteorology, and geophysics the primary goal of time series analysis is forecasting. In the context of signal processing, control engineering and communication engineering it is used for signal detection and estimation, while in the context of data mining, pattern recognition and machine learning time series analysis can be used for clustering, classification, query by content, anomaly detection as well as forecasting.\n\nThe clearest way to examine a regular time series manually is with a line chart such as the one shown for tuberculosis in the United States, made with a spreadsheet program. The number of cases was standardized to a rate per 100,000 and the percent change per year in this rate was calculated. The nearly steadily dropping line shows that the TB incidence was decreasing in most years, but the percent change in this rate varied by as much as +/- 10%, with 'surges' in 1975 and around the early 1990s. The use of both vertical axes allows the comparison of two time series in one graphic.\n\nOther techniques include:\n\n\nCurve fitting is the process of constructing a curve, or mathematical function, that has the best fit to a series of data points, possibly subject to constraints. Curve fitting can involve either interpolation, where an exact fit to the data is required, or smoothing, in which a \"smooth\" function is constructed that approximately fits the data. A related topic is regression analysis, which focuses more on questions of statistical inference such as how much uncertainty is present in a curve that is fit to data observed with random errors. Fitted curves can be used as an aid for data visualization, to infer values of a function where no data are available, and to summarize the relationships among two or more variables. Extrapolation refers to the use of a fitted curve beyond the range of the observed data, and is subject to a degree of uncertainty since it may reflect the method used to construct the curve as much as it reflects the observed data.\n\nThe construction of economic time series involves the estimation of some components for some dates by interpolation between values (\"benchmarks\") for earlier and later dates. Interpolation is estimation of an unknown quantity between two known quantities (historical data), or drawing conclusions about missing information from the available information (\"reading between the lines\"). Interpolation is useful where the data surrounding the missing data is available and its trend, seasonality, and longer-term cycles are known. This is often done by using a related series known for all relevant dates. Alternatively polynomial interpolation or spline interpolation is used where piecewise polynomial functions are fit into time intervals such that they fit smoothly together. A different problem which is closely related to interpolation is the approximation of a complicated function by a simple function (also called regression).The main difference between regression and interpolation is that polynomial regression gives a single polynomial that models the entire data set. Spline interpolation, however, yield a piecewise continuous function composed of many polynomials to model the data set.\n\nExtrapolation is the process of estimating, beyond the original observation range, the value of a variable on the basis of its relationship with another variable. It is similar to interpolation, which produces estimates between known observations, but extrapolation is subject to greater uncertainty and a higher risk of producing meaningless results.\n\nIn general, a function approximation problem asks us to select a function among a well-defined class that closely matches (\"approximates\") a target function in a task-specific way.\nOne can distinguish two major classes of function approximation problems: First, for known target functions approximation theory is the branch of numerical analysis that investigates how certain known functions (for example, special functions) can be approximated by a specific class of functions (for example, polynomials or rational functions) that often have desirable properties (inexpensive computation, continuity, integral and limit values, etc.).\n\nSecond, the target function, call it \"g\", may be unknown; instead of an explicit formula, only a set of points (a time series) of the form (\"x\", \"g\"(\"x\")) is provided. Depending on the structure of the domain and codomain of \"g\", several techniques for approximating \"g\" may be applicable. For example, if \"g\" is an operation on the real numbers, techniques of interpolation, extrapolation, regression analysis, and curve fitting can be used. If the codomain (range or target set) of \"g\" is a finite set, one is dealing with a classification problem instead. A related problem of \"online\" time series approximation is to summarize the data in one-pass and construct an approximate representation that can support a variety of time series queries with bounds on worst-case error.\n\nTo some extent the different problems (regression, classification, fitness approximation) have received a unified treatment in statistical learning theory, where they are viewed as supervised learning problems.\n\nIn statistics, prediction is a part of statistical inference. One particular approach to such inference is known as predictive inference, but the prediction can be undertaken within any of the several approaches to statistical inference. Indeed, one description of statistics is that it provides a means of transferring knowledge about a sample of a population to the whole population, and to other related populations, which is not necessarily the same as prediction over time. When information is transferred across time, often to specific points in time, the process is known as forecasting.\n\nAssigning time series pattern to a specific category, for example identify a word based on series of hand movements in sign language.\n\nThis approach is based on harmonic analysis and filtering of signals in the frequency domain using the Fourier transform, and spectral density estimation, the development of which was significantly accelerated during World War II by mathematician Norbert Wiener, electrical engineers Rudolf E. Kálmán, Dennis Gabor and others for filtering signals from noise and predicting signal values at a certain point in time. See Kalman filter, Estimation theory, and Digital signal processing\n\nSplitting a time-series into a sequence of segments. It is often the case that a time-series can be represented as a sequence of individual segments, each with its own characteristic properties. For example, the audio signal from a conference call can be partitioned into pieces corresponding to the times during which each person was speaking. In time-series segmentation, the goal is to identify the segment boundary points in the time-series, and to characterize the dynamical properties associated with each segment. One can approach this problem using change-point detection, or by modeling the time-series as a more sophisticated system, such as a Markov jump linear system.\n\nModels for time series data can have many forms and represent different stochastic processes. When modeling variations in the level of a process, three broad classes of practical importance are the \"autoregressive\" (AR) models, the \"integrated\" (I) models, and the \"moving average\" (MA) models. These three classes depend linearly on previous data points. Combinations of these ideas produce autoregressive moving average (ARMA) and autoregressive integrated moving average (ARIMA) models. The autoregressive fractionally integrated moving average (ARFIMA) model generalizes the former three. Extensions of these classes to deal with vector-valued data are available under the heading of multivariate time-series models and sometimes the preceding acronyms are extended by including an initial \"V\" for \"vector\", as in VAR for vector autoregression. An additional set of extensions of these models is available for use where the observed time-series is driven by some \"forcing\" time-series (which may not have a causal effect on the observed series): the distinction from the multivariate case is that the forcing series may be deterministic or under the experimenter's control. For these models, the acronyms are extended with a final \"X\" for \"exogenous\".\n\nNon-linear dependence of the level of a series on previous data points is of interest, partly because of the possibility of producing a chaotic time series. However, more importantly, empirical investigations can indicate the advantage of using predictions derived from non-linear models, over those from linear models, as for example in nonlinear autoregressive exogenous models. Further references on nonlinear time series analysis: (Kantz and Schreiber), and (Abarbanel)\n\nAmong other types of non-linear time series models, there are models to represent the changes of variance over time (heteroskedasticity). These models represent autoregressive conditional heteroskedasticity (ARCH) and the collection comprises a wide variety of representation (GARCH, TARCH, EGARCH, FIGARCH, CGARCH, etc.). Here changes in variability are related to, or predicted by, recent past values of the observed series. This is in contrast to other possible representations of locally varying variability, where the variability might be modelled as being driven by a separate time-varying process, as in a doubly stochastic model.\n\nIn recent work on model-free analyses, wavelet transform based methods (for example locally stationary wavelets and wavelet decomposed neural networks) have gained favor. Multiscale (often referred to as multiresolution) techniques decompose a given time series, attempting to illustrate time dependence at multiple scales. See also Markov switching multifractal (MSMF) techniques for modeling volatility evolution.\n\nA Hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states. An HMM can be considered as the simplest dynamic Bayesian network. HMM models are widely used in speech recognition, for translating a time series of spoken words into text.\n\nA number of different notations are in use for time-series analysis. A common notation specifying a time series \"X\" that is indexed by the natural numbers is written\n\nAnother common notation is\nwhere \"T\" is the index set.\n\nThere are two sets of conditions under which much of the theory is built:\n\nHowever, ideas of stationarity must be expanded to consider two important ideas: strict stationarity and second-order stationarity. Both models and applications can be developed under each of these conditions, although the models in the latter case might be considered as only partly specified.\n\nIn addition, time-series analysis can be applied where the series are seasonally stationary or non-stationary. Situations where the amplitudes of frequency components change with time can be dealt with in time-frequency analysis which makes use of a time–frequency representation of a time-series or signal.\n\nTools for investigating time-series data include:\n\n\nTime series metrics or features that can be used for time series classification or regression analysis:\n\n\nTime series can be visualized with two categories of chart: Overlapping Charts and Separated Charts. Overlapping Charts display all-time series on the same layout while Separated Charts presents them on different layouts (but aligned for comparison purpose)\n\n\n\nWorking with Time Series data is a relatively common use for statistical analysis software. As a result of this, there are many offerings both commercial and open source. Some examples include:\n\n\n", "id": "406624", "title": "Time series"}
{"url": "https://en.wikipedia.org/wiki?curid=56142183", "text": "Paraphrasing (computational linguistics)\n\nParaphrase or Paraphrasing in computational linguistics is the natural language processing task of detecting and generating paraphrases. Applications of paraphrasing are varied including information retrieval, question answering, text summarization, and plagiarism detection. Paraphrasing is also useful in the evaluation of machine translation, as well as generation of new samples to expand existing corpora.\n\nBarzilay and Lee proposed a method to generate paraphrases through the usage of monolingual parallel corpora, namely news articles covering the same event on the same day. Training consists of using multi-sequence alignment to generate sentence-level paraphrases from an unannotated corpus. This is done by\n\n\nThis is achieved by first clustering similar sentences together using n-gram overlap. Recurring patterns are found within clusters by using multi-sequence alignment. Then the position of argument words are determined by finding areas of high variability within each clusters, aka between words shared by more than 50% of a cluster's sentences. Pairings between patterns are then found by comparing similar variable words between different corpora. Finally new paraphrases can be generated by choosing a matching cluster for a source sentence, then substituting the source sentence's argument into any number of patterns in the cluster.\n\nParaphrase can also be generated through the use of phrase-based translation as proposed by Bannard and Callison-Burch. The chief concept consists of aligning phrases in a pivot language to produce potential paraphrases in the original language. For example, the phrase \"under control\" in an English sentence is aligned with the phrase \"unter kontrolle\" in its German counterpart. The phrase \"unter kontrolle\" is then found in another German sentence with the aligned English phrase being \"in check\", a paraphrase of \"under control\".\n\nThe probability distribution can be modeled as formula_1, the probability phrase formula_2 is a paraphrase of formula_3, which is equivalent to formula_4 summed over all formula_5, a potential phrase translation in the pivot language. Additionally, the sentence formula_3 is added as a prior to add context to the paraphrase. Thus the optimal paraphrase, formula_7 can be modeled as:\n\nformula_9 and formula_10 can be approximated by simply taking their frequencies. Adding formula_11 as a prior is modeled by calculating the probability of forming the formula_11 when formula_3 is substituted with \n\nThere has been success in using long short-term memory (LSTM) models to generate paraphrases. In short, the model consists of an encoder and decoder component, both implemented using variations of a stacked residual LSTM. First, the encoding LSTM takes a one-hot encoding of all the words in a sentence as input and produces a final hidden vector, which can be viewed as a representation of the input sentence. The decoding LSTM then takes the hidden vector as input and generates new sentence, terminating in an end-of-sentence token. The encoder and decoder are trained to take a phrase and reproduce the one-hot distribution of a corresponding paraphrase by minimizing perplexity using simple stochastic gradient descent. New paraphrases are generated by inputting a new phrase to the encoder and passing the output to the decoder.\n\nParaphrase recognition has been attempted by Socher et al through the use of recursive autoencoders. The main concept is to produce a vector representation of a sentence along with its components through recursively using an autoencoder. The vector representations of paraphrases should have similar vector representations; they are processed, then fed as input into a neural network for classification.\n\nGiven a sentence formula_14 with formula_15 words, the autoencoder is designed to take 2 formula_16-dimensional word embeddings as input and produce an formula_16-dimensional vector as output. The same autoencoder is applied to every pair of words in formula_11 to produce formula_19 vectors. The autoencoder is then applied recursively with the new vectors as inputs until a single vector is produced. Given an odd number of inputs, the first vector is forwarded as is to the next level of recursion. The autoencoder is then trained to reproduce every vector in the full recursion tree including the initial word embeddings.\n\nGiven two sentences formula_20 and formula_21 of length 4 and 3 respectively, the autoencoders would produce 7 and 5 vector representations including the initial word embeddings. The euclidean distance is then taken between every combination of vectors in formula_20 and formula_21 to produce a similarity matrix formula_24. formula_11 is then subject to a dynamic min-pooling layer to produce a fixed size formula_26 matrix. Since formula_11 are not uniform in size among all potential sentences, formula_11 is split into formula_29 roughly even sections. The output is then normalized to have mean 0 and standard deviation 1 and is fed into a fully connected layer with a softmax output. The dynamic pooling to softmax model is trained using pairs of known paraphrases.\n\nSkip-thought vectors are an attempt to create a vector representation of the semantic meaning of a sentence in a similar fashion as the skip gram model. Skip-thought vectors are produced through the use of a skip-thought model which consists of three key components, an encoder and two decoders. Given a corpus of documents, the skip-thought model is trained to take a sentence as input and encode it into a skip-thought vector. The skip-thought vector is used as input for both decoders, one of which attempts to reproduce the previous sentence and the other the following sentence in its entirety. The encoder and decoder can be implemented through the use of a recursive neural network (RNN) or an LSTM.\n\nSince paraphrases carry the same semantic meaning between one another, they should have similar skip-thought vectors. Thus a simple logistic regression can be trained to a good performance with the absolute difference and component-wise product of two skip-thought vectors as input.\n\nThere are multiple methods that can be used to evaluate paraphrases. Since paraphrase recognition can be posed as a classification problem, most standard evaluations metrics such as accuracy, f1 score, or an ROC curve do relatively well. However, there is difficulty calculating f1-scores due to trouble produce a complete list of paraphrases for a given phrase along with the fact that good paraphrases are dependent upon context. A metric designed to counter these problems is ParaMetric. ParaMetric aims to calculate the precision and recall of an automatic paraphrase system by comparing the automatic alignment of paraphrases to a manual alignment of similar phrases. Since ParaMetric is simply rating the quality of phrase alignment, it can be used to rate paraphrase generation systems as well assuming it uses phrase alignment as part of its generation process. A noted drawback to ParaMetric is the large and exhaustive set of manual alignments that must be initially created before a rating can be produced.\n\nThe evaluation of paraphrase generation has similar difficulties as the evaluation of machine translation. Often the quality of a paraphrase is dependent upon its context, whether it is being used as a summary, and how it is generated among other factors. Additionally, a good paraphrase usually is lexically dissimilar from its source phrase. The simplest method used to evaluate paraphrase generation would be through the use of human judges. Unfortunately, evaluation through human judges tends to be time consuming. Automated approaches to evaluation prove to be challenging as it is essentially a problem as difficult as paraphrase recognition. While originally used to evaluate machine translations, bilingual evaluation understudy (BLEU) has been used successfully to evaluate paraphrase generation models as well. However, paraphrases often have several lexically different but equally valid solutions which hurts BLEU and other similar evaluation metrics.\n\nMetrics specifically designed to evaluate paraphrase generation include paraphrase in n-gram change (PINC) and paraphrase evaluation metric (PEM) along with the aforementioned ParaMetric. PINC is designed to be used in conjunction with BLEU and help cover its inadequacies. Since BLEU has difficulty measuring lexical dissimilarity, PINC is a measurement of the lack of n-gram overlap between a source sentence and a candidate paraphrase. It is essentially the Jaccard distance between the sentence excluding n-grams that appear in the source sentence to maintain some semantic equivalence. PEM, on the other hand, attempts to evaluate the \"adequacy, fluency, and lexical dissimilarity\" of paraphrases by returning a single value heuristic calculated using N-grams overlap in a pivot language. However, a large drawback to PEM is that must be trained using a large, in-domain parallel corpora as well as human judges. In other words, it is tantamount to training a paraphrase recognition system in order to evaluate a paraphrase generation system.\n\n", "id": "56142183", "title": "Paraphrasing (computational linguistics)"}
{"url": "https://en.wikipedia.org/wiki?curid=56332516", "text": "Hierarchical Deep Learning\n\nHierarchical Deep Learning is the machine learning task with the \"hierarchical labeled\" data (a classification or categorization). Since the examples given to the learner are hierarchical labeled, the evaluation is based on the accuracy or F1-measure based on multi level of the model hierarchy. This model have been used for text classification, as in Hierarchical Deep Learning employs stacks of deep learning architectures to provide specialized understanding at each level of the data hierarchy.\n\nApproaches as part of HDL include:\n\n\nThe primary contribution of this technique is hierarchical classification of documents. A traditional multi-class classification technique can work well for a limited number classes, but performance drops with increasing number of categories or classes, as is present in hierarchically organized documents. Many techniques works on Hierarchical Attention for text classification, or Multimodel Deep Learning for classification task\n. Hence, they provide extensions over current methods for document classification that only consider the multi-class problem. The methods described as HDLTex can improved in multiple ways. Additional training and testing with other hierarchically structured document data sets will continue to identify architectures that work best for these problems. Also, it is possible to extend the hierarchy to more than two levels to capture more of the complexity in the hierarchical classification. For example, if keywords are treated as ordered then the hierarchy continues down multiple levels. HDLTex can also be applied to unlabeled (unsupervised) documents, such as those found in news or other media outlets. In hierarchical deep learning model, this problem is solved by creating architectures that specialize deep learning approaches for their level of the document hierarchy. The structure of Hierarchical Deep Learning for Text (HDLTex) architecture for each deep learning model is as follows:\n\nHierarchical Deep Learning can be used for sentiment analysis in any languages for social networks of documents.\n\n\n", "id": "56332516", "title": "Hierarchical Deep Learning"}
{"url": "https://en.wikipedia.org/wiki?curid=57154430", "text": "Deep reinforcement learning\n\nDeep reinforcement learning (DRL) is a machine learning method that extends reinforcement learning approach to learning of the entire process from sensors to motors (end-to-end) using deep learning techniques. Therefore it is often called end-to-end reinforcement learning from the point of the aim.\n\nThe traditional Reinforcement learning aims to solve problems of how agents can learn to take the best actions on the environment to get the maximum cumulative reward over time. A major part of this process is carefully engineering feature representations. The recent advances of deep learning for learning feature representations have paved the way for end-to-end reinforcement learning.\n\nSome of the main algorithms include:\n\n\n\n", "id": "57154430", "title": "Deep reinforcement learning"}
{"url": "https://en.wikipedia.org/wiki?curid=31978226", "text": "Life-time of correlation\n\nThe life-time of correlation measures the timespan over which there is appreciable autocorrelation or cross correlation in stochastic processes.\n\nThe correlation coefficient \"ρ\", expressed as an autocorrelation function or cross-correlation function, depends on the lag-time between the times being considered. Typically such functions, \"ρ\"(\"t\"), decay to zero with increasing lag-time, but they can assume values across all levels of correlations: strong and weak, and positive and negative as in the table.\n\nThe life-time of a correlation is defined as the length of time when the correlation coefficient is at the strong level. The durability of correlation is determined by signal (the strong level of correlation is separated from weak and negative levels). The mean life-time of correlation could measure how the durability of correlation depends on the window width size (the window is the length of time series used to calculate correlation).\n", "id": "31978226", "title": "Life-time of correlation"}
